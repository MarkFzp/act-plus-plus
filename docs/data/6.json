{
    "600": {
        "file_id": 32,
        "content": "    parser.add_argument('--vq_class', action='store', type=int, help='vq_class')\n    parser.add_argument('--vq_dim', action='store', type=int, help='vq_dim')\n    main(vars(parser.parse_args()))",
        "type": "code",
        "location": "/train_latent_model.py:467-470"
    },
    "601": {
        "file_id": 32,
        "content": "This code is adding two arguments, \"--vq_class\" and \"--vq_dim\", to the parser using store action and specifying their types as integer (int). These arguments provide parameters for a latent model's class and dimensionality. The main function is then called with these parameters obtained from parsing command line arguments.",
        "type": "comment"
    },
    "602": {
        "file_id": 33,
        "content": "/truncate_data.py",
        "type": "filepath"
    },
    "603": {
        "file_id": 33,
        "content": "This script truncates and compresses a dataset using h5py, creating an observation group with limited image data. It saves truncated datasets or videos, extracts camera names, resizes images, and requires 'act-plus-plus' for argument parsing and directory manipulation. Output dataset directory has '_truncated' suffix.",
        "type": "summary"
    },
    "604": {
        "file_id": 33,
        "content": "\"\"\"\nExample usage:\n$ python3 script/compress_data.py --dataset_dir /scr/lucyshi/dataset/aloha_test\n\"\"\"\nimport os\nimport h5py\nimport cv2\nimport numpy as np\nimport argparse\nfrom tqdm import tqdm\n# Constants\nDT = 0.02\nJOINT_NAMES = [\"waist\", \"shoulder\", \"elbow\", \"forearm_roll\", \"wrist_angle\", \"wrist_rotate\"]\nSTATE_NAMES = JOINT_NAMES + [\"gripper\"]\nTRUNCATE_LEN = 2250\ndef compress_dataset(input_dataset_path, output_dataset_path):\n    # Check if output path exists\n    if os.path.exists(output_dataset_path):\n        print(f\"The file {output_dataset_path} already exists. Exiting...\")\n        return\n    # Load the uncompressed dataset\n    with h5py.File(input_dataset_path, 'r') as infile:\n        # Create the compressed dataset\n        with h5py.File(output_dataset_path, 'w') as outfile:\n            outfile.attrs['sim'] = infile.attrs['sim']\n            outfile.attrs['compress'] = True\n            # Copy non-image data directly\n            for key in infile.keys():\n                if key != 'observations' and key != 'compress_len':",
        "type": "code",
        "location": "/truncate_data.py:1-35"
    },
    "605": {
        "file_id": 33,
        "content": "This script compresses a dataset by truncating its length and storing the compressed dataset in a new file. It checks if the output path already exists and copies non-image data directly to the output file. The script takes an input_dataset_path and an output_dataset_path as arguments, and it uses h5py library for handling HDF5 files.",
        "type": "comment"
    },
    "606": {
        "file_id": 33,
        "content": "                    data = infile[key][:TRUNCATE_LEN]\n                    out_data = outfile.create_dataset(key, (TRUNCATE_LEN, data.shape[1]))\n                    out_data[:] = data\n            data_compress_len = infile['compress_len']\n            out_data_compress_len = outfile.create_dataset('compress_len', data_compress_len.shape)\n            out_data_compress_len[:] = data_compress_len\n            # Create observation group in the output\n            obs_group = infile['observations']\n            out_obs_group = outfile.create_group('observations')\n            for key in obs_group.keys():\n                if key != 'images':\n                    data = obs_group[key][:TRUNCATE_LEN]\n                    out_data = out_obs_group.create_dataset(key, (TRUNCATE_LEN, data.shape[1]))\n                    out_data[:] = data\n            image_group = obs_group['images']\n            out_image_group = out_obs_group.create_group('images')\n            for cam_name in image_group.keys():\n                data = image_group[cam_name][:TRUNCATE_LEN]",
        "type": "code",
        "location": "/truncate_data.py:36-57"
    },
    "607": {
        "file_id": 33,
        "content": "Truncates and compresses data, creates observation group with limited image data.",
        "type": "comment"
    },
    "608": {
        "file_id": 33,
        "content": "                out_data = out_image_group.create_dataset(cam_name, (TRUNCATE_LEN, data.shape[1]), dtype='uint8')\n                out_data[:] = data\n    print(f\"Truncated dataset saved to {output_dataset_path}\")\ndef save_videos(video, dt, video_path=None):\n    if isinstance(video, list):\n        cam_names = list(video[0].keys())\n        h, w, _ = video[0][cam_names[0]].shape\n        w = w * len(cam_names)\n        fps = int(1/dt)\n        out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n        # bitrate = 1000000\n        # out.set(cv2.VIDEOWRITER_PROP_BITRATE, bitrate)\n        for ts, image_dict in enumerate(video):\n            images = []\n            for cam_name in cam_names:\n                image = image_dict[cam_name]\n                image = image[:, :, [2, 1, 0]] # swap B and R channel\n                images.append(image)\n            images = np.concatenate(images, axis=1)\n            out.write(images)\n        out.release()\n        print(f'Saved video to: {video_path}')\n    elif isinstance(video, dict):",
        "type": "code",
        "location": "/truncate_data.py:58-84"
    },
    "609": {
        "file_id": 33,
        "content": "This code saves a truncated dataset or video depending on the input format. If a list of videos is given, it extracts camera names, resizes the images, and concatenates them into a single video. It then writes the video to the specified path and prints a success message.",
        "type": "comment"
    },
    "610": {
        "file_id": 33,
        "content": "        cam_names = list(video.keys())\n        # Remove depth images\n        cam_names = [cam_name for cam_name in cam_names if '_depth' not in cam_name]\n        all_cam_videos = []\n        for cam_name in cam_names:\n            all_cam_videos.append(video[cam_name])\n        all_cam_videos = np.concatenate(all_cam_videos, axis=2) # width dimension\n        n_frames, h, w, _ = all_cam_videos.shape\n        fps = int(1 / dt)\n        out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n        for t in range(n_frames):\n            image = all_cam_videos[t]\n            image = image[:, :, [2, 1, 0]]  # swap B and R channel\n            out.write(image)\n        out.release()\n        print(f'Saved video to: {video_path}')\ndef load_and_save_first_episode_video(dataset_dir, video_path):\n    dataset_name = 'episode_0'\n    _, _, _, _, image_dict = load_hdf5(dataset_dir, dataset_name)\n    save_videos(image_dict, DT, video_path=video_path)\ndef load_hdf5(dataset_dir, dataset_name):\n    dataset_path = os.path.join(dataset_dir, dataset_name + '.hdf5')",
        "type": "code",
        "location": "/truncate_data.py:85-111"
    },
    "611": {
        "file_id": 33,
        "content": "The code loads and saves a video from an HDF5 file. It first removes depth images, concatenates the remaining videos along the width dimension, converts the BGR image to RGB, then writes the video to a specified path at the given frame rate. The function `load_and_save_first_episode_video` calls other functions to load the dataset and save the video.",
        "type": "comment"
    },
    "612": {
        "file_id": 33,
        "content": "    if not os.path.isfile(dataset_path):\n        print(f'Dataset does not exist at \\n{dataset_path}\\n')\n        exit()\n    with h5py.File(dataset_path, 'r') as root:\n        compressed = root.attrs.get('compress', False)\n        image_dict = dict()\n        for cam_name in root[f'/observations/images/'].keys():\n            image_dict[cam_name] = root[f'/observations/images/{cam_name}'][()]\n        if compressed:\n            compress_len = root['/compress_len'][()]\n    if compressed:\n        for cam_id, cam_name in enumerate(image_dict.keys()):\n            padded_compressed_image_list = image_dict[cam_name]\n            image_list = []\n            for frame_id, padded_compressed_image in enumerate(padded_compressed_image_list):\n                image_len = int(compress_len[cam_id, frame_id])\n                compressed_image = padded_compressed_image\n                image = cv2.imdecode(compressed_image, 1)\n                image_list.append(image)\n            image_dict[cam_name] = image_list\n    return None, None, None, None, image_dict  # Return only the image dict for this application",
        "type": "code",
        "location": "/truncate_data.py:112-135"
    },
    "613": {
        "file_id": 33,
        "content": "This code checks if a dataset exists and reads the compressed image data from it. If compression is enabled, it decompresses the images and stores them in an image dictionary for further processing. The function returns only the image dictionary as the output.",
        "type": "comment"
    },
    "614": {
        "file_id": 33,
        "content": "if __name__ == '__main__':\n    parser = argparse.ArgumentParser(description=\"Compress all HDF5 datasets in a directory.\")\n    parser.add_argument('--dataset_dir', action='store', type=str, required=True, help='Directory containing the uncompressed datasets.')\n    args = parser.parse_args()\n    output_dataset_dir = args.dataset_dir + '_truncated'\n    os.makedirs(output_dataset_dir, exist_ok=True)\n    # Iterate over each file in the directory\n    for filename in tqdm(os.listdir(args.dataset_dir), desc=\"Truncating data\"):\n        if filename.endswith('.hdf5'):\n            input_path = os.path.join(args.dataset_dir, filename)\n            output_path = os.path.join(output_dataset_dir, filename)\n            compress_dataset(input_path, output_path)\n    # After processing all datasets, load and save the video for the first episode\n    print(f'Saving video for episode 0 in {output_dataset_dir}')\n    video_path = os.path.join(output_dataset_dir, 'episode_0_video.mp4')\n    load_and_save_first_episode_video(output_dataset_dir, video_path)",
        "type": "code",
        "location": "/truncate_data.py:138-157"
    },
    "615": {
        "file_id": 33,
        "content": "This code compresses all HDF5 datasets in a specified directory and saves the video for the first episode. It requires the 'act-plus-plus' library and utilizes argument parsing, file iteration, and directory creation/manipulation. The output dataset directory is created as a suffix of the input dataset directory with '_truncated'.",
        "type": "comment"
    },
    "616": {
        "file_id": 34,
        "content": "/utils.py",
        "type": "filepath"
    },
    "617": {
        "file_id": 34,
        "content": "EpisodicDataset class processes data, applies augmentations, handles legacy data, and provides torch tensor compatibility for model usage. It loads images, creates masks, retrieves stats, and includes functions for locating HDF5 files, generating batches, pre/post-processing, sampling poses, calculating means, and setting random seeds.",
        "type": "summary"
    },
    "618": {
        "file_id": 34,
        "content": "import numpy as np\nimport torch\nimport os\nimport h5py\nimport pickle\nimport fnmatch\nimport cv2\nfrom time import time\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision.transforms as transforms\nimport IPython\ne = IPython.embed\ndef flatten_list(l):\n    return [item for sublist in l for item in sublist]\nclass EpisodicDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset_path_list, camera_names, norm_stats, episode_ids, episode_len, chunk_size, policy_class):\n        super(EpisodicDataset).__init__()\n        self.episode_ids = episode_ids\n        self.dataset_path_list = dataset_path_list\n        self.camera_names = camera_names\n        self.norm_stats = norm_stats\n        self.episode_len = episode_len\n        self.chunk_size = chunk_size\n        self.cumulative_len = np.cumsum(self.episode_len)\n        self.max_episode_len = max(episode_len)\n        self.policy_class = policy_class\n        if self.policy_class == 'Diffusion':\n            self.augment_images = True\n        else:\n            self.augment_images = False",
        "type": "code",
        "location": "/utils.py:1-33"
    },
    "619": {
        "file_id": 34,
        "content": "Class EpisodicDataset loads episode data from a list of paths. It can optionally augment images depending on the chosen policy class. The dataset is initialized with the given parameters, including the number of episodes, their IDs, and lengths. It calculates the cumulative length of episodes and checks if the policy class is \"Diffusion\" to determine whether or not to apply image augmentations.",
        "type": "comment"
    },
    "620": {
        "file_id": 34,
        "content": "        self.transformations = None\n        self.__getitem__(0) # initialize self.is_sim and self.transformations\n        self.is_sim = False\n    # def __len__(self):\n    #     return sum(self.episode_len)\n    def _locate_transition(self, index):\n        assert index < self.cumulative_len[-1]\n        episode_index = np.argmax(self.cumulative_len > index) # argmax returns first True index\n        start_ts = index - (self.cumulative_len[episode_index] - self.episode_len[episode_index])\n        episode_id = self.episode_ids[episode_index]\n        return episode_id, start_ts\n    def __getitem__(self, index):\n        episode_id, start_ts = self._locate_transition(index)\n        dataset_path = self.dataset_path_list[episode_id]\n        try:\n            # print(dataset_path)\n            with h5py.File(dataset_path, 'r') as root:\n                try: # some legacy data does not have this attribute\n                    is_sim = root.attrs['sim']\n                except:\n                    is_sim = False\n                compressed = root.attrs.get('compress', False)",
        "type": "code",
        "location": "/utils.py:34-58"
    },
    "621": {
        "file_id": 34,
        "content": "This code initializes transformations and is_sim, defines a function to locate transition based on index, and gets item at specified index by locating the transition using episode ID and start timestamp. It also handles legacy data without certain attributes.",
        "type": "comment"
    },
    "622": {
        "file_id": 34,
        "content": "                if '/base_action' in root:\n                    base_action = root['/base_action'][()]\n                    base_action = preprocess_base_action(base_action)\n                    action = np.concatenate([root['/action'][()], base_action], axis=-1)\n                else:  \n                    action = root['/action'][()]\n                    dummy_base_action = np.zeros([action.shape[0], 2])\n                    action = np.concatenate([action, dummy_base_action], axis=-1)\n                original_action_shape = action.shape\n                episode_len = original_action_shape[0]\n                # get observation at start_ts only\n                qpos = root['/observations/qpos'][start_ts]\n                qvel = root['/observations/qvel'][start_ts]\n                image_dict = dict()\n                for cam_name in self.camera_names:\n                    image_dict[cam_name] = root[f'/observations/images/{cam_name}'][start_ts]\n                if compressed:\n                    for cam_name in image_dict.keys():",
        "type": "code",
        "location": "/utils.py:59-77"
    },
    "623": {
        "file_id": 34,
        "content": "This code block is for processing the input data based on whether a base action is specified or not. If it exists, the base action is preprocessed and concatenated with the given action, otherwise a dummy base action is added before concatenation. It also stores the initial observation and image data at the start timestamp.",
        "type": "comment"
    },
    "624": {
        "file_id": 34,
        "content": "                        decompressed_image = cv2.imdecode(image_dict[cam_name], 1)\n                        image_dict[cam_name] = np.array(decompressed_image)\n                # get all actions after and including start_ts\n                if is_sim:\n                    action = action[start_ts:]\n                    action_len = episode_len - start_ts\n                else:\n                    action = action[max(0, start_ts - 1):] # hack, to make timesteps more aligned\n                    action_len = episode_len - max(0, start_ts - 1) # hack, to make timesteps more aligned\n            # self.is_sim = is_sim\n            padded_action = np.zeros((self.max_episode_len, original_action_shape[1]), dtype=np.float32)\n            padded_action[:action_len] = action\n            is_pad = np.zeros(self.max_episode_len)\n            is_pad[action_len:] = 1\n            padded_action = padded_action[:self.chunk_size]\n            is_pad = is_pad[:self.chunk_size]\n            # new axis for different cameras\n            all_cam_images = []",
        "type": "code",
        "location": "/utils.py:78-99"
    },
    "625": {
        "file_id": 34,
        "content": "This code segment is preprocessing video data for an agent in a simulation. It loads and decompresses images from the dictionary, adjusts actions based on timestamps, pads actions to match the maximum episode length, creates a padding mask, and stores camera images into a list.",
        "type": "comment"
    },
    "626": {
        "file_id": 34,
        "content": "            for cam_name in self.camera_names:\n                all_cam_images.append(image_dict[cam_name])\n            all_cam_images = np.stack(all_cam_images, axis=0)\n            # construct observations\n            image_data = torch.from_numpy(all_cam_images)\n            qpos_data = torch.from_numpy(qpos).float()\n            action_data = torch.from_numpy(padded_action).float()\n            is_pad = torch.from_numpy(is_pad).bool()\n            # channel last\n            image_data = torch.einsum('k h w c -> k c h w', image_data)\n            # augmentation\n            if self.transformations is None:\n                print('Initializing transformations')\n                original_size = image_data.shape[2:]\n                ratio = 0.95\n                self.transformations = [\n                    transforms.RandomCrop(size=[int(original_size[0] * ratio), int(original_size[1] * ratio)]),\n                    transforms.Resize(original_size, antialias=True),\n                    transforms.RandomRotation(degrees=[-5.0, 5.0], expand=False),",
        "type": "code",
        "location": "/utils.py:100-121"
    },
    "627": {
        "file_id": 34,
        "content": "The code reads images from multiple camera sources, stacks them into a single numpy array, and converts the arrays to torch tensors. It then rearranges the image tensor's dimensions for compatibility with the model, applies optional augmentations such as cropping and rotation, and assigns boolean values to indicate padding positions.",
        "type": "comment"
    },
    "628": {
        "file_id": 34,
        "content": "                    transforms.ColorJitter(brightness=0.3, contrast=0.4, saturation=0.5) #, hue=0.08)\n                ]\n            if self.augment_images:\n                for transform in self.transformations:\n                    image_data = transform(image_data)\n            # normalize image and change dtype to float\n            image_data = image_data / 255.0\n            if self.policy_class == 'Diffusion':\n                # normalize to [-1, 1]\n                action_data = ((action_data - self.norm_stats[\"action_min\"]) / (self.norm_stats[\"action_max\"] - self.norm_stats[\"action_min\"])) * 2 - 1\n            else:\n                # normalize to mean 0 std 1\n                action_data = (action_data - self.norm_stats[\"action_mean\"]) / self.norm_stats[\"action_std\"]\n            qpos_data = (qpos_data - self.norm_stats[\"qpos_mean\"]) / self.norm_stats[\"qpos_std\"]\n        except:\n            print(f'Error loading {dataset_path} in __getitem__')\n            quit()\n        # print(image_data.dtype, qpos_data.dtype, action_data.dtype, is_pad.dtype)",
        "type": "code",
        "location": "/utils.py:122-145"
    },
    "629": {
        "file_id": 34,
        "content": "The code applies transformations to image data, normalizes the image and action data based on policy class, and adjusts qpos data based on mean and std. It also handles any potential errors while loading the dataset.",
        "type": "comment"
    },
    "630": {
        "file_id": 34,
        "content": "        return image_data, qpos_data, action_data, is_pad\ndef get_norm_stats(dataset_path_list):\n    all_qpos_data = []\n    all_action_data = []\n    all_episode_len = []\n    for dataset_path in dataset_path_list:\n        try:\n            with h5py.File(dataset_path, 'r') as root:\n                qpos = root['/observations/qpos'][()]\n                qvel = root['/observations/qvel'][()]\n                if '/base_action' in root:\n                    base_action = root['/base_action'][()]\n                    base_action = preprocess_base_action(base_action)\n                    action = np.concatenate([root['/action'][()], base_action], axis=-1)\n                else:\n                    action = root['/action'][()]\n                    dummy_base_action = np.zeros([action.shape[0], 2])\n                    action = np.concatenate([action, dummy_base_action], axis=-1)\n        except Exception as e:\n            print(f'Error loading {dataset_path} in get_norm_stats')\n            print(e)\n            quit()\n        all_qpos_data.append(torch.from_numpy(qpos))",
        "type": "code",
        "location": "/utils.py:146-171"
    },
    "631": {
        "file_id": 34,
        "content": "This function, \"get_norm_stats\", takes a list of dataset paths and returns image data, qpos data, action data, and an indicator whether the pad is needed or not. It first initializes empty lists for all_qpos_data, all_action_data, and all_episode_len. Then, it iterates over each dataset path in the list. For each path, it opens the HDF5 file using 'r' mode and extracts qpos and qvel data from specific paths within the file. If a '/base_action' path exists, it retrieves base_action data and preprocesses it before concatenating with action data. Otherwise, it assumes dummy base_action and performs concatenation. The extracted data is appended to their respective lists, but if an error occurs during loading, the function prints an error message and quits.",
        "type": "comment"
    },
    "632": {
        "file_id": 34,
        "content": "        all_action_data.append(torch.from_numpy(action))\n        all_episode_len.append(len(qpos))\n    all_qpos_data = torch.cat(all_qpos_data, dim=0)\n    all_action_data = torch.cat(all_action_data, dim=0)\n    # normalize action data\n    action_mean = all_action_data.mean(dim=[0]).float()\n    action_std = all_action_data.std(dim=[0]).float()\n    action_std = torch.clip(action_std, 1e-2, np.inf) # clipping\n    # normalize qpos data\n    qpos_mean = all_qpos_data.mean(dim=[0]).float()\n    qpos_std = all_qpos_data.std(dim=[0]).float()\n    qpos_std = torch.clip(qpos_std, 1e-2, np.inf) # clipping\n    action_min = all_action_data.min(dim=0).values.float()\n    action_max = all_action_data.max(dim=0).values.float()\n    eps = 0.0001\n    stats = {\"action_mean\": action_mean.numpy(), \"action_std\": action_std.numpy(),\n             \"action_min\": action_min.numpy() - eps,\"action_max\": action_max.numpy() + eps,\n             \"qpos_mean\": qpos_mean.numpy(), \"qpos_std\": qpos_std.numpy(),\n             \"example_qpos\": qpos}\n    return stats, all_episode_len",
        "type": "code",
        "location": "/utils.py:172-196"
    },
    "633": {
        "file_id": 34,
        "content": "This code is processing and normalizing data for training in a machine learning context. It appends action and qpos data, normalizes the action and qpos data by calculating their means, standard deviations, and clipping them to avoid large values, and stores these statistics along with minimum and maximum action values and an example qpos. Finally, it returns these statistics and all episode lengths.",
        "type": "comment"
    },
    "634": {
        "file_id": 34,
        "content": "def find_all_hdf5(dataset_dir, skip_mirrored_data):\n    hdf5_files = []\n    for root, dirs, files in os.walk(dataset_dir):\n        for filename in fnmatch.filter(files, '*.hdf5'):\n            if 'features' in filename: continue\n            if skip_mirrored_data and 'mirror' in filename:\n                continue\n            hdf5_files.append(os.path.join(root, filename))\n    print(f'Found {len(hdf5_files)} hdf5 files')\n    return hdf5_files\ndef BatchSampler(batch_size, episode_len_l, sample_weights):\n    sample_probs = np.array(sample_weights) / np.sum(sample_weights) if sample_weights is not None else None\n    sum_dataset_len_l = np.cumsum([0] + [np.sum(episode_len) for episode_len in episode_len_l])\n    while True:\n        batch = []\n        for _ in range(batch_size):\n            episode_idx = np.random.choice(len(episode_len_l), p=sample_probs)\n            step_idx = np.random.randint(sum_dataset_len_l[episode_idx], sum_dataset_len_l[episode_idx + 1])\n            batch.append(step_idx)\n        yield batch",
        "type": "code",
        "location": "/utils.py:198-218"
    },
    "635": {
        "file_id": 34,
        "content": "The code provides two functions: \"find_all_hdf5\" and \"BatchSampler\". The first function searches for all HDF5 files in a specified directory, excluding any with 'features' in their name or 'mirror' if skipping mirrored data is set. It then returns the list of found files. The second function, BatchSampler, generates batches of samples from a list of episode lengths and sample weights (if provided). It randomly selects an episode, a step within that episode, and appends it to the batch until the desired batch size is reached.",
        "type": "comment"
    },
    "636": {
        "file_id": 34,
        "content": "def load_data(dataset_dir_l, name_filter, camera_names, batch_size_train, batch_size_val, chunk_size, skip_mirrored_data=False, load_pretrain=False, policy_class=None, stats_dir_l=None, sample_weights=None, train_ratio=0.99):\n    if type(dataset_dir_l) == str:\n        dataset_dir_l = [dataset_dir_l]\n    dataset_path_list_list = [find_all_hdf5(dataset_dir, skip_mirrored_data) for dataset_dir in dataset_dir_l]\n    num_episodes_0 = len(dataset_path_list_list[0])\n    dataset_path_list = flatten_list(dataset_path_list_list)\n    dataset_path_list = [n for n in dataset_path_list if name_filter(n)]\n    num_episodes_l = [len(dataset_path_list) for dataset_path_list in dataset_path_list_list]\n    num_episodes_cumsum = np.cumsum(num_episodes_l)\n    # obtain train test split on dataset_dir_l[0]\n    shuffled_episode_ids_0 = np.random.permutation(num_episodes_0)\n    train_episode_ids_0 = shuffled_episode_ids_0[:int(train_ratio * num_episodes_0)]\n    val_episode_ids_0 = shuffled_episode_ids_0[int(train_ratio * num_episodes_0):]",
        "type": "code",
        "location": "/utils.py:220-233"
    },
    "637": {
        "file_id": 34,
        "content": "This function loads data from one or multiple directories, applying a name filter and splitting the data into training and validation sets. It also supports skipping mirrored data and loading pre-trained data. The train/val split is done based on a provided ratio, and the data is shuffled randomly before splitting.",
        "type": "comment"
    },
    "638": {
        "file_id": 34,
        "content": "    train_episode_ids_l = [train_episode_ids_0] + [np.arange(num_episodes) + num_episodes_cumsum[idx] for idx, num_episodes in enumerate(num_episodes_l[1:])]\n    val_episode_ids_l = [val_episode_ids_0]\n    train_episode_ids = np.concatenate(train_episode_ids_l)\n    val_episode_ids = np.concatenate(val_episode_ids_l)\n    print(f'\\n\\nData from: {dataset_dir_l}\\n- Train on {[len(x) for x in train_episode_ids_l]} episodes\\n- Test on {[len(x) for x in val_episode_ids_l]} episodes\\n\\n')\n    # obtain normalization stats for qpos and action\n    # if load_pretrain:\n    #     with open(os.path.join('/home/zfu/interbotix_ws/src/act/ckpts/pretrain_all', 'dataset_stats.pkl'), 'rb') as f:\n    #         norm_stats = pickle.load(f)\n    #     print('Loaded pretrain dataset stats')\n    _, all_episode_len = get_norm_stats(dataset_path_list)\n    train_episode_len_l = [[all_episode_len[i] for i in train_episode_ids] for train_episode_ids in train_episode_ids_l]\n    val_episode_len_l = [[all_episode_len[i] for i in val_episode_ids] for val_episode_ids in val_episode_ids_l]",
        "type": "code",
        "location": "/utils.py:234-247"
    },
    "639": {
        "file_id": 34,
        "content": "Code generates train and validation episode IDs for multiple datasets, concatenates them, and prints details about the data. It also loads normalization stats for qpos and action (if load_pretrain is True) from a specific file path. The code then calculates the length of each episode for training and validation sets based on all_episode_len list.",
        "type": "comment"
    },
    "640": {
        "file_id": 34,
        "content": "    train_episode_len = flatten_list(train_episode_len_l)\n    val_episode_len = flatten_list(val_episode_len_l)\n    if stats_dir_l is None:\n        stats_dir_l = dataset_dir_l\n    elif type(stats_dir_l) == str:\n        stats_dir_l = [stats_dir_l]\n    norm_stats, _ = get_norm_stats(flatten_list([find_all_hdf5(stats_dir, skip_mirrored_data) for stats_dir in stats_dir_l]))\n    print(f'Norm stats from: {stats_dir_l}')\n    batch_sampler_train = BatchSampler(batch_size_train, train_episode_len_l, sample_weights)\n    batch_sampler_val = BatchSampler(batch_size_val, val_episode_len_l, None)\n    # print(f'train_episode_len: {train_episode_len}, val_episode_len: {val_episode_len}, train_episode_ids: {train_episode_ids}, val_episode_ids: {val_episode_ids}')\n    # construct dataset and dataloader\n    train_dataset = EpisodicDataset(dataset_path_list, camera_names, norm_stats, train_episode_ids, train_episode_len, chunk_size, policy_class)\n    val_dataset = EpisodicDataset(dataset_path_list, camera_names, norm_stats, val_episode_ids, val_episode_len, chunk_size, policy_class)",
        "type": "code",
        "location": "/utils.py:248-264"
    },
    "641": {
        "file_id": 34,
        "content": "This code block initializes training and validation episode lengths, checks the stats directory type, fetches normalization statistics from HDF5 files, creates batch samplers for training and validation sets, constructs EpisodicDataset instances for training and validation data.",
        "type": "comment"
    },
    "642": {
        "file_id": 34,
        "content": "    train_num_workers = (8 if os.getlogin() == 'zfu' else 16) if train_dataset.augment_images else 2\n    val_num_workers = 8 if train_dataset.augment_images else 2\n    print(f'Augment images: {train_dataset.augment_images}, train_num_workers: {train_num_workers}, val_num_workers: {val_num_workers}')\n    train_dataloader = DataLoader(train_dataset, batch_sampler=batch_sampler_train, pin_memory=True, num_workers=train_num_workers, prefetch_factor=2)\n    val_dataloader = DataLoader(val_dataset, batch_sampler=batch_sampler_val, pin_memory=True, num_workers=val_num_workers, prefetch_factor=2)\n    return train_dataloader, val_dataloader, norm_stats, train_dataset.is_sim\ndef calibrate_linear_vel(base_action, c=None):\n    if c is None:\n        c = 0.0 # 0.19\n    v = base_action[..., 0]\n    w = base_action[..., 1]\n    base_action = base_action.copy()\n    base_action[..., 0] = v - c * w\n    return base_action\ndef smooth_base_action(base_action):\n    return np.stack([\n        np.convolve(base_action[:, i], np.ones(5)/5, mode='same') for i in range(base_action.shape[1])",
        "type": "code",
        "location": "/utils.py:265-284"
    },
    "643": {
        "file_id": 34,
        "content": "This code sets the number of workers for training and validation data loaders based on whether images are being augmented or not. It also defines a function to calibrate linear velocity, smooths the base action using convolution with a moving average filter, and returns the train and validation dataloaders along with other variables.",
        "type": "comment"
    },
    "644": {
        "file_id": 34,
        "content": "    ], axis=-1).astype(np.float32)\ndef preprocess_base_action(base_action):\n    # base_action = calibrate_linear_vel(base_action)\n    base_action = smooth_base_action(base_action)\n    return base_action\ndef postprocess_base_action(base_action):\n    linear_vel, angular_vel = base_action\n    linear_vel *= 1.0\n    angular_vel *= 1.0\n    # angular_vel = 0\n    # if np.abs(linear_vel) < 0.05:\n    #     linear_vel = 0\n    return np.array([linear_vel, angular_vel])\n### env utils\ndef sample_box_pose():\n    x_range = [0.0, 0.2]\n    y_range = [0.4, 0.6]\n    z_range = [0.05, 0.05]\n    ranges = np.vstack([x_range, y_range, z_range])\n    cube_position = np.random.uniform(ranges[:, 0], ranges[:, 1])\n    cube_quat = np.array([1, 0, 0, 0])\n    return np.concatenate([cube_position, cube_quat])\ndef sample_insertion_pose():\n    # Peg\n    x_range = [0.1, 0.2]\n    y_range = [0.4, 0.6]\n    z_range = [0.05, 0.05]\n    ranges = np.vstack([x_range, y_range, z_range])\n    peg_position = np.random.uniform(ranges[:, 0], ranges[:, 1])\n    peg_quat = np.array([1, 0, 0, 0])",
        "type": "code",
        "location": "/utils.py:285-324"
    },
    "645": {
        "file_id": 34,
        "content": "This code defines several functions for preprocessing and postprocessing base actions, as well as sampling random poses for objects. It uses numpy array manipulations and random sampling to accomplish these tasks. The calibration and smoothing of the base action are used to refine input data before it is passed on or returned from a function. The two pose-sampling functions generate random positions and orientations for an object (cube or peg) within specified ranges.",
        "type": "comment"
    },
    "646": {
        "file_id": 34,
        "content": "    peg_pose = np.concatenate([peg_position, peg_quat])\n    # Socket\n    x_range = [-0.2, -0.1]\n    y_range = [0.4, 0.6]\n    z_range = [0.05, 0.05]\n    ranges = np.vstack([x_range, y_range, z_range])\n    socket_position = np.random.uniform(ranges[:, 0], ranges[:, 1])\n    socket_quat = np.array([1, 0, 0, 0])\n    socket_pose = np.concatenate([socket_position, socket_quat])\n    return peg_pose, socket_pose\n### helper functions\ndef compute_dict_mean(epoch_dicts):\n    result = {k: None for k in epoch_dicts[0]}\n    num_items = len(epoch_dicts)\n    for k in result:\n        value_sum = 0\n        for epoch_dict in epoch_dicts:\n            value_sum += epoch_dict[k]\n        result[k] = value_sum / num_items\n    return result\ndef detach_dict(d):\n    new_d = dict()\n    for k, v in d.items():\n        new_d[k] = v.detach()\n    return new_d\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    np.random.seed(seed)",
        "type": "code",
        "location": "/utils.py:325-360"
    },
    "647": {
        "file_id": 34,
        "content": "Function: compute_dict_mean\nPurpose: Calculate the mean of values for each key in a list of dictionaries.\n\nFunction: detach_dict\nPurpose: Create a new dictionary where all values are detached from their current computation graph.\n\nFunction: set_seed\nPurpose: Set random seed for both PyTorch and NumPy to ensure reproducible results.",
        "type": "comment"
    },
    "648": {
        "file_id": 35,
        "content": "/vinn_cache_feature.py",
        "type": "filepath"
    },
    "649": {
        "file_id": 35,
        "content": "The code imports libraries, sets parameters, initializes models and preprocesses images for feature extraction. It performs inference, saves features to an HDF5 file, converts tensors to NumPy arrays, and prints the total time taken using argument parser.",
        "type": "summary"
    },
    "650": {
        "file_id": 35,
        "content": "import torch\nimport argparse\nimport pathlib\nfrom torch import nn\nimport torchvision\nimport os\nimport time\nimport h5py\nimport h5py\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport cv2\nimport numpy as np\nimport IPython\ne = IPython.embed\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\ndef expand_greyscale(t):\n    return t.expand(3, -1, -1)\ndef main(args):\n    #################################################\n    batch_size = 256\n    #################################################\n    ckpt_path = args.ckpt_path\n    dataset_dir = args.dataset_dir\n    ckpt_name = pathlib.PurePath(ckpt_path).name\n    dataset_name = ckpt_name.split('-')[1]\n    repr_type = ckpt_name.split('-')[0]\n    seed = int(ckpt_name.split('-')[-1][:-3])\n    if 'cotrain' in ckpt_name:\n        repr_type += '_cotrain'\n    episode_idxs = [int(name.split('_')[1].split('.')[0]) for name in os.listdir(dataset_dir) if ('.hdf5' in name) and ('features' not in name)]",
        "type": "code",
        "location": "/vinn_cache_feature.py:1-44"
    },
    "651": {
        "file_id": 35,
        "content": "This code imports necessary libraries, defines functions for chunking lists and expanding greyscale images, and sets parameters such as batch size. It also takes command-line arguments for the checkpoint path and dataset directory, extracts relevant information from the checkpoint name, and lists all episode indexes in the dataset.",
        "type": "comment"
    },
    "652": {
        "file_id": 35,
        "content": "    episode_idxs.sort()\n    assert len(episode_idxs) == episode_idxs[-1] + 1 # no holes\n    num_episodes = len(episode_idxs)\n    feature_extractors = {}\n    for episode_idx in range(num_episodes):\n        # load all images\n        print(f'loading data')\n        dataset_path = os.path.join(dataset_dir, f'episode_{episode_idx}.hdf5')\n        with h5py.File(dataset_path, 'r') as root:\n            image_dict = {}\n            camera_names = list(root[f'/observations/images/'].keys())\n            print(f'Camera names: {camera_names}')\n            for cam_name in camera_names:\n                image = root[f'/observations/images/{cam_name}'][:]\n                uncompressed_image = []\n                for im in image:\n                    im = np.array(cv2.imdecode(im, 1))\n                    uncompressed_image.append(im)\n                image = np.stack(uncompressed_image, axis=0)\n                image_dict[cam_name] = image\n        print(f'loading model')\n        # load pretrain nets after cam names are known\n        if not feature_extractors:",
        "type": "code",
        "location": "/vinn_cache_feature.py:45-72"
    },
    "653": {
        "file_id": 35,
        "content": "Loading data and models for each episode, ensuring no holes in the episode indices, and creating feature extractors. The code first checks if there are any existing feature extractors, then loads images and models for each camera name within the dataset, and stores them in a dictionary.",
        "type": "comment"
    },
    "654": {
        "file_id": 35,
        "content": "            for cam_name in camera_names:\n                resnet = torchvision.models.resnet18(pretrained=True)\n                loading_status = resnet.load_state_dict(torch.load(ckpt_path.replace('DUMMY', cam_name)))\n                print(cam_name, loading_status)\n                resnet = nn.Sequential(*list(resnet.children())[:-1])\n                resnet = resnet.cuda()\n                resnet.eval()\n                feature_extractors[cam_name] = resnet\n        # inference with resnet\n        feature_dict = {}\n        for cam_name, images in image_dict.items():\n            # Preprocess images\n            image_size = 120 # TODO NOTICE: reduced resolution\n            transform = transforms.Compose([\n                transforms.Resize(image_size),  # will scale the image\n                transforms.CenterCrop(image_size),\n                transforms.ToTensor(),\n                transforms.Lambda(expand_greyscale),\n                transforms.Normalize(\n                    mean=torch.tensor([0.485, 0.456, 0.406]),",
        "type": "code",
        "location": "/vinn_cache_feature.py:73-93"
    },
    "655": {
        "file_id": 35,
        "content": "This code initializes a ResNet18 model for each camera name, loads the checkpoint file with the corresponding camera name, modifies the model, and stores it in feature_extractors. Then, it preprocesses images using specified transforms and normalization before passing them to the model for inference.",
        "type": "comment"
    },
    "656": {
        "file_id": 35,
        "content": "                    std=torch.tensor([0.229, 0.224, 0.225])),\n            ])\n            processed_images = []\n            for image in tqdm(images):\n                image = Image.fromarray(image)\n                image = transform(image)\n                processed_images.append(image)\n            processed_images = torch.stack(processed_images).cuda()\n            # query the model\n            all_features = []\n            with torch.inference_mode():\n                for batch in chunks(processed_images, batch_size):\n                    print('inference')\n                    features = feature_extractors[cam_name](batch)\n                    features = features.squeeze(axis=3).squeeze(axis=2)\n                    all_features.append(features)\n            all_features = torch.cat(all_features, axis=0)\n            max_timesteps = all_features.shape[0]\n            feature_dict[cam_name] = all_features\n            # TODO START diagnostics\n            # first_image = images[0]\n            # first_processed_image = processed_images[0].cpu().numpy()",
        "type": "code",
        "location": "/vinn_cache_feature.py:94-117"
    },
    "657": {
        "file_id": 35,
        "content": "This code processes images, queries a model for features, and stores the extracted features in a dictionary. It uses torch.tensor for standardization, Image.fromarray to convert image to PIL image, transforms images, stacks them, performs inference mode, extracts features from each batch of processed images, concatenates them into all_features list, and finally stores them in feature_dict.",
        "type": "comment"
    },
    "658": {
        "file_id": 35,
        "content": "            # first_feature = all_features[0].cpu().numpy()\n            # import numpy as np\n            # np.save('first_image.npy', first_image)\n            # np.save('first_processed_image.npy', first_processed_image)\n            # np.save('first_feature.npy', first_feature)\n            # torch.save(resnet.state_dict(), 'rn.ckpt')\n            # e()\n            # exit()\n            # TODO END diagnostics\n        # save\n        dataset_path = os.path.join(dataset_dir, f'{repr_type}_features_seed{seed}_episode_{episode_idx}.hdf5')\n        print(dataset_path)\n        # HDF5\n        t0 = time.time()\n        with h5py.File(dataset_path, 'w', rdcc_nbytes=1024 ** 2 * 2) as root:\n            features = root.create_group('features')\n            for cam_name, array in feature_dict.items():\n                cam_feature = features.create_dataset(cam_name, (max_timesteps, 512))\n                features[cam_name][...] = array.cpu().numpy()\n        print(f'Saving: {time.time() - t0:.1f} secs\\n')\nif __name__ == '__main__':",
        "type": "code",
        "location": "/vinn_cache_feature.py:118-142"
    },
    "659": {
        "file_id": 35,
        "content": "The code is saving features to an HDF5 file. It creates a group called 'features' within the file and then saves feature data for each camera name in the feature_dict as datasets within the 'features' group. The feature data is converted from PyTorch tensors to NumPy arrays before being saved, and the total time taken to save the features is printed.",
        "type": "comment"
    },
    "660": {
        "file_id": 35,
        "content": "    parser = argparse.ArgumentParser(description='cache features')\n    parser.add_argument('--ckpt_path', type=str, required=True, help='ckpt_path')\n    parser.add_argument('--dataset_dir', type=str, required=True, help='dataset_dir')\n    args = parser.parse_args()\n    main(args)",
        "type": "code",
        "location": "/vinn_cache_feature.py:143-148"
    },
    "661": {
        "file_id": 35,
        "content": "This code sets up an argument parser, adds arguments for ckpt_path and dataset_dir with necessary types and requirements, and then parses the given arguments to be used in the main function.",
        "type": "comment"
    },
    "662": {
        "file_id": 36,
        "content": "/vinn_eval.py",
        "type": "filepath"
    },
    "663": {
        "file_id": 36,
        "content": "This code defines a function for nearest neighbor calculation, performs rollouts, and preprocesses features for image classification tasks. It uses command-line arguments to run the script with specific directories and checkpoints.",
        "type": "summary"
    },
    "664": {
        "file_id": 36,
        "content": "import torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\nimport h5py\nimport pathlib\nimport os\nimport argparse\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torchvision\nfrom torchvision import transforms\n# from visualize_episodes import visualize_joints\nfrom utils import set_seed, sample_box_pose\n# from imitate_episodes import get_image\nfrom sim_env import BOX_POSE\nfrom constants import DT\nfrom imitate_episodes import save_videos\nfrom einops import rearrange\nimport time\nDT = 0.02\nimport IPython\ne = IPython.embed\n# modified from https://github.com/jyopari/VINN/blob/main/nearest-neighbor-eval/handle_nn.ipynb\ndef calculate_nearest_neighbors(curr_feature, support_inputs, support_targets, k, state_weight):\n    has_skip = len(support_targets.shape) == 3\n    if has_skip: # when there is action skip\n        num_targets, skip, a_dim = support_targets.shape\n        support_targets = support_targets.view((num_targets, -1))\n    curr_vis_feature, curr_s_feature = curr_feature\n    support_vis_feature, support_s_feature = support_inputs",
        "type": "code",
        "location": "/vinn_eval.py:1-35"
    },
    "665": {
        "file_id": 36,
        "content": "This code imports necessary libraries and defines a function that calculates nearest neighbors for a given feature. The function takes the current feature, support inputs, support targets, number of neighbors to consider (k), and state weight as input parameters. It also handles cases where there is an action skip in the support targets by reshaping them before processing. The code defines separate features for visual and spatial modalities (curr_vis_feature, curr_s_feature, support_vis_feature, support_s_feature).",
        "type": "comment"
    },
    "666": {
        "file_id": 36,
        "content": "    pairwise_dist_vis = torch.norm(curr_vis_feature - support_vis_feature, dim=1).unsqueeze(0)\n    pairwise_dist_s = torch.norm(curr_s_feature - support_s_feature, dim=1).unsqueeze(0)\n    pairwise_dist = pairwise_dist_vis + pairwise_dist_s * state_weight\n    sorted_dist, index = torch.sort(pairwise_dist, dim=1) # sort the support axis\n    permuted_support_targets = support_targets[index]\n    topk_dist = pairwise_dist[:, :k]\n    topk_support_targets = permuted_support_targets[:, :k]\n    weights = F.softmax(-topk_dist, dim=1)\n    weighted_support_targets = weights.unsqueeze(2) * topk_support_targets\n    prediction = torch.sum(weighted_support_targets, dim=1)\n    if has_skip:\n        num_predictions = prediction.shape[0]\n        prediction = prediction.reshape((num_predictions, skip, a_dim))\n    return prediction\ndef main(args):\n    # TODO ######################\n    k = None # for scripted box transfer\n    skip = 100\n    real_robot = True\n    save_episode = True\n    # TODO ######################\n    onscreen_cam = 'main'",
        "type": "code",
        "location": "/vinn_eval.py:37-63"
    },
    "667": {
        "file_id": 36,
        "content": "The code calculates pairwise distances between current and support features, sorts them, and assigns weights to the top-k distances. It then uses these weights to create a weighted sum of support targets as the prediction. The function takes arguments 'args', but they are not used in this snippet. Additionally, it allows skipping predictions for every 100th frame with 'has_skip' flag.",
        "type": "comment"
    },
    "668": {
        "file_id": 36,
        "content": "    state_dim = 14\n    dataset_dir = args['dataset_dir']\n    onscreen_render = args['onscreen_render']\n    ckpt_dir = args['ckpt_dir']\n    model_dir = args['model_dir']\n    task_name = args['task_name']\n    if 'insertion' in task_name:\n        sim_episode_len = 400\n        env_max_reward = 4\n        ks = [None]\n    elif 'transfer_cube' in task_name:\n        sim_episode_len = 400\n        env_max_reward = 4\n        ks = [1, 1, 1]\n        if 'human' in dataset_dir:\n            state_weight = 5\n        else:\n            state_weight = 10\n        print(f'{state_weight=}')\n    elif task_name == 'ziploc_slide':\n        env_max_reward = 1\n        ks = [71]\n        state_weight = 0\n    elif task_name == 'aloha_mobile_wipe_wine':\n        sim_episode_len = 1300\n        env_max_reward = 4\n        ks = [2, 2, 2]\n        state_weight = 5\n        print(f'{state_weight=}')\n    else:\n        raise NotImplementedError\n    model_name = pathlib.PurePath(model_dir).name\n    seed = int(model_name.split('-')[-1][:-3])\n    repr_type = 'byol'\n    if 'cotrain' in model_name:",
        "type": "code",
        "location": "/vinn_eval.py:64-101"
    },
    "669": {
        "file_id": 36,
        "content": "This code sets various parameters and configurations for different tasks based on the task name provided. It assigns specific episode lengths, maximum rewards, kernel sizes (ks), and state weights depending on the task type. If the task is not implemented, it raises a NotImplementedError. The model name's last part before the file extension is used as the seed, and the representation type is set to 'byol'. For models with 'cotrain' in their names, it assigns the repr_type accordingly.",
        "type": "comment"
    },
    "670": {
        "file_id": 36,
        "content": "        repr_type += '_cotrain'\n    e() # make sure!\n    k = ks[seed]\n    if real_robot:\n        BASE_DELAY = 15\n        query_freq = skip - BASE_DELAY\n    # load train data\n    vis_features = []\n    state_features = []\n    Y = []\n    for episode_id in range(0, 40):\n        dataset_path = os.path.join(dataset_dir, f'episode_{episode_id}.hdf5')\n        with h5py.File(dataset_path, 'r') as root:\n            action = root['/action'][:]\n            base_action = root['/base_action'][:]\n            action = np.concatenate([action, base_action], axis=1)\n            camera_names = list(root[f'/observations/images/'].keys())\n        # Visual feature\n        all_cam_feature = []\n        for cam_name in camera_names:\n            feature_dataset_path = os.path.join(dataset_dir, f'{repr_type}_features_seed{seed}_episode_{episode_id}.hdf5')\n            with h5py.File(feature_dataset_path, 'r') as root:\n                cam_feature = root[f'/features/{cam_name}'][:]\n                all_cam_feature.append(cam_feature)\n        vis_fea = np.concatenate(all_cam_feature, axis=1)",
        "type": "code",
        "location": "/vinn_eval.py:102-130"
    },
    "671": {
        "file_id": 36,
        "content": "This code loads train data by iterating over 40 episodes. It retrieves action, base_action, and camera names from a dataset file. For each episode, it concatenates the visual features of all cameras into 'vis_fea'. The repr_type is extended with '_cotrain', and BASE_DELAY is set to 15 for real_robot cases.",
        "type": "comment"
    },
    "672": {
        "file_id": 36,
        "content": "        ## State feature\n        dataset_path = os.path.join(dataset_dir, f'episode_{episode_id}.hdf5')\n        with h5py.File(dataset_path, 'r') as root:\n            s_fea = root['/observations/qpos'][:]\n        # stack actions together\n        eps_len = len(action)\n        indices = np.tile(np.arange(skip), eps_len).reshape(eps_len, skip) # each row is 0, 1, ... skip\n        offset = np.expand_dims(np.arange(eps_len), axis=1)\n        indices = indices + offset # row1: 0, 1, ... skip; row2: 1, 2, ... skip+1\n        # indices will exceed eps_len, thus clamp to eps_len-1\n        indices = np.clip(indices, 0, eps_len-1)\n        # stack action\n        action = action[indices] # new shape: eps_len, skip, a_dim\n        vis_features.append(vis_fea)\n        state_features.append(s_fea)\n        Y.append(action)\n    vis_features = np.concatenate(vis_features)\n    state_features  = np.concatenate(state_features)\n    Y = np.concatenate(Y)\n    train_inputs = [torch.from_numpy(vis_features).cuda(), torch.from_numpy(state_features).cuda()]",
        "type": "code",
        "location": "/vinn_eval.py:132-154"
    },
    "673": {
        "file_id": 36,
        "content": "This code reads episode data from a file, stacks actions together, appends them to feature lists, and then concatenates the feature lists. Finally, it creates torch tensors for training inputs.",
        "type": "comment"
    },
    "674": {
        "file_id": 36,
        "content": "    train_targets = torch.from_numpy(Y).cuda()\n    set_seed(1000)\n    feature_extractors = {}\n    for cam_name in camera_names:\n        resnet = torchvision.models.resnet18(pretrained=True)\n        loading_status = resnet.load_state_dict(torch.load(model_dir.replace('DUMMY', cam_name)))\n        print(cam_name, loading_status)\n        resnet = nn.Sequential(*list(resnet.children())[:-1])\n        resnet = resnet.cuda()\n        resnet.eval()\n        feature_extractors[cam_name] = resnet\n    # load environment\n    if real_robot:\n        from aloha_scripts.real_env import make_real_env #### TODO TODO\n        env = make_real_env(init_node=True, setup_robots=True, setup_base=True)\n        max_timesteps = sim_episode_len\n        camera_names = ['cam_high', 'cam_left_wrist', 'cam_right_wrist']\n    else:\n        from sim_env import make_sim_env\n        env = make_sim_env(task_name)\n        max_timesteps = sim_episode_len\n    num_rollouts = 50\n    episode_returns = []\n    max_rewards = []\n    for rollout_id in range(num_rollouts):",
        "type": "code",
        "location": "/vinn_eval.py:155-185"
    },
    "675": {
        "file_id": 36,
        "content": "The code initializes feature extractors for each camera, loads the environment based on real_robot flag, and starts a loop to perform rollouts. It creates episode returns and maximum rewards lists for tracking performance metrics during the rollouts.",
        "type": "comment"
    },
    "676": {
        "file_id": 36,
        "content": "        ### set task\n        BOX_POSE[0] = sample_box_pose() # used in sim reset\n        ts = env.reset()\n        ### evaluation loop\n        qpos_history = torch.zeros((1, max_timesteps, state_dim)).cuda()\n        image_list = [] # for visualization\n        qpos_list = []\n        target_qpos_list = []\n        rewards = []\n        with torch.inference_mode():\n            for t in range(sim_episode_len):\n                start_time = time.time()\n                if t % 100 == 0: print(t)\n                if t % query_freq == 0:\n                    ### process previous timestep to get qpos and image_list\n                    obs = ts.observation\n                    if 'images' in obs:\n                        image_list.append(obs['images'])\n                    else:\n                        image_list.append({'main': obs['image']})\n                    qpos_numpy = np.array(obs['qpos'])\n                    # qpos = pre_process(qpos_numpy)\n                    qpos = torch.from_numpy(qpos_numpy).float().cuda().unsqueeze(0)",
        "type": "code",
        "location": "/vinn_eval.py:186-209"
    },
    "677": {
        "file_id": 36,
        "content": "This code sets up a task, resets the environment, and enters an evaluation loop. It collects data for visualization, including qpos and images, and stores them in lists. The code is performing these actions at specific intervals based on the provided conditions.",
        "type": "comment"
    },
    "678": {
        "file_id": 36,
        "content": "                    qpos_history[:, t] = qpos\n                    _, curr_image_raw = get_image(ts, camera_names)\n                    image_size = 120\n                    transform = transforms.Compose([\n                        transforms.Resize(image_size),  # will scale the image\n                        transforms.CenterCrop(image_size),\n                        transforms.ToTensor(),\n                        transforms.Lambda(expand_greyscale),\n                        transforms.Normalize(\n                            mean=torch.tensor([0.485, 0.456, 0.406]),\n                            std=torch.tensor([0.229, 0.224, 0.225])),\n                    ])\n                    all_cam_features = []\n                    for cam_id, curr_image in enumerate(curr_image_raw):\n                        curr_image = Image.fromarray(curr_image) # TODO only one camera\n                        curr_image = transform(curr_image)\n                        curr_image = curr_image.unsqueeze(dim=0).cuda()\n                        curr_image_feature = feature_extractors[camera_names[cam_id]](curr_image)",
        "type": "code",
        "location": "/vinn_eval.py:210-229"
    },
    "679": {
        "file_id": 36,
        "content": "This code segment processes an image for a robotics task. It stores the current qpos in history, retrieves and preprocesses raw camera images using transforms such as resizing, cropping, normalization, and tensor conversion. It then collects features from each camera using respective feature extractors and stores them in all_cam_features.",
        "type": "comment"
    },
    "680": {
        "file_id": 36,
        "content": "                        curr_image_feature = curr_image_feature.squeeze(3).squeeze(2)\n                        all_cam_features.append(curr_image_feature)\n                    curr_image_feature = torch.cat(all_cam_features, dim=1)\n                    ### Visual feature\n                    # curr_feature = curr_image_feature\n                    ### State feature\n                    # curr_feature = qpos\n                    ### Both features\n                    curr_feature = [curr_image_feature, qpos]\n                    action = calculate_nearest_neighbors(curr_feature, train_inputs, train_targets, k, state_weight) # TODO use this\n                    action = action.squeeze(0).cpu().numpy()\n                    action = np.concatenate([action[:-BASE_DELAY, :-2], action[BASE_DELAY:, -2:]], axis=1)\n                    print(f'Query: {(time.time() - start_time):.3f}s')\n                curr_action = action[t % query_freq]\n                target_qpos = curr_action[:-2]\n                base_action = curr_action[-2:]",
        "type": "code",
        "location": "/vinn_eval.py:230-250"
    },
    "681": {
        "file_id": 36,
        "content": "The code preprocesses visual and state features, calculates nearest neighbors for action selection using a specified metric, and filters out the required action based on query frequency. The resulting target position and base action are extracted for further processing.",
        "type": "comment"
    },
    "682": {
        "file_id": 36,
        "content": "                # ### SAFETY\n                # max_a = 0.05\n                # curr_qpos = qpos.squeeze().cpu().numpy()\n                # target_qpos = target_qpos.clip(curr_qpos - max_a, curr_qpos + max_a)\n                # ### SAFETY\n                ### step the environment\n                ts = env.step(target_qpos, base_action=base_action)\n                duration = time.time() - start_time\n                # print(f'{duration:.3f}')\n                time.sleep(max(0, DT - duration))\n                ### save things for visualization\n                qpos_list.append(qpos_numpy)\n                target_qpos_list.append(target_qpos)\n                rewards.append(ts.reward)\n                # if real_robot and t != 0 and t % 60 == 0:\n                #    e()\n            plt.close()\n        if real_robot:\n            env.puppet_bot_left.dxl.robot_set_operating_modes(\"single\", \"gripper\", \"position\")\n            env.puppet_bot_right.dxl.robot_set_operating_modes(\"single\", \"gripper\", \"position\")\n            env.puppet_bot_left.dxl.robot_set_operating_modes(\"single\", \"gripper\", \"pwm\")",
        "type": "code",
        "location": "/vinn_eval.py:252-275"
    },
    "683": {
        "file_id": 36,
        "content": "This code chunk is responsible for controlling the movement of a robot's joints, ensuring safety by clipping target positions within safe limits. It steps through the environment and saves information for visualization. If the robot is real, it sets the operating modes for the gripper and pwm.",
        "type": "comment"
    },
    "684": {
        "file_id": 36,
        "content": "            env.puppet_bot_right.dxl.robot_set_operating_modes(\"single\", \"gripper\", \"pwm\")\n        rewards = np.array(rewards)\n        episode_return = np.sum(rewards[rewards!=None])\n        episode_returns.append(episode_return)\n        max_reward = np.max(rewards)\n        max_rewards.append(max_reward)\n        print(f'{episode_return=}, {max_reward=}')\n        if save_episode:\n            save_videos(image_list, DT, video_path=os.path.join(ckpt_dir, f'video{rollout_id}.mp4'))\n            # visualize_joints(qpos_list, target_qpos_list, plot_path=os.path.join(ckpt_dir, f'qpos{rollout_id}.png'))\n            # visualize_joints(qpos_list, example_qpos, plot_path=os.path.join(ckpt_dir, f'qpos_reference{rollout_id}.png'), label_overwrite=(\"policy\", \"dataset\"))\n    success_rate = np.mean(np.array(max_rewards) == env_max_reward)\n    avg_return = np.mean(episode_returns)\n    summary_str = f'\\nSuccess rate: {success_rate}\\nAverage return: {avg_return}\\n\\n'\n    for r in range(env_max_reward+1):\n        more_or_equal_r = (np.array(max_rewards) >= r).sum()",
        "type": "code",
        "location": "/vinn_eval.py:276-294"
    },
    "685": {
        "file_id": 36,
        "content": "This code sets the operating modes for the robot's gripper and calculates rewards, episode returns, and maximum rewards. It then prints these values and saves videos or images if required. Finally, it calculates success rate and average return and constructs a summary string.",
        "type": "comment"
    },
    "686": {
        "file_id": 36,
        "content": "        more_or_equal_r_rate = more_or_equal_r / num_rollouts\n        summary_str += f'Reward >= {r}: {more_or_equal_r}/{num_rollouts} = {more_or_equal_r_rate*100}%\\n'\n    print(summary_str)\n    # save success rate to txt\n    result_file_name = f'result_{skip}_{k}' + '.txt'\n    with open(os.path.join(ckpt_dir, result_file_name), 'w') as f:\n        f.write(summary_str)\n        f.write(repr(episode_returns))\n        f.write('\\n\\n')\n        f.write(repr(max_rewards))\n    return success_rate, avg_return\ndef get_image(ts, camera_names):\n    if 'images' in ts.observation:\n        curr_images = []\n        for cam_name in camera_names:\n            curr_image = rearrange(ts.observation['images'][cam_name], 'h w c -> c h w')\n            curr_images.append(curr_image)\n        curr_image_raw = np.stack(curr_images, axis=0)\n    else:\n        curr_image_raw = rearrange(ts.observation['image'], 'h w c -> c h w')\n    curr_image = torch.from_numpy(curr_image_raw / 255.0).float().cuda().unsqueeze(0)\n    curr_image_raw = rearrange(curr_image_raw, 'b c h w -> b h w c')",
        "type": "code",
        "location": "/vinn_eval.py:295-322"
    },
    "687": {
        "file_id": 36,
        "content": "This function calculates the success rate, average return, and saves results to a text file for each episode. It retrieves images from observations, processes them, and stores the current image raw data in the correct format for further processing or visualization.",
        "type": "comment"
    },
    "688": {
        "file_id": 36,
        "content": "    return curr_image, curr_image_raw\ndef expand_greyscale(t):\n    return t.expand(3, -1, -1)\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--onscreen_render', action='store_true')\n    parser.add_argument('--dataset_dir', action='store', type=str, help='The text to parse.', required=True)\n    parser.add_argument('--model_dir', action='store', type=str, help='model_dir', required=True)\n    parser.add_argument('--task_name', action='store', type=str, help='task_name', required=True)\n    parser.add_argument('--ckpt_dir', action='store', type=str, help='The text to parse.', required=True)\n    main(vars(parser.parse_args()))",
        "type": "code",
        "location": "/vinn_eval.py:323-336"
    },
    "689": {
        "file_id": 36,
        "content": "The code defines a function expand_greyscale, sets up argument parsing with required parameters like dataset_dir and model_dir, and calls main function with the parsed arguments. The main function is not defined in this chunk but is called by passing the command-line arguments as variables. It seems to be a script for running an image classification task with specific directories and checkpoints.",
        "type": "comment"
    },
    "690": {
        "file_id": 37,
        "content": "/vinn_select_k.py",
        "type": "filepath"
    },
    "691": {
        "file_id": 37,
        "content": "The function `calculate_nearest_neighbors()` computes nearest neighbor losses and is used to select the optimal value of 'k' for a dataset, plotting and saving the best loss. User inputs: dataset directory, checkpoint directory.",
        "type": "summary"
    },
    "692": {
        "file_id": 37,
        "content": "import torch\nimport torch.nn.functional as F\nimport numpy as np\nimport h5py\nimport pathlib\nimport os\nimport argparse\nimport matplotlib.pyplot as plt\nimport IPython\ne = IPython.embed\n# modified from https://github.com/jyopari/VINN/blob/main/nearest-neighbor-eval/handle_nn.ipynb\ndef calculate_nearest_neighbors(query_inputs, query_targets, support_inputs, support_targets, max_k):\n    with torch.no_grad():\n        pairwise_dist = []\n        for q_in in query_inputs:\n            diff = support_inputs - q_in.unsqueeze(0)\n            dist = torch.norm(diff, dim=1)\n            pairwise_dist.append(dist)\n        pairwise_dist = torch.stack(pairwise_dist)\n        sorted_dist, index = torch.sort(pairwise_dist, dim=1) # sort the support axis\n        permuted_support_targets = support_targets[index]\n        errors = []\n        for k in range(1, max_k):\n            topk_dist = pairwise_dist[:, :k]\n            topk_support_targets = permuted_support_targets[:, :k]\n            weights = F.softmax(-topk_dist, dim=1)\n            weighted_support_targets = weights.unsqueeze(2) * topk_support_targets",
        "type": "code",
        "location": "/vinn_select_k.py:1-31"
    },
    "693": {
        "file_id": 37,
        "content": "Code imports necessary libraries and defines a function `calculate_nearest_neighbors()` that takes in query inputs, target values, support inputs, and support targets as well as a maximum value of K. It then calculates the pairwise distances between the query inputs and support inputs, sorts them, and calculates weights for the nearest neighbors using softmax. Finally, it computes errors by weighting the support targets based on these calculated weights.",
        "type": "comment"
    },
    "694": {
        "file_id": 37,
        "content": "            prediction = torch.sum(weighted_support_targets, dim=1)\n            error = F.mse_loss(prediction, query_targets)\n            errors.append(error)\n        return errors\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\ndef main(args):\n    # TODO ######################\n    dataset_dir = args['dataset_dir']\n    ckpt_dir = args['ckpt_dir']\n    seed = 0\n    max_k = 400\n    batch_size = 100\n    # TODO ######################\n    repr_type = 'byol'\n    if 'cotrain' in ckpt_dir:\n        repr_type += '_cotrain'\n    e() # make sure!\n    if not os.path.isdir(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    episode_idxs = [int(name.split('_')[1].split('.')[0]) for name in os.listdir(dataset_dir) if ('.hdf5' in name) and ('features' not in name)]\n    episode_idxs.sort()\n    assert len(episode_idxs) == episode_idxs[-1] + 1 # no holes\n    num_episodes = len(episode_idxs)\n    val_split = int(num_episodes * 0.8)\n    # load train data\n    X = []",
        "type": "code",
        "location": "/vinn_select_k.py:32-66"
    },
    "695": {
        "file_id": 37,
        "content": "This code reads episode indices from a specified directory, sorts them, and asserts there are no gaps. It then determines a validation split of 80% for training data. The code loads the training data into list X.",
        "type": "comment"
    },
    "696": {
        "file_id": 37,
        "content": "    Y = []\n    for episode_id in range(0, val_split):\n        dataset_path = os.path.join(dataset_dir, f'episode_{episode_id}.hdf5')\n        with h5py.File(dataset_path, 'r') as root:\n            action = root['/action'][:]\n            camera_names = list(root[f'/observations/images/'].keys())\n        all_cam_feature = []\n        feature_dataset_path = os.path.join(dataset_dir, f'{repr_type}_features_seed{seed}_episode_{episode_id}.hdf5')\n        with h5py.File(feature_dataset_path, 'r') as root:\n            for cam_name in camera_names:\n                cam_feature = root[f'/features/{cam_name}'][:]\n                all_cam_feature.append(cam_feature)\n        cam_feature = np.concatenate(all_cam_feature, axis=1)\n        X.append(cam_feature)\n        Y.append(action)\n    X = np.concatenate(X)\n    Y = np.concatenate(Y)\n    train_inputs = torch.from_numpy(X).cuda()\n    train_targets = torch.from_numpy(Y).cuda()\n    print(f'All features: {train_inputs.shape}')\n    # load test data\n    X = []\n    Y = []\n    for episode_id in range(val_split, num_episodes):",
        "type": "code",
        "location": "/vinn_select_k.py:67-94"
    },
    "697": {
        "file_id": 37,
        "content": "This code loads data from HDF5 files and concatenates it for training. It reads action labels and camera features for each episode, then combines them into a single feature matrix (X) and action label matrix (Y). The code also prints the shape of the feature matrices.",
        "type": "comment"
    },
    "698": {
        "file_id": 37,
        "content": "        dataset_path = os.path.join(dataset_dir, f'episode_{episode_id}.hdf5')\n        with h5py.File(dataset_path, 'r') as root:\n            action = root['/action'][:]\n        all_cam_feature = []\n        feature_dataset_path = os.path.join(dataset_dir, f'{repr_type}_features_seed{seed}_episode_{episode_id}.hdf5')\n        with h5py.File(feature_dataset_path, 'r') as root:\n            for cam_name in camera_names:\n                cam_feature = root[f'/features/{cam_name}'][:]\n                all_cam_feature.append(cam_feature)\n        cam_feature = np.concatenate(all_cam_feature, axis=1)\n        X.append(cam_feature)\n        Y.append(action)\n    X = np.concatenate(X)\n    Y = np.concatenate(Y)\n    val_inputs = torch.from_numpy(X).cuda()\n    val_targets = torch.from_numpy(Y).cuda()\n    val_losses = []\n    for inputs, targets in zip(chunks(val_inputs, batch_size), chunks(val_targets, batch_size)):\n        val_loss = calculate_nearest_neighbors(inputs, targets, train_inputs, train_targets, max_k)\n        val_loss = torch.stack(val_loss)",
        "type": "code",
        "location": "/vinn_select_k.py:95-118"
    },
    "699": {
        "file_id": 37,
        "content": "This code loads data from multiple HDF5 files, concatenates camera features into a single feature matrix (X), and associates corresponding actions as targets (Y). It then prepares the data for training by converting to PyTorch tensors and computing nearest neighbor losses using a custom function. The resulting losses are stored in val_losses list.",
        "type": "comment"
    }
}