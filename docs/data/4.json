{
    "400": {
        "file_id": 23,
        "content": "            torch.save(policy.serialize(), ckpt_path)\n    ckpt_path = os.path.join(ckpt_dir, f'policy_last.ckpt')\n    torch.save(policy.serialize(), ckpt_path)\n    best_step, min_val_loss, best_state_dict = best_ckpt_info\n    ckpt_path = os.path.join(ckpt_dir, f'policy_step_{best_step}_seed_{seed}.ckpt')\n    torch.save(best_state_dict, ckpt_path)\n    print(f'Training finished:\\nSeed {seed}, val loss {min_val_loss:.6f} at step {best_step}')\n    return best_ckpt_info\ndef repeater(data_loader):\n    epoch = 0\n    for loader in repeat(data_loader):\n        for data in loader:\n            yield data\n        print(f'Epoch {epoch} done')\n        epoch += 1\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--eval', action='store_true')\n    parser.add_argument('--onscreen_render', action='store_true')\n    parser.add_argument('--ckpt_dir', action='store', type=str, help='ckpt_dir', required=True)\n    parser.add_argument('--policy_class', action='store', type=str, help='policy_class, capitalize', required=True)",
        "type": "code",
        "location": "/imitate_episodes.py:612-638"
    },
    "401": {
        "file_id": 23,
        "content": "The code defines a function to train and save a policy, repeats the data loader for multiple epochs, and takes command-line arguments for evaluation, on-screen rendering, checkpoint directory, and policy class. The training finishes when it finds the best model based on validation loss, saves it, and prints information about the best step, seed, and validation loss.",
        "type": "comment"
    },
    "402": {
        "file_id": 23,
        "content": "    parser.add_argument('--task_name', action='store', type=str, help='task_name', required=True)\n    parser.add_argument('--batch_size', action='store', type=int, help='batch_size', required=True)\n    parser.add_argument('--seed', action='store', type=int, help='seed', required=True)\n    parser.add_argument('--num_steps', action='store', type=int, help='num_steps', required=True)\n    parser.add_argument('--lr', action='store', type=float, help='lr', required=True)\n    parser.add_argument('--load_pretrain', action='store_true', default=False)\n    parser.add_argument('--eval_every', action='store', type=int, default=500, help='eval_every', required=False)\n    parser.add_argument('--validate_every', action='store', type=int, default=500, help='validate_every', required=False)\n    parser.add_argument('--save_every', action='store', type=int, default=500, help='save_every', required=False)\n    parser.add_argument('--resume_ckpt_path', action='store', type=str, help='resume_ckpt_path', required=False)",
        "type": "code",
        "location": "/imitate_episodes.py:639-648"
    },
    "403": {
        "file_id": 23,
        "content": "The code above is using the ArgumentParser from Python's argparse module to add various command-line arguments for a task. These arguments include 'task_name', 'batch_size', 'seed', 'num_steps', 'lr', 'load_pretrain', 'eval_every', and 'validate_every'. The 'save_every' argument is optional, as well as the 'resume_ckpt_path'. These arguments are required or defaulted depending on the specifications.",
        "type": "comment"
    },
    "404": {
        "file_id": 23,
        "content": "    parser.add_argument('--skip_mirrored_data', action='store_true')\n    parser.add_argument('--actuator_network_dir', action='store', type=str, help='actuator_network_dir', required=False)\n    parser.add_argument('--history_len', action='store', type=int)\n    parser.add_argument('--future_len', action='store', type=int)\n    parser.add_argument('--prediction_len', action='store', type=int)\n    # for ACT\n    parser.add_argument('--kl_weight', action='store', type=int, help='KL Weight', required=False)\n    parser.add_argument('--chunk_size', action='store', type=int, help='chunk_size', required=False)\n    parser.add_argument('--hidden_dim', action='store', type=int, help='hidden_dim', required=False)\n    parser.add_argument('--dim_feedforward', action='store', type=int, help='dim_feedforward', required=False)\n    parser.add_argument('--temporal_agg', action='store_true')\n    parser.add_argument('--use_vq', action='store_true')\n    parser.add_argument('--vq_class', action='store', type=int, help='vq_class')",
        "type": "code",
        "location": "/imitate_episodes.py:649-662"
    },
    "405": {
        "file_id": 23,
        "content": "This code is using the Argparse module to define command-line arguments for a Python script. The arguments include options such as skipping mirrored data, specifying directories and lengths for history, future, and prediction. For ACT (Adaptive Computation Time) model, additional arguments like KL weight, chunk size, hidden dimension, feedforward dimension, and use of Variational Quantization are defined. These arguments allow the user to customize the behavior of the script based on their specific needs.",
        "type": "comment"
    },
    "406": {
        "file_id": 23,
        "content": "    parser.add_argument('--vq_dim', action='store', type=int, help='vq_dim')\n    parser.add_argument('--no_encoder', action='store_true')\n    main(vars(parser.parse_args()))",
        "type": "code",
        "location": "/imitate_episodes.py:663-666"
    },
    "407": {
        "file_id": 23,
        "content": "These lines are adding command line arguments to the parser object, allowing users to specify values for 'vq_dim' and 'no_encoder'. The first argument, '--vq_dim', uses integer type and provides a help message. The second argument, '--no_encoder', is set as a boolean flag when true. Lastly, the main function is called with the parsed arguments passed in as keyword arguments.",
        "type": "comment"
    },
    "408": {
        "file_id": 24,
        "content": "/policy.py",
        "type": "filepath"
    },
    "409": {
        "file_id": 24,
        "content": "The code creates a policy network for multi-camera image tasks, trains a noise residual prediction model, and includes an ACTPolicy class for reinforcement learning with normalization and loss calculation. It also defines a CNNMLP model for processing states, images, actions, with KL divergence, MSE loss, and training/inference modes.",
        "type": "summary"
    },
    "410": {
        "file_id": 24,
        "content": "import torch.nn as nn\nfrom torch.nn import functional as F\nimport torchvision.transforms as transforms\nimport torch\nimport numpy as np\nfrom detr.main import build_ACT_model_and_optimizer, build_CNNMLP_model_and_optimizer\nimport IPython\ne = IPython.embed\nfrom collections import OrderedDict\nfrom robomimic.models.base_nets import ResNet18Conv, SpatialSoftmax\nfrom robomimic.algo.diffusion_policy import replace_bn_with_gn, ConditionalUnet1D\nfrom diffusers.schedulers.scheduling_ddpm import DDPMScheduler\nfrom diffusers.schedulers.scheduling_ddim import DDIMScheduler\nfrom diffusers.training_utils import EMAModel\nclass DiffusionPolicy(nn.Module):\n    def __init__(self, args_override):\n        super().__init__()\n        self.camera_names = args_override['camera_names']\n        self.observation_horizon = args_override['observation_horizon'] ### TODO TODO TODO DO THIS\n        self.action_horizon = args_override['action_horizon'] # apply chunk size\n        self.prediction_horizon = args_override['prediction_horizon'] # chunk size",
        "type": "code",
        "location": "/policy.py:1-28"
    },
    "411": {
        "file_id": 24,
        "content": "The code imports necessary libraries and classes, defines a class for the DiffusionPolicy model, and includes parameters such as camera names, observation horizon, action horizon, and prediction horizon. The function build_ACT_model_and_optimizer and build_CNNMLP_model_and_optimizer are used to create models and optimizers, while replace_bn_with_gn and ConditionalUnet1D functions are called. EMAModel and scheduling classes DDPMScheduler and DDIMScheduler are also imported for training and scheduling purposes.",
        "type": "comment"
    },
    "412": {
        "file_id": 24,
        "content": "        self.num_inference_timesteps = args_override['num_inference_timesteps']\n        self.ema_power = args_override['ema_power']\n        self.lr = args_override['lr']\n        self.weight_decay = 0\n        self.num_kp = 32\n        self.feature_dimension = 64\n        self.ac_dim = args_override['action_dim'] # 14 + 2\n        self.obs_dim = self.feature_dimension * len(self.camera_names) + 14 # camera features and proprio\n        backbones = []\n        pools = []\n        linears = []\n        for _ in self.camera_names:\n            backbones.append(ResNet18Conv(**{'input_channel': 3, 'pretrained': False, 'input_coord_conv': False}))\n            pools.append(SpatialSoftmax(**{'input_shape': [512, 15, 20], 'num_kp': self.num_kp, 'temperature': 1.0, 'learnable_temperature': False, 'noise_std': 0.0}))\n            linears.append(torch.nn.Linear(int(np.prod([self.num_kp, 2])), self.feature_dimension))\n        backbones = nn.ModuleList(backbones)\n        pools = nn.ModuleList(pools)\n        linears = nn.ModuleList(linears)",
        "type": "code",
        "location": "/policy.py:29-48"
    },
    "413": {
        "file_id": 24,
        "content": "Initializing the model's parameters with values from args_override dictionary. Creating lists of ResNet18Conv, SpatialSoftmax, and Linear layers for each camera name. Converting lists to nn.ModuleList to facilitate efficient computation during model execution.",
        "type": "comment"
    },
    "414": {
        "file_id": 24,
        "content": "        backbones = replace_bn_with_gn(backbones) # TODO\n        noise_pred_net = ConditionalUnet1D(\n            input_dim=self.ac_dim,\n            global_cond_dim=self.obs_dim*self.observation_horizon\n        )\n        nets = nn.ModuleDict({\n            'policy': nn.ModuleDict({\n                'backbones': backbones,\n                'pools': pools,\n                'linears': linears,\n                'noise_pred_net': noise_pred_net\n            })\n        })\n        nets = nets.float().cuda()\n        ENABLE_EMA = True\n        if ENABLE_EMA:\n            ema = EMAModel(model=nets, power=self.ema_power)\n        else:\n            ema = None\n        self.nets = nets\n        self.ema = ema\n        # setup noise scheduler\n        self.noise_scheduler = DDIMScheduler(\n            num_train_timesteps=50,\n            beta_schedule='squaredcos_cap_v2',\n            clip_sample=True,\n            set_alpha_to_one=True,\n            steps_offset=0,\n            prediction_type='epsilon'\n        )\n        n_parameters = sum(p.numel() for p in self.parameters())",
        "type": "code",
        "location": "/policy.py:50-86"
    },
    "415": {
        "file_id": 24,
        "content": "This code defines a policy network with backbones, pools, linears, and noise prediction. The model is created as a PyTorch module, converted to float type, and moved to the GPU for faster computation. Optionally, an exponential moving average (EMA) model is also created if ENABLE_EMA flag is set. A noise scheduler is setup to manage the noise during training.",
        "type": "comment"
    },
    "416": {
        "file_id": 24,
        "content": "        print(\"number of parameters: %.2fM\" % (n_parameters/1e6,))\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.nets.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        return optimizer\n    def __call__(self, qpos, image, actions=None, is_pad=None):\n        B = qpos.shape[0]\n        if actions is not None: # training time\n            nets = self.nets\n            all_features = []\n            for cam_id in range(len(self.camera_names)):\n                cam_image = image[:, cam_id]\n                cam_features = nets['policy']['backbones'][cam_id](cam_image)\n                pool_features = nets['policy']['pools'][cam_id](cam_features)\n                pool_features = torch.flatten(pool_features, start_dim=1)\n                out_features = nets['policy']['linears'][cam_id](pool_features)\n                all_features.append(out_features)\n            obs_cond = torch.cat(all_features + [qpos], dim=1)\n            # sample noise to add to actions\n            noise = torch.randn(actions.shape, device=obs_cond.device)",
        "type": "code",
        "location": "/policy.py:87-111"
    },
    "417": {
        "file_id": 24,
        "content": "This code initializes an optimizer for the policy network in a multi-camera image task. It prints the number of parameters in the model and defines the __call__ method, which takes in input poses, images, actions (if training), and is_pad flags. During training, it extracts features from each camera's input, concatenates them with qpos, and adds noise to actions for better exploration.",
        "type": "comment"
    },
    "418": {
        "file_id": 24,
        "content": "            # sample a diffusion iteration for each data point\n            timesteps = torch.randint(\n                0, self.noise_scheduler.config.num_train_timesteps, \n                (B,), device=obs_cond.device\n            ).long()\n            # add noise to the clean actions according to the noise magnitude at each diffusion iteration\n            # (this is the forward diffusion process)\n            noisy_actions = self.noise_scheduler.add_noise(\n                actions, noise, timesteps)\n            # predict the noise residual\n            noise_pred = nets['policy']['noise_pred_net'](noisy_actions, timesteps, global_cond=obs_cond)\n            # L2 loss\n            all_l2 = F.mse_loss(noise_pred, noise, reduction='none')\n            loss = (all_l2 * ~is_pad.unsqueeze(-1)).mean()\n            loss_dict = {}\n            loss_dict['l2_loss'] = loss\n            loss_dict['loss'] = loss\n            if self.training and self.ema is not None:\n                self.ema.step(nets)\n            return loss_dict",
        "type": "code",
        "location": "/policy.py:113-137"
    },
    "419": {
        "file_id": 24,
        "content": "This code snippet samples diffusion iterations for each data point, adds noise to clean actions based on the noise magnitude at each iteration, predicts the noise residual using a neural network, calculates the L2 loss between predicted and actual noise, and returns the loss for training purposes. It also optionally updates an exponential moving average (EMA) of the model's parameters if in training mode and EMA is not None.",
        "type": "comment"
    },
    "420": {
        "file_id": 24,
        "content": "        else: # inference time\n            To = self.observation_horizon\n            Ta = self.action_horizon\n            Tp = self.prediction_horizon\n            action_dim = self.ac_dim\n            nets = self.nets\n            if self.ema is not None:\n                nets = self.ema.averaged_model\n            all_features = []\n            for cam_id in range(len(self.camera_names)):\n                cam_image = image[:, cam_id]\n                cam_features = nets['policy']['backbones'][cam_id](cam_image)\n                pool_features = nets['policy']['pools'][cam_id](cam_features)\n                pool_features = torch.flatten(pool_features, start_dim=1)\n                out_features = nets['policy']['linears'][cam_id](pool_features)\n                all_features.append(out_features)\n            obs_cond = torch.cat(all_features + [qpos], dim=1)\n            # initialize action from Guassian noise\n            noisy_action = torch.randn(\n                (B, Tp, action_dim), device=obs_cond.device)\n            naction = noisy_action",
        "type": "code",
        "location": "/policy.py:138-162"
    },
    "421": {
        "file_id": 24,
        "content": "This code is initializing action from Gaussian noise at inference time. It first determines the observation, action, and prediction horizons based on the policy settings. Then it retrieves the camera-specific networks and, if the exponential moving average (EMA) is not None, uses the averaged model instead of the current one. For each camera, it extracts features by passing images through the corresponding backbones, pools, and linears. Finally, it concatenates all extracted features with qpos, initializes noisy action from Gaussian noise, and sets naction to this noisy action.",
        "type": "comment"
    },
    "422": {
        "file_id": 24,
        "content": "            # init scheduler\n            self.noise_scheduler.set_timesteps(self.num_inference_timesteps)\n            for k in self.noise_scheduler.timesteps:\n                # predict noise\n                noise_pred = nets['policy']['noise_pred_net'](\n                    sample=naction, \n                    timestep=k,\n                    global_cond=obs_cond\n                )\n                # inverse diffusion step (remove noise)\n                naction = self.noise_scheduler.step(\n                    model_output=noise_pred,\n                    timestep=k,\n                    sample=naction\n                ).prev_sample\n            return naction\n    def serialize(self):\n        return {\n            \"nets\": self.nets.state_dict(),\n            \"ema\": self.ema.averaged_model.state_dict() if self.ema is not None else None,\n        }\n    def deserialize(self, model_dict):\n        status = self.nets.load_state_dict(model_dict[\"nets\"])\n        print('Loaded model')\n        if model_dict.get(\"ema\", None) is not None:",
        "type": "code",
        "location": "/policy.py:164-193"
    },
    "423": {
        "file_id": 24,
        "content": "The code initializes the noise scheduler and iterates through timesteps, predicting noise and performing inverse diffusion steps to remove noise from samples. It also includes functions for serializing and deserializing the model's parameters.",
        "type": "comment"
    },
    "424": {
        "file_id": 24,
        "content": "            print('Loaded EMA')\n            status_ema = self.ema.averaged_model.load_state_dict(model_dict[\"ema\"])\n            status = [status, status_ema]\n        return status\nclass ACTPolicy(nn.Module):\n    def __init__(self, args_override):\n        super().__init__()\n        model, optimizer = build_ACT_model_and_optimizer(args_override)\n        self.model = model # CVAE decoder\n        self.optimizer = optimizer\n        self.kl_weight = args_override['kl_weight']\n        self.vq = args_override['vq']\n        print(f'KL Weight {self.kl_weight}')\n    def __call__(self, qpos, image, actions=None, is_pad=None, vq_sample=None):\n        env_state = None\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n        image = normalize(image)\n        if actions is not None: # training time\n            actions = actions[:, :self.model.num_queries]\n            is_pad = is_pad[:, :self.model.num_queries]\n            loss_dict = dict()",
        "type": "code",
        "location": "/policy.py:194-218"
    },
    "425": {
        "file_id": 24,
        "content": "The code defines an `ACTPolicy` class that uses the ACT model and optimizer for reinforcement learning tasks. It normalizes images, handles both training and testing scenarios, and calculates loss during training time. The kl_weight and vq arguments are taken from args_override.",
        "type": "comment"
    },
    "426": {
        "file_id": 24,
        "content": "            a_hat, is_pad_hat, (mu, logvar), probs, binaries = self.model(qpos, image, env_state, actions, is_pad, vq_sample)\n            if self.vq or self.model.encoder is None:\n                total_kld = [torch.tensor(0.0)]\n            else:\n                total_kld, dim_wise_kld, mean_kld = kl_divergence(mu, logvar)\n            if self.vq:\n                loss_dict['vq_discrepancy'] = F.l1_loss(probs, binaries, reduction='mean')\n            all_l1 = F.l1_loss(actions, a_hat, reduction='none')\n            l1 = (all_l1 * ~is_pad.unsqueeze(-1)).mean()\n            loss_dict['l1'] = l1\n            loss_dict['kl'] = total_kld[0]\n            loss_dict['loss'] = loss_dict['l1'] + loss_dict['kl'] * self.kl_weight\n            return loss_dict\n        else: # inference time\n            a_hat, _, (_, _), _, _ = self.model(qpos, image, env_state, vq_sample=vq_sample) # no action, sample from prior\n            return a_hat\n    def configure_optimizers(self):\n        return self.optimizer\n    @torch.no_grad()\n    def vq_encode(self, qpos, actions, is_pad):",
        "type": "code",
        "location": "/policy.py:219-240"
    },
    "427": {
        "file_id": 24,
        "content": "The code is defining a policy function for an agent in a reinforcement learning environment. It calculates loss based on differences between predicted and actual actions, as well as KL divergence to penalize the model's confidence in its predictions. The code also defines an optimizer for training and a function to encode actions into binary representations for VQ-VAE (Variable Quantization Variational Autoencoder) models.",
        "type": "comment"
    },
    "428": {
        "file_id": 24,
        "content": "        actions = actions[:, :self.model.num_queries]\n        is_pad = is_pad[:, :self.model.num_queries]\n        _, _, binaries, _, _ = self.model.encode(qpos, actions, is_pad)\n        return binaries\n    def serialize(self):\n        return self.state_dict()\n    def deserialize(self, model_dict):\n        return self.load_state_dict(model_dict)\nclass CNNMLPPolicy(nn.Module):\n    def __init__(self, args_override):\n        super().__init__()\n        model, optimizer = build_CNNMLP_model_and_optimizer(args_override)\n        self.model = model # decoder\n        self.optimizer = optimizer\n    def __call__(self, qpos, image, actions=None, is_pad=None):\n        env_state = None # TODO\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n        image = normalize(image)\n        if actions is not None: # training time\n            actions = actions[:, 0]\n            a_hat = self.model(qpos, image, env_state, actions)\n            mse = F.mse_loss(actions, a_hat)",
        "type": "code",
        "location": "/policy.py:241-270"
    },
    "429": {
        "file_id": 24,
        "content": "This code defines a class for the CNNMLP policy model in an environment. The __init__ function initializes the model and optimizer based on arguments override, while the __call__ function takes in state (qpos), image, actions (if training time), and is_pad for processing. It normalizes the image, and if actions are provided, it calculates the MSE loss between predicted (a_hat) and actual (actions) actions.",
        "type": "comment"
    },
    "430": {
        "file_id": 24,
        "content": "            loss_dict = dict()\n            loss_dict['mse'] = mse\n            loss_dict['loss'] = loss_dict['mse']\n            return loss_dict\n        else: # inference time\n            a_hat = self.model(qpos, image, env_state) # no action, sample from prior\n            return a_hat\n    def configure_optimizers(self):\n        return self.optimizer\ndef kl_divergence(mu, logvar):\n    batch_size = mu.size(0)\n    assert batch_size != 0\n    if mu.data.ndimension() == 4:\n        mu = mu.view(mu.size(0), mu.size(1))\n    if logvar.data.ndimension() == 4:\n        logvar = logvar.view(logvar.size(0), logvar.size(1))\n    klds = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n    total_kld = klds.sum(1).mean(0, True)\n    dimension_wise_kld = klds.mean(0)\n    mean_kld = klds.mean(1).mean(0, True)\n    return total_kld, dimension_wise_kld, mean_kld",
        "type": "code",
        "location": "/policy.py:271-295"
    },
    "431": {
        "file_id": 24,
        "content": "This code is a part of a neural network policy model. It calculates the KL divergence between two variables, and depending on whether it's training or inference time, it either returns the action estimate (a_hat) or the losses for different loss types like mse. The optimizer configuration function returns the optimizer used by the model.",
        "type": "comment"
    },
    "432": {
        "file_id": 25,
        "content": "/postprocess_episodes.py",
        "type": "filepath"
    },
    "433": {
        "file_id": 25,
        "content": "The code imports libraries, loads data, processes episode information, scales actions, compresses images with JPEG quality 50, and saves in HDF5 format. It generates datasets for image variables and populates root dataset from data_dict.",
        "type": "summary"
    },
    "434": {
        "file_id": 25,
        "content": "import os\nimport numpy as np\nimport cv2\nimport h5py\nimport argparse\nimport time\nfrom visualize_episodes import visualize_joints, visualize_timestamp, save_videos\nimport matplotlib.pyplot as plt\nfrom constants import DT\nimport IPython\ne = IPython.embed\nJOINT_NAMES = [\"waist\", \"shoulder\", \"elbow\", \"forearm_roll\", \"wrist_angle\", \"wrist_rotate\"]\nSTATE_NAMES = JOINT_NAMES + [\"gripper\"]\nMIRROR_STATE_MULTIPLY = np.array([-1, 1, 1, -1, 1, -1, 1]).astype('float32')\nMIRROR_BASE_MULTIPLY = np.array([1, -1]).astype('float32')\ndef load_hdf5(dataset_dir, dataset_name):\n    dataset_path = os.path.join(dataset_dir, dataset_name + '.hdf5')\n    if not os.path.isfile(dataset_path):\n        print(f'Dataset does not exist at \\n{dataset_path}\\n')\n        exit()\n    with h5py.File(dataset_path, 'r') as root:\n        is_sim = root.attrs['sim']\n        compressed = root.attrs.get('compress', False)\n        qpos = root['/observations/qpos'][()]\n        qvel = root['/observations/qvel'][()]\n        action = root['/action'][()]\n        image_dict = dict()",
        "type": "code",
        "location": "/postprocess_episodes.py:1-33"
    },
    "435": {
        "file_id": 25,
        "content": "This code imports necessary libraries and defines constants for a robotics data processing script. It loads data from .hdf5 files, including robot joint positions and velocities, as well as actions performed by the robot.",
        "type": "comment"
    },
    "436": {
        "file_id": 25,
        "content": "        for cam_name in root[f'/observations/images/'].keys():\n            image_dict[cam_name] = root[f'/observations/images/{cam_name}'][()]\n        if 'base_action' in root.keys():\n            print('base_action exists')\n            base_action = root['/base_action'][()]\n        else:\n            base_action = None\n        if compressed:\n            compress_len = root['/compress_len'][()]\n    if compressed:\n        for cam_id, cam_name in enumerate(image_dict.keys()):\n            # un-pad and uncompress\n            padded_compressed_image_list = image_dict[cam_name]\n            image_list = []\n            for padded_compressed_image in padded_compressed_image_list: # [:1000] to save memory\n                image = cv2.imdecode(padded_compressed_image, 1)\n                image_list.append(image)\n            image_dict[cam_name] = np.array(image_list)\n    return qpos, qvel, action, base_action, image_dict, is_sim\ndef main(args):\n    dataset_dir = args['dataset_dir']\n    num_episodes = args['num_episodes']\n    start_idx = 0",
        "type": "code",
        "location": "/postprocess_episodes.py:34-60"
    },
    "437": {
        "file_id": 25,
        "content": "Iterates through image keys, stores in image_dict.\nChecks if base_action exists and assigns value accordingly.\nIf compressed, un-pads and uncompresses images, stores in image_dict.\nReturns various variables including base_action and image_dict.",
        "type": "comment"
    },
    "438": {
        "file_id": 25,
        "content": "    for episode_idx in range(start_idx, start_idx + num_episodes):\n        dataset_name = f'episode_{episode_idx}'\n        qpos, qvel, action, base_action, image_dict, is_sim = load_hdf5(dataset_dir, dataset_name)\n        # process proprioception\n        qpos = np.concatenate([qpos[:, 7:] * MIRROR_STATE_MULTIPLY, qpos[:, :7] * MIRROR_STATE_MULTIPLY], axis=1)\n        qvel = np.concatenate([qvel[:, 7:] * MIRROR_STATE_MULTIPLY, qvel[:, :7] * MIRROR_STATE_MULTIPLY], axis=1)\n        action = np.concatenate([action[:, 7:] * MIRROR_STATE_MULTIPLY, action[:, :7] * MIRROR_STATE_MULTIPLY], axis=1)\n        if base_action is not None:\n            base_action = base_action * MIRROR_BASE_MULTIPLY\n        # mirror image obs\n        if 'left_wrist' in image_dict.keys():\n            image_dict['left_wrist'], image_dict['right_wrist'] = image_dict['right_wrist'][:, :, ::-1], image_dict['left_wrist'][:, :, ::-1]\n        elif 'cam_left_wrist' in image_dict.keys():\n            image_dict['cam_left_wrist'], image_dict['",
        "type": "code",
        "location": "/postprocess_episodes.py:61-77"
    },
    "439": {
        "file_id": 25,
        "content": "This code is part of a function that loads and processes episode data from HDF5 files. It iterates over multiple episodes, concatenating mirrored proprioception and action data, and optionally scales the base action. If any images with 'left_wrist' or 'cam_left_wrist' keys exist in the image dictionary, it swaps their positions for mirroring purposes.",
        "type": "comment"
    },
    "440": {
        "file_id": 25,
        "content": "cam_right_wrist'] = image_dict['cam_right_wrist'][:, :, ::-1], image_dict['cam_left_wrist'][:, :, ::-1]\n        else:\n            raise Exception('No left_wrist or cam_left_wrist in image_dict')\n        if 'top' in image_dict.keys():\n            image_dict['top'] = image_dict['top'][:, :, ::-1]\n        elif 'cam_high' in image_dict.keys():\n            image_dict['cam_high'] = image_dict['cam_high'][:, :, ::-1]\n        else:\n            raise Exception('No top or cam_high in image_dict')\n        # saving\n        data_dict = {\n            '/observations/qpos': qpos,\n            '/observations/qvel': qvel,\n            '/action': action,\n            '/base_action': base_action,\n        } if base_action is not None else {\n            '/observations/qpos': qpos,\n            '/observations/qvel': qvel,\n            '/action': action,\n        }\n        for cam_name in image_dict.keys():\n            data_dict[f'/observations/images/{cam_name}'] = image_dict[cam_name]\n        max_timesteps = len(qpos)\n        COMPRESS = True",
        "type": "code",
        "location": "/postprocess_episodes.py:77-103"
    },
    "441": {
        "file_id": 25,
        "content": "This code checks for specific keys in the image_dict and adjusts the values if necessary. If 'left_wrist' or 'cam_left_wrist' is present, it flips the image. It also handles if 'top' or 'cam_high' are present, flipping them accordingly. Then, it creates a data_dict with necessary keys ('/observations/qpos', '/observations/qvel', '/action', and '/base_action') for saving. Finally, it loops through the image_dict to add its contents as key-value pairs in the data_dict, and sets max_timesteps as the length of qpos. The code uses compression while saving.",
        "type": "comment"
    },
    "442": {
        "file_id": 25,
        "content": "        if COMPRESS:\n            # JPEG compression\n            t0 = time.time()\n            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 50] # tried as low as 20, seems fine\n            compressed_len = []\n            for cam_name in image_dict.keys():\n                image_list = data_dict[f'/observations/images/{cam_name}']\n                compressed_list = []\n                compressed_len.append([])\n                for image in image_list:\n                    result, encoded_image = cv2.imencode('.jpg', image, encode_param) # 0.02 sec # cv2.imdecode(encoded_image, 1)\n                    compressed_list.append(encoded_image)\n                    compressed_len[-1].append(len(encoded_image))\n                data_dict[f'/observations/images/{cam_name}'] = compressed_list\n            print(f'compression: {time.time() - t0:.2f}s')\n            # pad so it has same length\n            t0 = time.time()\n            compressed_len = np.array(compressed_len)\n            padded_size = compressed_len.max()\n            for cam_name in image_dict.keys():",
        "type": "code",
        "location": "/postprocess_episodes.py:105-125"
    },
    "443": {
        "file_id": 25,
        "content": "This code compresses images using JPEG compression with a quality level of 50, stores the compressed images in the data dictionary, and measures the time taken for the compression process.",
        "type": "comment"
    },
    "444": {
        "file_id": 25,
        "content": "                compressed_image_list = data_dict[f'/observations/images/{cam_name}']\n                padded_compressed_image_list = []\n                for compressed_image in compressed_image_list:\n                    padded_compressed_image = np.zeros(padded_size, dtype='uint8')\n                    image_len = len(compressed_image)\n                    padded_compressed_image[:image_len] = compressed_image\n                    padded_compressed_image_list.append(padded_compressed_image)\n                data_dict[f'/observations/images/{cam_name}'] = padded_compressed_image_list\n            print(f'padding: {time.time() - t0:.2f}s')\n        # HDF5\n        t0 = time.time()\n        dataset_path = os.path.join(dataset_dir, f'mirror_episode_{episode_idx}')\n        with h5py.File(dataset_path + '.hdf5', 'w', rdcc_nbytes=1024 ** 2 * 2) as root:\n            root.attrs['sim'] = is_sim\n            root.attrs['compress'] = COMPRESS\n            obs = root.create_group('observations')\n            image = obs.create_group('images')",
        "type": "code",
        "location": "/postprocess_episodes.py:126-143"
    },
    "445": {
        "file_id": 25,
        "content": "This code is padding compressed images, adding them to the data dictionary, and saving the dataset in HDF5 format. The padding ensures all images have the same length for consistency in the HDF5 file. It also records the time taken to pad the images.",
        "type": "comment"
    },
    "446": {
        "file_id": 25,
        "content": "            for cam_name in image_dict.keys():\n                if COMPRESS:\n                    _ = image.create_dataset(cam_name, (max_timesteps, padded_size), dtype='uint8',\n                                            chunks=(1, padded_size), )\n                else:\n                    _ = image.create_dataset(cam_name, (max_timesteps, 480, 640, 3), dtype='uint8',\n                                            chunks=(1, 480, 640, 3), )\n            qpos = obs.create_dataset('qpos', (max_timesteps, 14))\n            qvel = obs.create_dataset('qvel', (max_timesteps, 14))\n            action = root.create_dataset('action', (max_timesteps, 14))\n            if base_action is not None:\n                base_action = root.create_dataset('base_action', (max_timesteps, 2))\n            for name, array in data_dict.items():\n                root[name][...] = array\n            if COMPRESS:\n                _ = root.create_dataset('compress_len', (len(image_dict.keys()), max_timesteps))\n                root['/compress_len'][...] = compressed_len",
        "type": "code",
        "location": "/postprocess_episodes.py:144-162"
    },
    "447": {
        "file_id": 25,
        "content": "This code creates datasets for image data and other variables, based on whether to compress or not. It also creates datasets for qpos, qvel, action, and base_action if they are not None. Additionally, it populates the root dataset with data from the data_dict and creates a 'compress_len' dataset if compression is enabled.",
        "type": "comment"
    },
    "448": {
        "file_id": 25,
        "content": "        print(f'Saving {dataset_path}: {time.time() - t0:.1f} secs\\n')\n        if episode_idx == start_idx:\n            save_videos(image_dict, DT, video_path=os.path.join(dataset_dir, dataset_name + '_mirror_video.mp4'))\n            # visualize_joints(qpos, action, plot_path=os.path.join(dataset_dir, dataset_name + '_mirror_qpos.png'))\n            # visualize_timestamp(t_list, dataset_path) # TODO addn timestamp back\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset_dir', action='store', type=str, help='Dataset dir.', required=True)\n    parser.add_argument('--num_episodes', action='store', type=int, help='Number of episodes.', required=True)\n    main(vars(parser.parse_args()))",
        "type": "code",
        "location": "/postprocess_episodes.py:164-175"
    },
    "449": {
        "file_id": 25,
        "content": "The code snippet saves the dataset, prints the time taken for the process, and has options to save videos and visualize joints. The user is required to specify the dataset directory and the number of episodes.",
        "type": "comment"
    },
    "450": {
        "file_id": 26,
        "content": "/record_sim_episodes.py",
        "type": "filepath"
    },
    "451": {
        "file_id": 26,
        "content": "The code imports libraries, initializes policy and environment, iterates over time steps, takes actions, updates state, determines success, and evaluates simulation episodes, storing data in an HDF5 file for visualization or analysis. It also creates datasets from camera images, qpos, and actions.",
        "type": "summary"
    },
    "452": {
        "file_id": 26,
        "content": "import time\nimport os\nimport numpy as np\nimport argparse\nimport matplotlib.pyplot as plt\nimport h5py\nfrom constants import PUPPET_GRIPPER_POSITION_NORMALIZE_FN, SIM_TASK_CONFIGS\nfrom ee_sim_env import make_ee_sim_env\nfrom sim_env import make_sim_env, BOX_POSE\nfrom scripted_policy import PickAndTransferPolicy, InsertionPolicy\nimport IPython\ne = IPython.embed\ndef main(args):\n    \"\"\"\n    Generate demonstration data in simulation.\n    First rollout the policy (defined in ee space) in ee_sim_env. Obtain the joint trajectory.\n    Replace the gripper joint positions with the commanded joint position.\n    Replay this joint trajectory (as action sequence) in sim_env, and record all observations.\n    Save this episode of data, and continue to next episode of data collection.\n    \"\"\"\n    task_name = args['task_name']\n    dataset_dir = args['dataset_dir']\n    num_episodes = args['num_episodes']\n    onscreen_render = args['onscreen_render']\n    inject_noise = False\n    render_cam_name = 'top'\n    if not os.path.isdir(dataset_dir):",
        "type": "code",
        "location": "/record_sim_episodes.py:1-33"
    },
    "453": {
        "file_id": 26,
        "content": "The code imports necessary libraries, defines the main function to generate demonstration data in simulation. It first rolls out policy in ee_sim_env and obtains joint trajectory, then replaces gripper joint positions with commanded positions. Finally, it replay joint trajectory in sim_env and record observations for each episode before saving the dataset.",
        "type": "comment"
    },
    "454": {
        "file_id": 26,
        "content": "        os.makedirs(dataset_dir, exist_ok=True)\n    episode_len = SIM_TASK_CONFIGS[task_name]['episode_len']\n    camera_names = SIM_TASK_CONFIGS[task_name]['camera_names']\n    if task_name == 'sim_transfer_cube_scripted':\n        policy_cls = PickAndTransferPolicy\n    elif task_name == 'sim_insertion_scripted':\n        policy_cls = InsertionPolicy\n    elif task_name == 'sim_transfer_cube_scripted_mirror':\n        policy_cls = PickAndTransferPolicy\n    else:\n        raise NotImplementedError\n    success = []\n    for episode_idx in range(num_episodes):\n        print(f'{episode_idx=}')\n        print('Rollout out EE space scripted policy')\n        # setup the environment\n        env = make_ee_sim_env(task_name)\n        ts = env.reset()\n        episode = [ts]\n        policy = policy_cls(inject_noise)\n        # setup plotting\n        if onscreen_render:\n            ax = plt.subplot()\n            plt_img = ax.imshow(ts.observation['images'][render_cam_name])\n            plt.ion()\n        for step in range(episode_len):",
        "type": "code",
        "location": "/record_sim_episodes.py:34-61"
    },
    "455": {
        "file_id": 26,
        "content": "This code snippet is creating a new directory for the dataset, setting up the episode length and camera names based on the task name, and then initializing the policy class depending on the task. It also creates an empty list for success and starts a loop for each episode where it sets up the environment, resets the environment, creates an episode list with the first observation, initializes the policy, and then starts another loop to iterate through steps in each episode.",
        "type": "comment"
    },
    "456": {
        "file_id": 26,
        "content": "            action = policy(ts)\n            ts = env.step(action)\n            episode.append(ts)\n            if onscreen_render:\n                plt_img.set_data(ts.observation['images'][render_cam_name])\n                plt.pause(0.002)\n        plt.close()\n        episode_return = np.sum([ts.reward for ts in episode[1:]])\n        episode_max_reward = np.max([ts.reward for ts in episode[1:]])\n        if episode_max_reward == env.task.max_reward:\n            print(f\"{episode_idx=} Successful, {episode_return=}\")\n        else:\n            print(f\"{episode_idx=} Failed\")\n        joint_traj = [ts.observation['qpos'] for ts in episode]\n        # replace gripper pose with gripper control\n        gripper_ctrl_traj = [ts.observation['gripper_ctrl'] for ts in episode]\n        for joint, ctrl in zip(joint_traj, gripper_ctrl_traj):\n            left_ctrl = PUPPET_GRIPPER_POSITION_NORMALIZE_FN(ctrl[0])\n            right_ctrl = PUPPET_GRIPPER_POSITION_NORMALIZE_FN(ctrl[2])\n            joint[6] = left_ctrl\n            joint[6+7] = right_ctrl",
        "type": "code",
        "location": "/record_sim_episodes.py:62-84"
    },
    "457": {
        "file_id": 26,
        "content": "This code is iterating over each time step in the episode, taking actions based on a policy, updating the state, and appending the state to the episode list. It also renders images for each state if the onscreen_render flag is set. It calculates the episode return and maximum reward, then prints whether the episode was successful or not. Finally, it extracts joint and gripper control trajectories from the episode and applies normalization to gripper positions.",
        "type": "comment"
    },
    "458": {
        "file_id": 26,
        "content": "        subtask_info = episode[0].observation['env_state'].copy() # box pose at step 0\n        # clear unused variables\n        del env\n        del episode\n        del policy\n        # setup the environment\n        print('Replaying joint commands')\n        env = make_sim_env(task_name)\n        BOX_POSE[0] = subtask_info # make sure the sim_env has the same object configurations as ee_sim_env\n        ts = env.reset()\n        episode_replay = [ts]\n        # setup plotting\n        if onscreen_render:\n            ax = plt.subplot()\n            plt_img = ax.imshow(ts.observation['images'][render_cam_name])\n            plt.ion()\n        for t in range(len(joint_traj)): # note: this will increase episode length by 1\n            action = joint_traj[t]\n            ts = env.step(action)\n            episode_replay.append(ts)\n            if onscreen_render:\n                plt_img.set_data(ts.observation['images'][render_cam_name])\n                plt.pause(0.02)\n        episode_return = np.sum([ts.reward for ts in episode_replay[1:]])",
        "type": "code",
        "location": "/record_sim_episodes.py:86-113"
    },
    "459": {
        "file_id": 26,
        "content": "This code is replaying joint commands from a previous episode. It first saves the initial box pose, clears unused variables, sets up the environment, and resets it. Then, for each joint command in the trajectory, it performs an action in the environment and appends the new state to the episode_replay list. If onscreen_render is True, it updates a plot with the current observation image. Finally, it calculates the total reward from the episode and stores it as episode_return.",
        "type": "comment"
    },
    "460": {
        "file_id": 26,
        "content": "        episode_max_reward = np.max([ts.reward for ts in episode_replay[1:]])\n        if episode_max_reward == env.task.max_reward:\n            success.append(1)\n            print(f\"{episode_idx=} Successful, {episode_return=}\")\n        else:\n            success.append(0)\n            print(f\"{episode_idx=} Failed\")\n        plt.close()\n        \"\"\"\n        For each timestep:\n        observations\n        - images\n            - each_cam_name     (480, 640, 3) 'uint8'\n        - qpos                  (14,)         'float64'\n        - qvel                  (14,)         'float64'\n        action                  (14,)         'float64'\n        \"\"\"\n        data_dict = {\n            '/observations/qpos': [],\n            '/observations/qvel': [],\n            '/action': [],\n        }\n        for cam_name in camera_names:\n            data_dict[f'/observations/images/{cam_name}'] = []\n        # because the replaying, there will be eps_len + 1 actions and eps_len + 2 timesteps\n        # truncate here to be consistent\n        joint_traj = joint_traj[:-1]",
        "type": "code",
        "location": "/record_sim_episodes.py:114-145"
    },
    "461": {
        "file_id": 26,
        "content": "This code measures the success of each episode in a simulation by checking if the maximum reward reached the maximum possible reward. If it did, the episode is considered successful and printed as such; otherwise, it's considered a failure. The code also collects observations and actions into a data dictionary for potential visualization or analysis purposes.",
        "type": "comment"
    },
    "462": {
        "file_id": 26,
        "content": "        episode_replay = episode_replay[:-1]\n        # len(joint_traj) i.e. actions: max_timesteps\n        # len(episode_replay) i.e. time steps: max_timesteps + 1\n        max_timesteps = len(joint_traj)\n        while joint_traj:\n            action = joint_traj.pop(0)\n            ts = episode_replay.pop(0)\n            data_dict['/observations/qpos'].append(ts.observation['qpos'])\n            data_dict['/observations/qvel'].append(ts.observation['qvel'])\n            data_dict['/action'].append(action)\n            for cam_name in camera_names:\n                data_dict[f'/observations/images/{cam_name}'].append(ts.observation['images'][cam_name])\n        # HDF5\n        t0 = time.time()\n        dataset_path = os.path.join(dataset_dir, f'episode_{episode_idx}')\n        with h5py.File(dataset_path + '.hdf5', 'w', rdcc_nbytes=1024 ** 2 * 2) as root:\n            root.attrs['sim'] = True\n            obs = root.create_group('observations')\n            image = obs.create_group('images')\n            for cam_name in camera_names:",
        "type": "code",
        "location": "/record_sim_episodes.py:146-167"
    },
    "463": {
        "file_id": 26,
        "content": "This code segment is part of a function that processes episode data from a simulation and saves it as an HDF5 file. It extracts observations, actions, and camera images from the episode replay and stores them in the dictionary \"data_dict\". After processing all timesteps, it creates an HDF5 file with the episode data, including attributes and groups for observations and images.",
        "type": "comment"
    },
    "464": {
        "file_id": 26,
        "content": "                _ = image.create_dataset(cam_name, (max_timesteps, 480, 640, 3), dtype='uint8',\n                                         chunks=(1, 480, 640, 3), )\n            # compression='gzip',compression_opts=2,)\n            # compression=32001, compression_opts=(0, 0, 0, 0, 9, 1, 1), shuffle=False)\n            qpos = obs.create_dataset('qpos', (max_timesteps, 14))\n            qvel = obs.create_dataset('qvel', (max_timesteps, 14))\n            action = root.create_dataset('action', (max_timesteps, 14))\n            for name, array in data_dict.items():\n                root[name][...] = array\n        print(f'Saving: {time.time() - t0:.1f} secs\\n')\n    print(f'Saved to {dataset_dir}')\n    print(f'Success: {np.sum(success)} / {len(success)}')\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--task_name', action='store', type=str, help='task_name', required=True)\n    parser.add_argument('--dataset_dir', action='store', type=str, help='dataset saving dir', required=True)",
        "type": "code",
        "location": "/record_sim_episodes.py:168-186"
    },
    "465": {
        "file_id": 26,
        "content": "This code creates datasets for camera images, qpos, qvel, and actions in a specific order. It then assigns the array values to corresponding names within the root dataset. Finally, it provides statistics on the saving time, saved location, and success rate of the task. The code assumes 'max_timesteps', 'data_dict', 'cam_name' and 'obs' are predefined variables.",
        "type": "comment"
    },
    "466": {
        "file_id": 26,
        "content": "    parser.add_argument('--num_episodes', action='store', type=int, help='num_episodes', required=False)\n    parser.add_argument('--onscreen_render', action='store_true')\n    main(vars(parser.parse_args()))",
        "type": "code",
        "location": "/record_sim_episodes.py:187-190"
    },
    "467": {
        "file_id": 26,
        "content": "The code above adds command line arguments for the number of episodes and on-screen rendering to a parser. The 'num_episodes' argument is of type int, required=False, and helps specify the number of episodes to run. The 'onscreen_render' argument, when set to true, enables on-screen rendering during game playback. The main function takes the arguments parsed by the parser object to execute the program.",
        "type": "comment"
    },
    "468": {
        "file_id": 27,
        "content": "/replay_episodes.py",
        "type": "filepath"
    },
    "469": {
        "file_id": 27,
        "content": "The code imports libraries and defines a main function to replay an episode from an existing dataset, organizing images into videos. The 'save_videos' function is defined for command line arguments and executed if the script is run directly.",
        "type": "summary"
    },
    "470": {
        "file_id": 27,
        "content": "import os\nimport h5py\nimport argparse\nfrom collections import defaultdict \nfrom sim_env import make_sim_env\nfrom utils import sample_box_pose, sample_insertion_pose\nfrom sim_env import BOX_POSE\nfrom constants import DT\nfrom visualize_episodes import save_videos\nimport IPython\ne = IPython.embed\ndef main(args):\n    dataset_path = args['dataset_path']\n    if not os.path.isfile(dataset_path):\n        print(f'Dataset does not exist at \\n{dataset_path}\\n')\n        exit()\n    with h5py.File(dataset_path, 'r') as root:\n        actions = root['/action'][()]\n    env = make_sim_env('sim_transfer_cube')\n    BOX_POSE[0] = sample_box_pose() # used in sim reset\n    ts = env.reset()\n    episode_replay = [ts]\n    for action in actions:\n        ts = env.step(action)\n        episode_replay.append(ts)\n    # saving\n    image_dict = defaultdict(lambda: [])\n    while episode_replay:\n        ts = episode_replay.pop(0)\n        for cam_name, image in ts.observation['images'].items():\n            image_dict[cam_name].append(image)\n    video_path = dataset_path.replace('episode_', 'replay_episode_').replace('hdf5', 'mp4')",
        "type": "code",
        "location": "/replay_episodes.py:1-41"
    },
    "471": {
        "file_id": 27,
        "content": "The code imports necessary libraries and defines a main function to replay an episode from an existing dataset. It checks if the dataset file exists, then reads the actions and initializes the simulation environment. It performs the steps of the replayed episode by taking actions in the environment, appending states to the episode_replay list. The code then organizes images from each state into a dictionary for saving. Finally, it creates a video path with the modified name and saves the images as videos in that new path.",
        "type": "comment"
    },
    "472": {
        "file_id": 27,
        "content": "    save_videos(image_dict, DT, video_path=video_path)\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset_path', action='store', type=str, help='Dataset path.', required=True)\n    main(vars(parser.parse_args()))",
        "type": "code",
        "location": "/replay_episodes.py:42-48"
    },
    "473": {
        "file_id": 27,
        "content": "The code defines a function \"save_videos\" and checks if the script is run directly. It sets up an ArgumentParser for command line arguments, including '--dataset_path'. Then it calls main with the parsed command line arguments.",
        "type": "comment"
    },
    "474": {
        "file_id": 28,
        "content": "/scripted_policy.py",
        "type": "filepath"
    },
    "475": {
        "file_id": 28,
        "content": "The code introduces a `BasePolicy` class for robotic arm policy, incorporating trajectory generation, updating poses and gripper commands, and executing pre-generated trajectories. It initializes an environment and runs two episodes of actions using PickAndTransferPolicy to test cube transfer simulation scripts.",
        "type": "summary"
    },
    "476": {
        "file_id": 28,
        "content": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom pyquaternion import Quaternion\nfrom constants import SIM_TASK_CONFIGS\nfrom ee_sim_env import make_ee_sim_env\nimport IPython\ne = IPython.embed\nclass BasePolicy:\n    def __init__(self, inject_noise=False):\n        self.inject_noise = inject_noise\n        self.step_count = 0\n        self.left_trajectory = None\n        self.right_trajectory = None\n    def generate_trajectory(self, ts_first):\n        raise NotImplementedError\n    @staticmethod\n    def interpolate(curr_waypoint, next_waypoint, t):\n        t_frac = (t - curr_waypoint[\"t\"]) / (next_waypoint[\"t\"] - curr_waypoint[\"t\"])\n        curr_xyz = curr_waypoint['xyz']\n        curr_quat = curr_waypoint['quat']\n        curr_grip = curr_waypoint['gripper']\n        next_xyz = next_waypoint['xyz']\n        next_quat = next_waypoint['quat']\n        next_grip = next_waypoint['gripper']\n        xyz = curr_xyz + (next_xyz - curr_xyz) * t_frac\n        quat = curr_quat + (next_quat - curr_quat) * t_frac\n        gripper = curr_grip + (next_grip - curr_grip) * t_frac",
        "type": "code",
        "location": "/scripted_policy.py:1-33"
    },
    "477": {
        "file_id": 28,
        "content": "The code defines a `BasePolicy` class for a robotic arm policy with methods to generate and interpolate trajectories. It imports necessary libraries, handles injecting noise, and includes utility functions.",
        "type": "comment"
    },
    "478": {
        "file_id": 28,
        "content": "        return xyz, quat, gripper\n    def __call__(self, ts):\n        # generate trajectory at first timestep, then open-loop execution\n        if self.step_count == 0:\n            self.generate_trajectory(ts)\n        # obtain left and right waypoints\n        if self.left_trajectory[0]['t'] == self.step_count:\n            self.curr_left_waypoint = self.left_trajectory.pop(0)\n        next_left_waypoint = self.left_trajectory[0]\n        if self.right_trajectory[0]['t'] == self.step_count:\n            self.curr_right_waypoint = self.right_trajectory.pop(0)\n        next_right_waypoint = self.right_trajectory[0]\n        # interpolate between waypoints to obtain current pose and gripper command\n        left_xyz, left_quat, left_gripper = self.interpolate(self.curr_left_waypoint, next_left_waypoint, self.step_count)\n        right_xyz, right_quat, right_gripper = self.interpolate(self.curr_right_waypoint, next_right_waypoint, self.step_count)\n        # Inject noise\n        if self.inject_noise:\n            scale = 0.01",
        "type": "code",
        "location": "/scripted_policy.py:34-56"
    },
    "479": {
        "file_id": 28,
        "content": "This code is responsible for executing a pre-generated trajectory by interpolating between waypoints, obtaining the current pose and gripper command for both left and right sides. It also allows injecting noise if enabled. The function is called at each timestep to update the pose and gripper commands.",
        "type": "comment"
    },
    "480": {
        "file_id": 28,
        "content": "            left_xyz = left_xyz + np.random.uniform(-scale, scale, left_xyz.shape)\n            right_xyz = right_xyz + np.random.uniform(-scale, scale, right_xyz.shape)\n        action_left = np.concatenate([left_xyz, left_quat, [left_gripper]])\n        action_right = np.concatenate([right_xyz, right_quat, [right_gripper]])\n        self.step_count += 1\n        return np.concatenate([action_left, action_right])\nclass PickAndTransferPolicy(BasePolicy):\n    def generate_trajectory(self, ts_first):\n        init_mocap_pose_right = ts_first.observation['mocap_pose_right']\n        init_mocap_pose_left = ts_first.observation['mocap_pose_left']\n        box_info = np.array(ts_first.observation['env_state'])\n        box_xyz = box_info[:3]\n        box_quat = box_info[3:]\n        # print(f\"Generate trajectory for {box_xyz=}\")\n        gripper_pick_quat = Quaternion(init_mocap_pose_right[3:])\n        gripper_pick_quat = gripper_pick_quat * Quaternion(axis=[0.0, 1.0, 0.0], degrees=-60)\n        meet_left_quat = Quaternion(axis=[1.0, 0.0, 0.0], degrees=90)",
        "type": "code",
        "location": "/scripted_policy.py:57-81"
    },
    "481": {
        "file_id": 28,
        "content": "The code snippet is part of a PickAndTransferPolicy class. It generates a trajectory for picking up an object and transferring it from one robot arm to another. The code adds random uniform noise to the action coordinates, concatenates the actions with quaternions and gripper states, increments the step count, and returns the combined action for both arms. The method also initializes variables based on the first time step observation, including the initial mocap poses of both robot arms and box information (XYZ and quaternion).",
        "type": "comment"
    },
    "482": {
        "file_id": 28,
        "content": "        meet_xyz = np.array([0, 0.5, 0.25])\n        self.left_trajectory = [\n            {\"t\": 0, \"xyz\": init_mocap_pose_left[:3], \"quat\": init_mocap_pose_left[3:], \"gripper\": 0}, # sleep\n            {\"t\": 100, \"xyz\": meet_xyz + np.array([-0.1, 0, -0.02]), \"quat\": meet_left_quat.elements, \"gripper\": 1}, # approach meet position\n            {\"t\": 260, \"xyz\": meet_xyz + np.array([0.02, 0, -0.02]), \"quat\": meet_left_quat.elements, \"gripper\": 1}, # move to meet position\n            {\"t\": 310, \"xyz\": meet_xyz + np.array([0.02, 0, -0.02]), \"quat\": meet_left_quat.elements, \"gripper\": 0}, # close gripper\n            {\"t\": 360, \"xyz\": meet_xyz + np.array([-0.1, 0, -0.02]), \"quat\": np.array([1, 0, 0, 0]), \"gripper\": 0}, # move left\n            {\"t\": 400, \"xyz\": meet_xyz + np.array([-0.1, 0, -0.02]), \"quat\": np.array([1, 0, 0, 0]), \"gripper\": 0}, # stay\n        ]\n        self.right_trajectory = [\n            {\"t\": 0, \"xyz\": init_mocap_pose_right[:3], \"quat\": init_mocap_pose_right[3:], \"gripper\": 0}, # sleep",
        "type": "code",
        "location": "/scripted_policy.py:83-95"
    },
    "483": {
        "file_id": 28,
        "content": "Code defines trajectory for left and right robot arms. Left arm starts by sleeping, then approaches and moves to meet position, closes gripper, moves left, and stays at final position. Right arm also sleeps, follows similar steps as left arm. All movements are time-based.",
        "type": "comment"
    },
    "484": {
        "file_id": 28,
        "content": "            {\"t\": 90, \"xyz\": box_xyz + np.array([0, 0, 0.08]), \"quat\": gripper_pick_quat.elements, \"gripper\": 1}, # approach the cube\n            {\"t\": 130, \"xyz\": box_xyz + np.array([0, 0, -0.015]), \"quat\": gripper_pick_quat.elements, \"gripper\": 1}, # go down\n            {\"t\": 170, \"xyz\": box_xyz + np.array([0, 0, -0.015]), \"quat\": gripper_pick_quat.elements, \"gripper\": 0}, # close gripper\n            {\"t\": 200, \"xyz\": meet_xyz + np.array([0.05, 0, 0]), \"quat\": gripper_pick_quat.elements, \"gripper\": 0}, # approach meet position\n            {\"t\": 220, \"xyz\": meet_xyz, \"quat\": gripper_pick_quat.elements, \"gripper\": 0}, # move to meet position\n            {\"t\": 310, \"xyz\": meet_xyz, \"quat\": gripper_pick_quat.elements, \"gripper\": 1}, # open gripper\n            {\"t\": 360, \"xyz\": meet_xyz + np.array([0.1, 0, 0]), \"quat\": gripper_pick_quat.elements, \"gripper\": 1}, # move to right\n            {\"t\": 400, \"xyz\": meet_xyz + np.array([0.1, 0, 0]), \"quat\": gripper_pick_quat.elements, \"gripper\": 1}, # stay",
        "type": "code",
        "location": "/scripted_policy.py:96-103"
    },
    "485": {
        "file_id": 28,
        "content": "This code represents a sequence of actions for a robot gripper. It begins by approaching and gripping the cube, then moving downwards, closing the gripper at a certain position, moving to a meet position, opening the gripper, and finally moving right and staying in that position. The actions are time-based with specific positions and gripper states.",
        "type": "comment"
    },
    "486": {
        "file_id": 28,
        "content": "        ]\nclass InsertionPolicy(BasePolicy):\n    def generate_trajectory(self, ts_first):\n        init_mocap_pose_right = ts_first.observation['mocap_pose_right']\n        init_mocap_pose_left = ts_first.observation['mocap_pose_left']\n        peg_info = np.array(ts_first.observation['env_state'])[:7]\n        peg_xyz = peg_info[:3]\n        peg_quat = peg_info[3:]\n        socket_info = np.array(ts_first.observation['env_state'])[7:]\n        socket_xyz = socket_info[:3]\n        socket_quat = socket_info[3:]\n        gripper_pick_quat_right = Quaternion(init_mocap_pose_right[3:])\n        gripper_pick_quat_right = gripper_pick_quat_right * Quaternion(axis=[0.0, 1.0, 0.0], degrees=-60)\n        gripper_pick_quat_left = Quaternion(init_mocap_pose_right[3:])\n        gripper_pick_quat_left = gripper_pick_quat_left * Quaternion(axis=[0.0, 1.0, 0.0], degrees=60)\n        meet_xyz = np.array([0, 0.5, 0.15])\n        lift_right = 0.00715\n        self.left_trajectory = [\n            {\"t\": 0, \"xyz\": init_mocap_pose_left[:3], \"quat\": init_mocap_pose_left[3:], \"gripper\": 0}, # sleep",
        "type": "code",
        "location": "/scripted_policy.py:104-131"
    },
    "487": {
        "file_id": 28,
        "content": "This code initializes variables for the InsertionPolicy class's generate_trajectory method. It extracts information from the observation and calculates gripper quaternions for both hands, defining their starting positions and orientation. The meet_xyz variable represents a specific target position, while lift_right is an arbitrary value. The left_trajectory list is initialized with the first point as the initial mocap pose of the left hand in sleep mode.",
        "type": "comment"
    },
    "488": {
        "file_id": 28,
        "content": "            {\"t\": 120, \"xyz\": socket_xyz + np.array([0, 0, 0.08]), \"quat\": gripper_pick_quat_left.elements, \"gripper\": 1}, # approach the cube\n            {\"t\": 170, \"xyz\": socket_xyz + np.array([0, 0, -0.03]), \"quat\": gripper_pick_quat_left.elements, \"gripper\": 1}, # go down\n            {\"t\": 220, \"xyz\": socket_xyz + np.array([0, 0, -0.03]), \"quat\": gripper_pick_quat_left.elements, \"gripper\": 0}, # close gripper\n            {\"t\": 285, \"xyz\": meet_xyz + np.array([-0.1, 0, 0]), \"quat\": gripper_pick_quat_left.elements, \"gripper\": 0}, # approach meet position\n            {\"t\": 340, \"xyz\": meet_xyz + np.array([-0.05, 0, 0]), \"quat\": gripper_pick_quat_left.elements,\"gripper\": 0},  # insertion\n            {\"t\": 400, \"xyz\": meet_xyz + np.array([-0.05, 0, 0]), \"quat\": gripper_pick_quat_left.elements, \"gripper\": 0},  # insertion\n        ]\n        self.right_trajectory = [\n            {\"t\": 0, \"xyz\": init_mocap_pose_right[:3], \"quat\": init_mocap_pose_right[3:], \"gripper\": 0}, # sleep\n            {\"t\": 12",
        "type": "code",
        "location": "/scripted_policy.py:132-142"
    },
    "489": {
        "file_id": 28,
        "content": "This code defines a list of trajectory points for left and right arms, specifying their xyz coordinates, orientation quaternion, and gripper state at each time step. It follows a sequence of actions such as approaching the cube, going down, closing the gripper, and reaching insertion positions.",
        "type": "comment"
    },
    "490": {
        "file_id": 28,
        "content": "0, \"xyz\": peg_xyz + np.array([0, 0, 0.08]), \"quat\": gripper_pick_quat_right.elements, \"gripper\": 1}, # approach the cube\n            {\"t\": 170, \"xyz\": peg_xyz + np.array([0, 0, -0.03]), \"quat\": gripper_pick_quat_right.elements, \"gripper\": 1}, # go down\n            {\"t\": 220, \"xyz\": peg_xyz + np.array([0, 0, -0.03]), \"quat\": gripper_pick_quat_right.elements, \"gripper\": 0}, # close gripper\n            {\"t\": 285, \"xyz\": meet_xyz + np.array([0.1, 0, lift_right]), \"quat\": gripper_pick_quat_right.elements, \"gripper\": 0}, # approach meet position\n            {\"t\": 340, \"xyz\": meet_xyz + np.array([0.05, 0, lift_right]), \"quat\": gripper_pick_quat_right.elements, \"gripper\": 0},  # insertion\n            {\"t\": 400, \"xyz\": meet_xyz + np.array([0.05, 0, lift_right]), \"quat\": gripper_pick_quat_right.elements, \"gripper\": 0},  # insertion\n        ]\ndef test_policy(task_name):\n    # example rolling out pick_and_transfer policy\n    onscreen_render = True\n    inject_noise = False\n    # setup the environment\n    episode_len = SIM_TASK_CONFIGS[task_name]['episode_len']",
        "type": "code",
        "location": "/scripted_policy.py:142-158"
    },
    "491": {
        "file_id": 28,
        "content": "This code defines a policy for picking up and transferring an object, with specific timings and positions. The policy is applied within the `test_policy` function, which also sets up the environment and allows for onscreen rendering and noise injection.",
        "type": "comment"
    },
    "492": {
        "file_id": 28,
        "content": "    if 'sim_transfer_cube' in task_name:\n        env = make_ee_sim_env('sim_transfer_cube')\n    elif 'sim_insertion' in task_name:\n        env = make_ee_sim_env('sim_insertion')\n    else:\n        raise NotImplementedError\n    for episode_idx in range(2):\n        ts = env.reset()\n        episode = [ts]\n        if onscreen_render:\n            ax = plt.subplot()\n            plt_img = ax.imshow(ts.observation['images']['angle'])\n            plt.ion()\n        policy = PickAndTransferPolicy(inject_noise)\n        for step in range(episode_len):\n            action = policy(ts)\n            ts = env.step(action)\n            episode.append(ts)\n            if onscreen_render:\n                plt_img.set_data(ts.observation['images']['angle'])\n                plt.pause(0.02)\n        plt.close()\n        episode_return = np.sum([ts.reward for ts in episode[1:]])\n        if episode_return > 0:\n            print(f\"{episode_idx=} Successful, {episode_return=}\")\n        else:\n            print(f\"{episode_idx=} Failed\")\nif __name__ == '__main__':",
        "type": "code",
        "location": "/scripted_policy.py:159-191"
    },
    "493": {
        "file_id": 28,
        "content": "The code initializes an environment (env) depending on the task_name, and then executes two episodes of actions. For each episode, it resets the environment, performs actions based on a PickAndTransferPolicy, and updates the state. If onscreen_render is True, it renders the state using matplotlib. It calculates the episode return and prints whether the episode was successful or not based on the return value. The code is called as a main function.",
        "type": "comment"
    },
    "494": {
        "file_id": 28,
        "content": "    test_task_name = 'sim_transfer_cube_scripted'\n    test_policy(test_task_name)",
        "type": "code",
        "location": "/scripted_policy.py:192-193"
    },
    "495": {
        "file_id": 28,
        "content": "The code is calling a test_policy function with the task name \"sim_transfer_cube_scripted\". This suggests it's testing a simulation script for transferring a cube.",
        "type": "comment"
    },
    "496": {
        "file_id": 29,
        "content": "/setup.py",
        "type": "filepath"
    },
    "497": {
        "file_id": 29,
        "content": "This code is a Python setup script that utilizes the distutils and setuptools packages to create a distribution package for the 'act' software. It specifies the name, version, packages, license, and long_description of the software.",
        "type": "summary"
    },
    "498": {
        "file_id": 29,
        "content": "from distutils.core import setup\nfrom setuptools import find_packages\nsetup(\n    name='act',\n    version='0.0.0',\n    packages=find_packages(),\n    license='MIT License',\n    long_description=open('README.md').read(),\n)",
        "type": "code",
        "location": "/setup.py:1-10"
    },
    "499": {
        "file_id": 29,
        "content": "This code is a Python setup script that utilizes the distutils and setuptools packages to create a distribution package for the 'act' software. It specifies the name, version, packages, license, and long_description of the software.",
        "type": "comment"
    }
}