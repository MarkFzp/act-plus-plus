{
    "summary": "Latent_Model_Transformer extends nn.Module, uses self-attention for latent space sequence modeling, has configurable input/output dimensions and sequence length, defaulting to 256 latent dimension, 8 heads, and 3 layers. The class has 'forward' and 'generate' methods for generating new samples by iteratively sampling from the output of the forward pass using temperature-scaled softmax and one-hot encoding.",
    "details": [
        {
            "comment": "Causal Transformer block: LayerNormalization, MultiHeadAttention with dropout, and MLP sequential layers.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/latent_model.py\":0-27",
            "content": "import torch.nn as nn\nfrom torch.nn import functional as F\nimport torch\nDROPOUT_RATE = 0.1\n# a causal transformer block\nclass Causal_Transformer_Block(nn.Module):\n    def __init__(self, seq_len, latent_dim, num_head) -> None:\n        super().__init__()\n        self.num_head = num_head\n        self.latent_dim = latent_dim\n        self.ln_1 = nn.LayerNorm(latent_dim)\n        self.attn = nn.MultiheadAttention(latent_dim, num_head, dropout=DROPOUT_RATE, batch_first=True)\n        self.ln_2 = nn.LayerNorm(latent_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(latent_dim, 4 * latent_dim),\n            nn.GELU(),\n            nn.Linear(4 * latent_dim, latent_dim),\n            nn.Dropout(DROPOUT_RATE),\n        )\n        # self.register_buffer(\"attn_mask\", torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool())\n    def forward(self, x):\n        attn_mask = torch.triu(torch.ones(x.shape[1], x.shape[1], device=x.device, dtype=torch.bool), diagonal=1)\n        x = self.ln_1(x)\n        x = x + self.attn(x, x, x, attn_mask=attn_mask)[0]"
        },
        {
            "comment": "In \"act-plus-plus/detr/models/latent_model.py\", lines 28-54 define a class Latent_Model_Transformer that extends nn.Module. This model uses self-attention instead of RNN to model the latent space sequence. It takes an input dimension, output dimension, sequence length, latent dimension (default 256), number of heads (default 8), and number of layers (default 3). The forward method applies an input layer, adds positional embedding, passes through a series of causal transformer blocks, and finally outputs through an output layer.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/latent_model.py\":28-54",
            "content": "        x = self.ln_2(x)\n        x = x + self.mlp(x)\n        return x\n# use self-attention instead of RNN to model the latent space sequence\nclass Latent_Model_Transformer(nn.Module):\n    def __init__(self, input_dim, output_dim, seq_len, latent_dim=256, num_head=8, num_layer=3) -> None:\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.seq_len = seq_len\n        self.latent_dim = latent_dim\n        self.num_head = num_head\n        self.num_layer = num_layer\n        self.input_layer = nn.Linear(input_dim, latent_dim)\n        self.weight_pos_embed = nn.Embedding(seq_len, latent_dim)\n        self.attention_blocks = nn.Sequential(\n            nn.Dropout(DROPOUT_RATE),\n            *[Causal_Transformer_Block(seq_len, latent_dim, num_head) for _ in range(num_layer)],\n            nn.LayerNorm(latent_dim)\n        )\n        self.output_layer = nn.Linear(latent_dim, output_dim)\n    def forward(self, x):\n        x = self.input_layer(x)\n        x = x + self.weight_pos_embed(torch.arange(x.shape[1], device=x.device))"
        },
        {
            "comment": "This code defines a class with two methods: 'forward' and 'generate'. The 'forward' method applies attention blocks to the input, then passes it through an output layer to produce logits. The 'generate' method generates new samples by iteratively sampling from the output of the forward pass using temperature-scaled softmax and one-hot encoding. The generated samples are appended to the original input and returned after trimming unnecessary rows.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/latent_model.py\":55-71",
            "content": "        x = self.attention_blocks(x)\n        logits = self.output_layer(x)\n        return logits\n    @torch.no_grad()\n    def generate(self, n, temperature=0.1, x=None):\n        if x is None:\n            x = torch.zeros((n, 1, self.input_dim), device=self.weight_pos_embed.weight.device)\n        for i in range(self.seq_len):\n            logits = self.forward(x)[:, -1]\n            probs = torch.softmax(logits / temperature, dim=-1)\n            samples = torch.multinomial(probs, num_samples=1)[..., 0]\n            samples_one_hot = F.one_hot(samples.long(), num_classes=self.output_dim).float()\n            x = torch.cat([x, samples_one_hot[:, None, :]], dim=1)\n        return x[:, 1:, :]"
        }
    ]
}