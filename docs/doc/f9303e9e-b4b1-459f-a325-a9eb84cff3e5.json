{
    "summary": "The code defines a Transformer class in PyTorch for data processing, featuring encoder and decoder modules, positional embeddings, transformer layers, and optional masks and position embeddings.",
    "details": [
        {
            "comment": "This code defines the Transformer class from scratch with minor modifications to the original implementation, including passing positional encodings in MHAttention, removing an extra LN layer in the encoder, and allowing for intermediate decoder activations to be returned. It inherits from nn.Module and has several parameters for customization.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":0-29",
            "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\"\"\"\nDETR Transformer class.\nCopy-paste from torch.nn.Transformer with modifications:\n    * positional encodings are passed in MHattention\n    * extra LN at the end of encoder is removed\n    * decoder returns a stack of activations from all decoding layers\n\"\"\"\nimport copy\nfrom typing import Optional, List\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, Tensor\nimport IPython\ne = IPython.embed\nclass Transformer(nn.Module):\n    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n                 activation=\"relu\", normalize_before=False,\n                 return_intermediate_dec=False):\n        super().__init__()\n        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n                                                dropout, activation, normalize_before)\n        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None"
        },
        {
            "comment": "This code initializes a Transformer model with an encoder and decoder, performing parameter initialization and normalization. It also includes a forward method for processing input data with possible flattening for images.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":30-53",
            "content": "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n                                                dropout, activation, normalize_before)\n        decoder_norm = nn.LayerNorm(d_model)\n        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n                                          return_intermediate=return_intermediate_dec)\n        self._reset_parameters()\n        self.d_model = d_model\n        self.nhead = nhead\n    def _reset_parameters(self):\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n    def forward(self, src, mask, query_embed, pos_embed, latent_input=None, proprio_input=None, additional_pos_embed=None):\n        # TODO flatten only when input has H and W\n        if len(src.shape) == 4: # has H and W\n            # flatten NxCxHxW to HWxNxC\n            bs, c, h, w = src.shape\n            src = src.flatten(2).permute(2, 0, 1)"
        },
        {
            "comment": "The code initializes the transformer model by handling different source (src) input shapes. It either flattens and repeats the inputs if the shape is bs, hw, c or simply permutes and repeats if the shape is NxHWxC. Positional embeddings are calculated for both position and additional positional information. The decoder uses these embeddings to process target (tgt) and source memory.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":54-74",
            "content": "            pos_embed = pos_embed.flatten(2).permute(2, 0, 1).repeat(1, bs, 1)\n            query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n            # mask = mask.flatten(1)\n            additional_pos_embed = additional_pos_embed.unsqueeze(1).repeat(1, bs, 1) # seq, bs, dim\n            pos_embed = torch.cat([additional_pos_embed, pos_embed], axis=0)\n            addition_input = torch.stack([latent_input, proprio_input], axis=0)\n            src = torch.cat([addition_input, src], axis=0)\n        else:\n            assert len(src.shape) == 3\n            # flatten NxHWxC to HWxNxC\n            bs, hw, c = src.shape\n            src = src.permute(1, 0, 2)\n            pos_embed = pos_embed.unsqueeze(1).repeat(1, bs, 1)\n            query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n        tgt = torch.zeros_like(query_embed)\n        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n                          pos=pos_embed, query_pos=query_embed)"
        },
        {
            "comment": "This code defines two classes: TransformerEncoder and TransformerDecoder. The TransformerEncoder class initializes an encoder with a specified number of layers and normalization method, then forwards input through each layer in the encoder. The TransformerDecoder class initializes a decoder with a specified number of layers and normalization method, then forwards input through each layer in the decoder. Both classes can handle optional masks and positions during forward propagation.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":75-108",
            "content": "        hs = hs.transpose(1, 2)\n        return hs\nclass TransformerEncoder(nn.Module):\n    def __init__(self, encoder_layer, num_layers, norm=None):\n        super().__init__()\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n    def forward(self, src,\n                mask: Optional[Tensor] = None,\n                src_key_padding_mask: Optional[Tensor] = None,\n                pos: Optional[Tensor] = None):\n        output = src\n        for layer in self.layers:\n            output = layer(output, src_mask=mask,\n                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n        if self.norm is not None:\n            output = self.norm(output)\n        return output\nclass TransformerDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm"
        },
        {
            "comment": "The code defines a Transformer model's forward pass, where each layer applies its operations iteratively on the target (tgt) and memory inputs. The intermediate results are stored if return_intermediate is set to True. Finally, the norm layer normalizes the output, and if return_intermediate is set, stores the normalized outputs as intermediates.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":109-133",
            "content": "        self.return_intermediate = return_intermediate\n    def forward(self, tgt, memory,\n                tgt_mask: Optional[Tensor] = None,\n                memory_mask: Optional[Tensor] = None,\n                tgt_key_padding_mask: Optional[Tensor] = None,\n                memory_key_padding_mask: Optional[Tensor] = None,\n                pos: Optional[Tensor] = None,\n                query_pos: Optional[Tensor] = None):\n        output = tgt\n        intermediate = []\n        for layer in self.layers:\n            output = layer(output, memory, tgt_mask=tgt_mask,\n                           memory_mask=memory_mask,\n                           tgt_key_padding_mask=tgt_key_padding_mask,\n                           memory_key_padding_mask=memory_key_padding_mask,\n                           pos=pos, query_pos=query_pos)\n            if self.return_intermediate:\n                intermediate.append(self.norm(output))\n        if self.norm is not None:\n            output = self.norm(output)\n            if self.return_intermediate:"
        },
        {
            "comment": "This code defines a class called \"TransformerEncoderLayer\" which implements a layer for the transformer encoder in the Transformer model. It consists of a self-attention mechanism, followed by a feedforward network and normalization layers. The \"return_intermediate\" parameter controls whether intermediate results are returned or not.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":134-162",
            "content": "                intermediate.pop()\n                intermediate.append(output)\n        if self.return_intermediate:\n            return torch.stack(intermediate)\n        return output.unsqueeze(0)\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                 activation=\"relu\", normalize_before=False):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.activation = _get_activation_fn(activation)\n        self.normalize_before = normalize_before\n    def with_pos_embed(self, tensor, pos: Optional[Tensor]):"
        },
        {
            "comment": "This code defines three functions: `forward_post`, `forward_pre`, and a helper function that calculates the tensor based on positional embeddings. The `forward_post` function applies self-attention to the input source, adds it back to the original source, and performs two feed-forward layers with residual connections and layer normalization for each of them. The `forward_pre` function applies layer normalization to the input source, calculates self-attention based on positional embeddings, and performs two feed-forward layers similar to `forward_post`. The code seems to be part of a transformer model in natural language processing or computer vision tasks that incorporate position information.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":163-186",
            "content": "        return tensor if pos is None else tensor + pos\n    def forward_post(self,\n                     src,\n                     src_mask: Optional[Tensor] = None,\n                     src_key_padding_mask: Optional[Tensor] = None,\n                     pos: Optional[Tensor] = None):\n        q = k = self.with_pos_embed(src, pos)\n        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n                              key_padding_mask=src_key_padding_mask)[0]\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src\n    def forward_pre(self, src,\n                    src_mask: Optional[Tensor] = None,\n                    src_key_padding_mask: Optional[Tensor] = None,\n                    pos: Optional[Tensor] = None):\n        src2 = self.norm1(src)\n        q = k = self.with_pos_embed(src2, pos)\n        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,"
        },
        {
            "comment": "This code defines a TransformerDecoderLayer class that inherits from nn.Module and takes in parameters such as d_model, nhead, dim_feedforward, dropout, activation, and normalize_before. The class has methods for forward pass and initializing the layer. It also includes an instance of MultiheadAttention for self attention and multi-headed attention.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":187-209",
            "content": "                              key_padding_mask=src_key_padding_mask)[0]\n        src = src + self.dropout1(src2)\n        src2 = self.norm2(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n        src = src + self.dropout2(src2)\n        return src\n    def forward(self, src,\n                src_mask: Optional[Tensor] = None,\n                src_key_padding_mask: Optional[Tensor] = None,\n                pos: Optional[Tensor] = None):\n        if self.normalize_before:\n            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\nclass TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                 activation=\"relu\", normalize_before=False):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)"
        },
        {
            "comment": "This code defines a class for the Feedforward model in Transformer architecture. It includes several linear layers, dropout layers, and layer normalization. The forward_post method takes input tensors, masks, and positional embeddings as arguments to perform feed-forward operations.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":210-233",
            "content": "        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n        self.activation = _get_activation_fn(activation)\n        self.normalize_before = normalize_before\n    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n        return tensor if pos is None else tensor + pos\n    def forward_post(self, tgt, memory,\n                     tgt_mask: Optional[Tensor] = None,\n                     memory_mask: Optional[Tensor] = None,\n                     tgt_key_padding_mask: Optional[Tensor] = None,\n                     memory_key_padding_mask: Optional[Tensor] = None,\n                     pos: Optional[Tensor] = None,"
        },
        {
            "comment": "This function performs multi-head self-attention, applies layer normalization and feed-forward network layers to the target sequence. It takes in the target (tgt) and memory sequences, along with optional masking tensors for attention masks and key padding masks. It returns the processed target sequence.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":234-254",
            "content": "                     query_pos: Optional[Tensor] = None):\n        q = k = self.with_pos_embed(tgt, query_pos)\n        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n                              key_padding_mask=tgt_key_padding_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n                                   key=self.with_pos_embed(memory, pos),\n                                   value=memory, attn_mask=memory_mask,\n                                   key_padding_mask=memory_key_padding_mask)[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt\n    def forward_pre(self, tgt, memory,\n                    tgt_mask: Optional[Tensor] = None,\n                    memory_mask: Optional[Tensor] = None,\n                    tgt_key_padding_mask: Optional[Tensor] = None,"
        },
        {
            "comment": "This code defines a function for the transformer model in PyTorch. It performs self-attention on the target sequence (tgt) and applies multi-head attention to interact with memory, incorporating positional embeddings and masking for attentive processing. Finally, it passes through a feed-forward network and dropout layers before returning the modified target sequence.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":255-274",
            "content": "                    memory_key_padding_mask: Optional[Tensor] = None,\n                    pos: Optional[Tensor] = None,\n                    query_pos: Optional[Tensor] = None):\n        tgt2 = self.norm1(tgt)\n        q = k = self.with_pos_embed(tgt2, query_pos)\n        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n                              key_padding_mask=tgt_key_padding_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt2 = self.norm2(tgt)\n        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n                                   key=self.with_pos_embed(memory, pos),\n                                   value=memory, attn_mask=memory_mask,\n                                   key_padding_mask=memory_key_padding_mask)[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt2 = self.norm3(tgt)\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n        tgt = tgt + self.dropout3(tgt2)\n        return tgt\n    def forward(self, tgt, memory,"
        },
        {
            "comment": "The code defines a Transformer model with optional masks and position embeddings, using deepcopy to create N identical modules for parallel processing. The build_transformer function initializes the Transformer model with given argument values.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":275-298",
            "content": "                tgt_mask: Optional[Tensor] = None,\n                memory_mask: Optional[Tensor] = None,\n                tgt_key_padding_mask: Optional[Tensor] = None,\n                memory_key_padding_mask: Optional[Tensor] = None,\n                pos: Optional[Tensor] = None,\n                query_pos: Optional[Tensor] = None):\n        if self.normalize_before:\n            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\ndef _get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\ndef build_transformer(args):\n    return Transformer(\n        d_model=args.hidden_dim,\n        dropout=args.dropout,\n        nhead=args.nheads,\n        dim_feedforward=args.dim_feedforward,\n        num_encoder_layers=args.enc_layers,"
        },
        {
            "comment": "This code defines a function for creating a transformer model with specified parameters and returns an activation function based on the input string.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/transformer.py\":299-313",
            "content": "        num_decoder_layers=args.dec_layers,\n        normalize_before=args.pre_norm,\n        return_intermediate_dec=True,\n    )\ndef _get_activation_fn(activation):\n    \"\"\"Return an activation function given a string\"\"\"\n    if activation == \"relu\":\n        return F.relu\n    if activation == \"gelu\":\n        return F.gelu\n    if activation == \"glu\":\n        return F.glu\n    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")"
        }
    ]
}