{
    "summary": "This code defines a function for nearest neighbor calculation, performs rollouts, and preprocesses features for image classification tasks. It uses command-line arguments to run the script with specific directories and checkpoints.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines a function that calculates nearest neighbors for a given feature. The function takes the current feature, support inputs, support targets, number of neighbors to consider (k), and state weight as input parameters. It also handles cases where there is an action skip in the support targets by reshaping them before processing. The code defines separate features for visual and spatial modalities (curr_vis_feature, curr_s_feature, support_vis_feature, support_s_feature).",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":0-34",
            "content": "import torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\nimport h5py\nimport pathlib\nimport os\nimport argparse\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torchvision\nfrom torchvision import transforms\n# from visualize_episodes import visualize_joints\nfrom utils import set_seed, sample_box_pose\n# from imitate_episodes import get_image\nfrom sim_env import BOX_POSE\nfrom constants import DT\nfrom imitate_episodes import save_videos\nfrom einops import rearrange\nimport time\nDT = 0.02\nimport IPython\ne = IPython.embed\n# modified from https://github.com/jyopari/VINN/blob/main/nearest-neighbor-eval/handle_nn.ipynb\ndef calculate_nearest_neighbors(curr_feature, support_inputs, support_targets, k, state_weight):\n    has_skip = len(support_targets.shape) == 3\n    if has_skip: # when there is action skip\n        num_targets, skip, a_dim = support_targets.shape\n        support_targets = support_targets.view((num_targets, -1))\n    curr_vis_feature, curr_s_feature = curr_feature\n    support_vis_feature, support_s_feature = support_inputs"
        },
        {
            "comment": "The code calculates pairwise distances between current and support features, sorts them, and assigns weights to the top-k distances. It then uses these weights to create a weighted sum of support targets as the prediction. The function takes arguments 'args', but they are not used in this snippet. Additionally, it allows skipping predictions for every 100th frame with 'has_skip' flag.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":36-62",
            "content": "    pairwise_dist_vis = torch.norm(curr_vis_feature - support_vis_feature, dim=1).unsqueeze(0)\n    pairwise_dist_s = torch.norm(curr_s_feature - support_s_feature, dim=1).unsqueeze(0)\n    pairwise_dist = pairwise_dist_vis + pairwise_dist_s * state_weight\n    sorted_dist, index = torch.sort(pairwise_dist, dim=1) # sort the support axis\n    permuted_support_targets = support_targets[index]\n    topk_dist = pairwise_dist[:, :k]\n    topk_support_targets = permuted_support_targets[:, :k]\n    weights = F.softmax(-topk_dist, dim=1)\n    weighted_support_targets = weights.unsqueeze(2) * topk_support_targets\n    prediction = torch.sum(weighted_support_targets, dim=1)\n    if has_skip:\n        num_predictions = prediction.shape[0]\n        prediction = prediction.reshape((num_predictions, skip, a_dim))\n    return prediction\ndef main(args):\n    # TODO ######################\n    k = None # for scripted box transfer\n    skip = 100\n    real_robot = True\n    save_episode = True\n    # TODO ######################\n    onscreen_cam = 'main'"
        },
        {
            "comment": "This code sets various parameters and configurations for different tasks based on the task name provided. It assigns specific episode lengths, maximum rewards, kernel sizes (ks), and state weights depending on the task type. If the task is not implemented, it raises a NotImplementedError. The model name's last part before the file extension is used as the seed, and the representation type is set to 'byol'. For models with 'cotrain' in their names, it assigns the repr_type accordingly.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":63-100",
            "content": "    state_dim = 14\n    dataset_dir = args['dataset_dir']\n    onscreen_render = args['onscreen_render']\n    ckpt_dir = args['ckpt_dir']\n    model_dir = args['model_dir']\n    task_name = args['task_name']\n    if 'insertion' in task_name:\n        sim_episode_len = 400\n        env_max_reward = 4\n        ks = [None]\n    elif 'transfer_cube' in task_name:\n        sim_episode_len = 400\n        env_max_reward = 4\n        ks = [1, 1, 1]\n        if 'human' in dataset_dir:\n            state_weight = 5\n        else:\n            state_weight = 10\n        print(f'{state_weight=}')\n    elif task_name == 'ziploc_slide':\n        env_max_reward = 1\n        ks = [71]\n        state_weight = 0\n    elif task_name == 'aloha_mobile_wipe_wine':\n        sim_episode_len = 1300\n        env_max_reward = 4\n        ks = [2, 2, 2]\n        state_weight = 5\n        print(f'{state_weight=}')\n    else:\n        raise NotImplementedError\n    model_name = pathlib.PurePath(model_dir).name\n    seed = int(model_name.split('-')[-1][:-3])\n    repr_type = 'byol'\n    if 'cotrain' in model_name:"
        },
        {
            "comment": "This code loads train data by iterating over 40 episodes. It retrieves action, base_action, and camera names from a dataset file. For each episode, it concatenates the visual features of all cameras into 'vis_fea'. The repr_type is extended with '_cotrain', and BASE_DELAY is set to 15 for real_robot cases.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":101-129",
            "content": "        repr_type += '_cotrain'\n    e() # make sure!\n    k = ks[seed]\n    if real_robot:\n        BASE_DELAY = 15\n        query_freq = skip - BASE_DELAY\n    # load train data\n    vis_features = []\n    state_features = []\n    Y = []\n    for episode_id in range(0, 40):\n        dataset_path = os.path.join(dataset_dir, f'episode_{episode_id}.hdf5')\n        with h5py.File(dataset_path, 'r') as root:\n            action = root['/action'][:]\n            base_action = root['/base_action'][:]\n            action = np.concatenate([action, base_action], axis=1)\n            camera_names = list(root[f'/observations/images/'].keys())\n        # Visual feature\n        all_cam_feature = []\n        for cam_name in camera_names:\n            feature_dataset_path = os.path.join(dataset_dir, f'{repr_type}_features_seed{seed}_episode_{episode_id}.hdf5')\n            with h5py.File(feature_dataset_path, 'r') as root:\n                cam_feature = root[f'/features/{cam_name}'][:]\n                all_cam_feature.append(cam_feature)\n        vis_fea = np.concatenate(all_cam_feature, axis=1)"
        },
        {
            "comment": "This code reads episode data from a file, stacks actions together, appends them to feature lists, and then concatenates the feature lists. Finally, it creates torch tensors for training inputs.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":131-153",
            "content": "        ## State feature\n        dataset_path = os.path.join(dataset_dir, f'episode_{episode_id}.hdf5')\n        with h5py.File(dataset_path, 'r') as root:\n            s_fea = root['/observations/qpos'][:]\n        # stack actions together\n        eps_len = len(action)\n        indices = np.tile(np.arange(skip), eps_len).reshape(eps_len, skip) # each row is 0, 1, ... skip\n        offset = np.expand_dims(np.arange(eps_len), axis=1)\n        indices = indices + offset # row1: 0, 1, ... skip; row2: 1, 2, ... skip+1\n        # indices will exceed eps_len, thus clamp to eps_len-1\n        indices = np.clip(indices, 0, eps_len-1)\n        # stack action\n        action = action[indices] # new shape: eps_len, skip, a_dim\n        vis_features.append(vis_fea)\n        state_features.append(s_fea)\n        Y.append(action)\n    vis_features = np.concatenate(vis_features)\n    state_features  = np.concatenate(state_features)\n    Y = np.concatenate(Y)\n    train_inputs = [torch.from_numpy(vis_features).cuda(), torch.from_numpy(state_features).cuda()]"
        },
        {
            "comment": "The code initializes feature extractors for each camera, loads the environment based on real_robot flag, and starts a loop to perform rollouts. It creates episode returns and maximum rewards lists for tracking performance metrics during the rollouts.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":154-184",
            "content": "    train_targets = torch.from_numpy(Y).cuda()\n    set_seed(1000)\n    feature_extractors = {}\n    for cam_name in camera_names:\n        resnet = torchvision.models.resnet18(pretrained=True)\n        loading_status = resnet.load_state_dict(torch.load(model_dir.replace('DUMMY', cam_name)))\n        print(cam_name, loading_status)\n        resnet = nn.Sequential(*list(resnet.children())[:-1])\n        resnet = resnet.cuda()\n        resnet.eval()\n        feature_extractors[cam_name] = resnet\n    # load environment\n    if real_robot:\n        from aloha_scripts.real_env import make_real_env #### TODO TODO\n        env = make_real_env(init_node=True, setup_robots=True, setup_base=True)\n        max_timesteps = sim_episode_len\n        camera_names = ['cam_high', 'cam_left_wrist', 'cam_right_wrist']\n    else:\n        from sim_env import make_sim_env\n        env = make_sim_env(task_name)\n        max_timesteps = sim_episode_len\n    num_rollouts = 50\n    episode_returns = []\n    max_rewards = []\n    for rollout_id in range(num_rollouts):"
        },
        {
            "comment": "This code sets up a task, resets the environment, and enters an evaluation loop. It collects data for visualization, including qpos and images, and stores them in lists. The code is performing these actions at specific intervals based on the provided conditions.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":185-208",
            "content": "        ### set task\n        BOX_POSE[0] = sample_box_pose() # used in sim reset\n        ts = env.reset()\n        ### evaluation loop\n        qpos_history = torch.zeros((1, max_timesteps, state_dim)).cuda()\n        image_list = [] # for visualization\n        qpos_list = []\n        target_qpos_list = []\n        rewards = []\n        with torch.inference_mode():\n            for t in range(sim_episode_len):\n                start_time = time.time()\n                if t % 100 == 0: print(t)\n                if t % query_freq == 0:\n                    ### process previous timestep to get qpos and image_list\n                    obs = ts.observation\n                    if 'images' in obs:\n                        image_list.append(obs['images'])\n                    else:\n                        image_list.append({'main': obs['image']})\n                    qpos_numpy = np.array(obs['qpos'])\n                    # qpos = pre_process(qpos_numpy)\n                    qpos = torch.from_numpy(qpos_numpy).float().cuda().unsqueeze(0)"
        },
        {
            "comment": "This code segment processes an image for a robotics task. It stores the current qpos in history, retrieves and preprocesses raw camera images using transforms such as resizing, cropping, normalization, and tensor conversion. It then collects features from each camera using respective feature extractors and stores them in all_cam_features.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":209-228",
            "content": "                    qpos_history[:, t] = qpos\n                    _, curr_image_raw = get_image(ts, camera_names)\n                    image_size = 120\n                    transform = transforms.Compose([\n                        transforms.Resize(image_size),  # will scale the image\n                        transforms.CenterCrop(image_size),\n                        transforms.ToTensor(),\n                        transforms.Lambda(expand_greyscale),\n                        transforms.Normalize(\n                            mean=torch.tensor([0.485, 0.456, 0.406]),\n                            std=torch.tensor([0.229, 0.224, 0.225])),\n                    ])\n                    all_cam_features = []\n                    for cam_id, curr_image in enumerate(curr_image_raw):\n                        curr_image = Image.fromarray(curr_image) # TODO only one camera\n                        curr_image = transform(curr_image)\n                        curr_image = curr_image.unsqueeze(dim=0).cuda()\n                        curr_image_feature = feature_extractors[camera_names[cam_id]](curr_image)"
        },
        {
            "comment": "The code preprocesses visual and state features, calculates nearest neighbors for action selection using a specified metric, and filters out the required action based on query frequency. The resulting target position and base action are extracted for further processing.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":229-249",
            "content": "                        curr_image_feature = curr_image_feature.squeeze(3).squeeze(2)\n                        all_cam_features.append(curr_image_feature)\n                    curr_image_feature = torch.cat(all_cam_features, dim=1)\n                    ### Visual feature\n                    # curr_feature = curr_image_feature\n                    ### State feature\n                    # curr_feature = qpos\n                    ### Both features\n                    curr_feature = [curr_image_feature, qpos]\n                    action = calculate_nearest_neighbors(curr_feature, train_inputs, train_targets, k, state_weight) # TODO use this\n                    action = action.squeeze(0).cpu().numpy()\n                    action = np.concatenate([action[:-BASE_DELAY, :-2], action[BASE_DELAY:, -2:]], axis=1)\n                    print(f'Query: {(time.time() - start_time):.3f}s')\n                curr_action = action[t % query_freq]\n                target_qpos = curr_action[:-2]\n                base_action = curr_action[-2:]"
        },
        {
            "comment": "This code chunk is responsible for controlling the movement of a robot's joints, ensuring safety by clipping target positions within safe limits. It steps through the environment and saves information for visualization. If the robot is real, it sets the operating modes for the gripper and pwm.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":251-274",
            "content": "                # ### SAFETY\n                # max_a = 0.05\n                # curr_qpos = qpos.squeeze().cpu().numpy()\n                # target_qpos = target_qpos.clip(curr_qpos - max_a, curr_qpos + max_a)\n                # ### SAFETY\n                ### step the environment\n                ts = env.step(target_qpos, base_action=base_action)\n                duration = time.time() - start_time\n                # print(f'{duration:.3f}')\n                time.sleep(max(0, DT - duration))\n                ### save things for visualization\n                qpos_list.append(qpos_numpy)\n                target_qpos_list.append(target_qpos)\n                rewards.append(ts.reward)\n                # if real_robot and t != 0 and t % 60 == 0:\n                #    e()\n            plt.close()\n        if real_robot:\n            env.puppet_bot_left.dxl.robot_set_operating_modes(\"single\", \"gripper\", \"position\")\n            env.puppet_bot_right.dxl.robot_set_operating_modes(\"single\", \"gripper\", \"position\")\n            env.puppet_bot_left.dxl.robot_set_operating_modes(\"single\", \"gripper\", \"pwm\")"
        },
        {
            "comment": "This code sets the operating modes for the robot's gripper and calculates rewards, episode returns, and maximum rewards. It then prints these values and saves videos or images if required. Finally, it calculates success rate and average return and constructs a summary string.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":275-293",
            "content": "            env.puppet_bot_right.dxl.robot_set_operating_modes(\"single\", \"gripper\", \"pwm\")\n        rewards = np.array(rewards)\n        episode_return = np.sum(rewards[rewards!=None])\n        episode_returns.append(episode_return)\n        max_reward = np.max(rewards)\n        max_rewards.append(max_reward)\n        print(f'{episode_return=}, {max_reward=}')\n        if save_episode:\n            save_videos(image_list, DT, video_path=os.path.join(ckpt_dir, f'video{rollout_id}.mp4'))\n            # visualize_joints(qpos_list, target_qpos_list, plot_path=os.path.join(ckpt_dir, f'qpos{rollout_id}.png'))\n            # visualize_joints(qpos_list, example_qpos, plot_path=os.path.join(ckpt_dir, f'qpos_reference{rollout_id}.png'), label_overwrite=(\"policy\", \"dataset\"))\n    success_rate = np.mean(np.array(max_rewards) == env_max_reward)\n    avg_return = np.mean(episode_returns)\n    summary_str = f'\\nSuccess rate: {success_rate}\\nAverage return: {avg_return}\\n\\n'\n    for r in range(env_max_reward+1):\n        more_or_equal_r = (np.array(max_rewards) >= r).sum()"
        },
        {
            "comment": "This function calculates the success rate, average return, and saves results to a text file for each episode. It retrieves images from observations, processes them, and stores the current image raw data in the correct format for further processing or visualization.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":294-321",
            "content": "        more_or_equal_r_rate = more_or_equal_r / num_rollouts\n        summary_str += f'Reward >= {r}: {more_or_equal_r}/{num_rollouts} = {more_or_equal_r_rate*100}%\\n'\n    print(summary_str)\n    # save success rate to txt\n    result_file_name = f'result_{skip}_{k}' + '.txt'\n    with open(os.path.join(ckpt_dir, result_file_name), 'w') as f:\n        f.write(summary_str)\n        f.write(repr(episode_returns))\n        f.write('\\n\\n')\n        f.write(repr(max_rewards))\n    return success_rate, avg_return\ndef get_image(ts, camera_names):\n    if 'images' in ts.observation:\n        curr_images = []\n        for cam_name in camera_names:\n            curr_image = rearrange(ts.observation['images'][cam_name], 'h w c -> c h w')\n            curr_images.append(curr_image)\n        curr_image_raw = np.stack(curr_images, axis=0)\n    else:\n        curr_image_raw = rearrange(ts.observation['image'], 'h w c -> c h w')\n    curr_image = torch.from_numpy(curr_image_raw / 255.0).float().cuda().unsqueeze(0)\n    curr_image_raw = rearrange(curr_image_raw, 'b c h w -> b h w c')"
        },
        {
            "comment": "The code defines a function expand_greyscale, sets up argument parsing with required parameters like dataset_dir and model_dir, and calls main function with the parsed arguments. The main function is not defined in this chunk but is called by passing the command-line arguments as variables. It seems to be a script for running an image classification task with specific directories and checkpoints.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/vinn_eval.py\":322-335",
            "content": "    return curr_image, curr_image_raw\ndef expand_greyscale(t):\n    return t.expand(3, -1, -1)\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--onscreen_render', action='store_true')\n    parser.add_argument('--dataset_dir', action='store', type=str, help='The text to parse.', required=True)\n    parser.add_argument('--model_dir', action='store', type=str, help='model_dir', required=True)\n    parser.add_argument('--task_name', action='store', type=str, help='task_name', required=True)\n    parser.add_argument('--ckpt_dir', action='store', type=str, help='The text to parse.', required=True)\n    main(vars(parser.parse_args()))"
        }
    ]
}