{
    "summary": "This script truncates and compresses a dataset using h5py, creating an observation group with limited image data. It saves truncated datasets or videos, extracts camera names, resizes images, and requires 'act-plus-plus' for argument parsing and directory manipulation. Output dataset directory has '_truncated' suffix.",
    "details": [
        {
            "comment": "This script compresses a dataset by truncating its length and storing the compressed dataset in a new file. It checks if the output path already exists and copies non-image data directly to the output file. The script takes an input_dataset_path and an output_dataset_path as arguments, and it uses h5py library for handling HDF5 files.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/truncate_data.py\":0-34",
            "content": "\"\"\"\nExample usage:\n$ python3 script/compress_data.py --dataset_dir /scr/lucyshi/dataset/aloha_test\n\"\"\"\nimport os\nimport h5py\nimport cv2\nimport numpy as np\nimport argparse\nfrom tqdm import tqdm\n# Constants\nDT = 0.02\nJOINT_NAMES = [\"waist\", \"shoulder\", \"elbow\", \"forearm_roll\", \"wrist_angle\", \"wrist_rotate\"]\nSTATE_NAMES = JOINT_NAMES + [\"gripper\"]\nTRUNCATE_LEN = 2250\ndef compress_dataset(input_dataset_path, output_dataset_path):\n    # Check if output path exists\n    if os.path.exists(output_dataset_path):\n        print(f\"The file {output_dataset_path} already exists. Exiting...\")\n        return\n    # Load the uncompressed dataset\n    with h5py.File(input_dataset_path, 'r') as infile:\n        # Create the compressed dataset\n        with h5py.File(output_dataset_path, 'w') as outfile:\n            outfile.attrs['sim'] = infile.attrs['sim']\n            outfile.attrs['compress'] = True\n            # Copy non-image data directly\n            for key in infile.keys():\n                if key != 'observations' and key != 'compress_len':"
        },
        {
            "comment": "Truncates and compresses data, creates observation group with limited image data.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/truncate_data.py\":35-56",
            "content": "                    data = infile[key][:TRUNCATE_LEN]\n                    out_data = outfile.create_dataset(key, (TRUNCATE_LEN, data.shape[1]))\n                    out_data[:] = data\n            data_compress_len = infile['compress_len']\n            out_data_compress_len = outfile.create_dataset('compress_len', data_compress_len.shape)\n            out_data_compress_len[:] = data_compress_len\n            # Create observation group in the output\n            obs_group = infile['observations']\n            out_obs_group = outfile.create_group('observations')\n            for key in obs_group.keys():\n                if key != 'images':\n                    data = obs_group[key][:TRUNCATE_LEN]\n                    out_data = out_obs_group.create_dataset(key, (TRUNCATE_LEN, data.shape[1]))\n                    out_data[:] = data\n            image_group = obs_group['images']\n            out_image_group = out_obs_group.create_group('images')\n            for cam_name in image_group.keys():\n                data = image_group[cam_name][:TRUNCATE_LEN]"
        },
        {
            "comment": "This code saves a truncated dataset or video depending on the input format. If a list of videos is given, it extracts camera names, resizes the images, and concatenates them into a single video. It then writes the video to the specified path and prints a success message.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/truncate_data.py\":57-83",
            "content": "                out_data = out_image_group.create_dataset(cam_name, (TRUNCATE_LEN, data.shape[1]), dtype='uint8')\n                out_data[:] = data\n    print(f\"Truncated dataset saved to {output_dataset_path}\")\ndef save_videos(video, dt, video_path=None):\n    if isinstance(video, list):\n        cam_names = list(video[0].keys())\n        h, w, _ = video[0][cam_names[0]].shape\n        w = w * len(cam_names)\n        fps = int(1/dt)\n        out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n        # bitrate = 1000000\n        # out.set(cv2.VIDEOWRITER_PROP_BITRATE, bitrate)\n        for ts, image_dict in enumerate(video):\n            images = []\n            for cam_name in cam_names:\n                image = image_dict[cam_name]\n                image = image[:, :, [2, 1, 0]] # swap B and R channel\n                images.append(image)\n            images = np.concatenate(images, axis=1)\n            out.write(images)\n        out.release()\n        print(f'Saved video to: {video_path}')\n    elif isinstance(video, dict):"
        },
        {
            "comment": "The code loads and saves a video from an HDF5 file. It first removes depth images, concatenates the remaining videos along the width dimension, converts the BGR image to RGB, then writes the video to a specified path at the given frame rate. The function `load_and_save_first_episode_video` calls other functions to load the dataset and save the video.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/truncate_data.py\":84-110",
            "content": "        cam_names = list(video.keys())\n        # Remove depth images\n        cam_names = [cam_name for cam_name in cam_names if '_depth' not in cam_name]\n        all_cam_videos = []\n        for cam_name in cam_names:\n            all_cam_videos.append(video[cam_name])\n        all_cam_videos = np.concatenate(all_cam_videos, axis=2) # width dimension\n        n_frames, h, w, _ = all_cam_videos.shape\n        fps = int(1 / dt)\n        out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n        for t in range(n_frames):\n            image = all_cam_videos[t]\n            image = image[:, :, [2, 1, 0]]  # swap B and R channel\n            out.write(image)\n        out.release()\n        print(f'Saved video to: {video_path}')\ndef load_and_save_first_episode_video(dataset_dir, video_path):\n    dataset_name = 'episode_0'\n    _, _, _, _, image_dict = load_hdf5(dataset_dir, dataset_name)\n    save_videos(image_dict, DT, video_path=video_path)\ndef load_hdf5(dataset_dir, dataset_name):\n    dataset_path = os.path.join(dataset_dir, dataset_name + '.hdf5')"
        },
        {
            "comment": "This code checks if a dataset exists and reads the compressed image data from it. If compression is enabled, it decompresses the images and stores them in an image dictionary for further processing. The function returns only the image dictionary as the output.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/truncate_data.py\":111-134",
            "content": "    if not os.path.isfile(dataset_path):\n        print(f'Dataset does not exist at \\n{dataset_path}\\n')\n        exit()\n    with h5py.File(dataset_path, 'r') as root:\n        compressed = root.attrs.get('compress', False)\n        image_dict = dict()\n        for cam_name in root[f'/observations/images/'].keys():\n            image_dict[cam_name] = root[f'/observations/images/{cam_name}'][()]\n        if compressed:\n            compress_len = root['/compress_len'][()]\n    if compressed:\n        for cam_id, cam_name in enumerate(image_dict.keys()):\n            padded_compressed_image_list = image_dict[cam_name]\n            image_list = []\n            for frame_id, padded_compressed_image in enumerate(padded_compressed_image_list):\n                image_len = int(compress_len[cam_id, frame_id])\n                compressed_image = padded_compressed_image\n                image = cv2.imdecode(compressed_image, 1)\n                image_list.append(image)\n            image_dict[cam_name] = image_list\n    return None, None, None, None, image_dict  # Return only the image dict for this application"
        },
        {
            "comment": "This code compresses all HDF5 datasets in a specified directory and saves the video for the first episode. It requires the 'act-plus-plus' library and utilizes argument parsing, file iteration, and directory creation/manipulation. The output dataset directory is created as a suffix of the input dataset directory with '_truncated'.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/truncate_data.py\":137-156",
            "content": "if __name__ == '__main__':\n    parser = argparse.ArgumentParser(description=\"Compress all HDF5 datasets in a directory.\")\n    parser.add_argument('--dataset_dir', action='store', type=str, required=True, help='Directory containing the uncompressed datasets.')\n    args = parser.parse_args()\n    output_dataset_dir = args.dataset_dir + '_truncated'\n    os.makedirs(output_dataset_dir, exist_ok=True)\n    # Iterate over each file in the directory\n    for filename in tqdm(os.listdir(args.dataset_dir), desc=\"Truncating data\"):\n        if filename.endswith('.hdf5'):\n            input_path = os.path.join(args.dataset_dir, filename)\n            output_path = os.path.join(output_dataset_dir, filename)\n            compress_dataset(input_path, output_path)\n    # After processing all datasets, load and save the video for the first episode\n    print(f'Saving video for episode 0 in {output_dataset_dir}')\n    video_path = os.path.join(output_dataset_dir, 'episode_0_video.mp4')\n    load_and_save_first_episode_video(output_dataset_dir, video_path)"
        }
    ]
}