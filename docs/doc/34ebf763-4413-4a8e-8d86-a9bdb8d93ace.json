{
    "summary": "The code creates a policy network for multi-camera image tasks, trains a noise residual prediction model, and includes an ACTPolicy class for reinforcement learning with normalization and loss calculation. It also defines a CNNMLP model for processing states, images, actions, with KL divergence, MSE loss, and training/inference modes.",
    "details": [
        {
            "comment": "The code imports necessary libraries and classes, defines a class for the DiffusionPolicy model, and includes parameters such as camera names, observation horizon, action horizon, and prediction horizon. The function build_ACT_model_and_optimizer and build_CNNMLP_model_and_optimizer are used to create models and optimizers, while replace_bn_with_gn and ConditionalUnet1D functions are called. EMAModel and scheduling classes DDPMScheduler and DDIMScheduler are also imported for training and scheduling purposes.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/policy.py\":0-27",
            "content": "import torch.nn as nn\nfrom torch.nn import functional as F\nimport torchvision.transforms as transforms\nimport torch\nimport numpy as np\nfrom detr.main import build_ACT_model_and_optimizer, build_CNNMLP_model_and_optimizer\nimport IPython\ne = IPython.embed\nfrom collections import OrderedDict\nfrom robomimic.models.base_nets import ResNet18Conv, SpatialSoftmax\nfrom robomimic.algo.diffusion_policy import replace_bn_with_gn, ConditionalUnet1D\nfrom diffusers.schedulers.scheduling_ddpm import DDPMScheduler\nfrom diffusers.schedulers.scheduling_ddim import DDIMScheduler\nfrom diffusers.training_utils import EMAModel\nclass DiffusionPolicy(nn.Module):\n    def __init__(self, args_override):\n        super().__init__()\n        self.camera_names = args_override['camera_names']\n        self.observation_horizon = args_override['observation_horizon'] ### TODO TODO TODO DO THIS\n        self.action_horizon = args_override['action_horizon'] # apply chunk size\n        self.prediction_horizon = args_override['prediction_horizon'] # chunk size"
        },
        {
            "comment": "Initializing the model's parameters with values from args_override dictionary. Creating lists of ResNet18Conv, SpatialSoftmax, and Linear layers for each camera name. Converting lists to nn.ModuleList to facilitate efficient computation during model execution.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/policy.py\":28-47",
            "content": "        self.num_inference_timesteps = args_override['num_inference_timesteps']\n        self.ema_power = args_override['ema_power']\n        self.lr = args_override['lr']\n        self.weight_decay = 0\n        self.num_kp = 32\n        self.feature_dimension = 64\n        self.ac_dim = args_override['action_dim'] # 14 + 2\n        self.obs_dim = self.feature_dimension * len(self.camera_names) + 14 # camera features and proprio\n        backbones = []\n        pools = []\n        linears = []\n        for _ in self.camera_names:\n            backbones.append(ResNet18Conv(**{'input_channel': 3, 'pretrained': False, 'input_coord_conv': False}))\n            pools.append(SpatialSoftmax(**{'input_shape': [512, 15, 20], 'num_kp': self.num_kp, 'temperature': 1.0, 'learnable_temperature': False, 'noise_std': 0.0}))\n            linears.append(torch.nn.Linear(int(np.prod([self.num_kp, 2])), self.feature_dimension))\n        backbones = nn.ModuleList(backbones)\n        pools = nn.ModuleList(pools)\n        linears = nn.ModuleList(linears)"
        },
        {
            "comment": "This code defines a policy network with backbones, pools, linears, and noise prediction. The model is created as a PyTorch module, converted to float type, and moved to the GPU for faster computation. Optionally, an exponential moving average (EMA) model is also created if ENABLE_EMA flag is set. A noise scheduler is setup to manage the noise during training.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/policy.py\":49-85",
            "content": "        backbones = replace_bn_with_gn(backbones) # TODO\n        noise_pred_net = ConditionalUnet1D(\n            input_dim=self.ac_dim,\n            global_cond_dim=self.obs_dim*self.observation_horizon\n        )\n        nets = nn.ModuleDict({\n            'policy': nn.ModuleDict({\n                'backbones': backbones,\n                'pools': pools,\n                'linears': linears,\n                'noise_pred_net': noise_pred_net\n            })\n        })\n        nets = nets.float().cuda()\n        ENABLE_EMA = True\n        if ENABLE_EMA:\n            ema = EMAModel(model=nets, power=self.ema_power)\n        else:\n            ema = None\n        self.nets = nets\n        self.ema = ema\n        # setup noise scheduler\n        self.noise_scheduler = DDIMScheduler(\n            num_train_timesteps=50,\n            beta_schedule='squaredcos_cap_v2',\n            clip_sample=True,\n            set_alpha_to_one=True,\n            steps_offset=0,\n            prediction_type='epsilon'\n        )\n        n_parameters = sum(p.numel() for p in self.parameters())"
        },
        {
            "comment": "This code initializes an optimizer for the policy network in a multi-camera image task. It prints the number of parameters in the model and defines the __call__ method, which takes in input poses, images, actions (if training), and is_pad flags. During training, it extracts features from each camera's input, concatenates them with qpos, and adds noise to actions for better exploration.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/policy.py\":86-110",
            "content": "        print(\"number of parameters: %.2fM\" % (n_parameters/1e6,))\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.nets.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        return optimizer\n    def __call__(self, qpos, image, actions=None, is_pad=None):\n        B = qpos.shape[0]\n        if actions is not None: # training time\n            nets = self.nets\n            all_features = []\n            for cam_id in range(len(self.camera_names)):\n                cam_image = image[:, cam_id]\n                cam_features = nets['policy']['backbones'][cam_id](cam_image)\n                pool_features = nets['policy']['pools'][cam_id](cam_features)\n                pool_features = torch.flatten(pool_features, start_dim=1)\n                out_features = nets['policy']['linears'][cam_id](pool_features)\n                all_features.append(out_features)\n            obs_cond = torch.cat(all_features + [qpos], dim=1)\n            # sample noise to add to actions\n            noise = torch.randn(actions.shape, device=obs_cond.device)"
        },
        {
            "comment": "This code snippet samples diffusion iterations for each data point, adds noise to clean actions based on the noise magnitude at each iteration, predicts the noise residual using a neural network, calculates the L2 loss between predicted and actual noise, and returns the loss for training purposes. It also optionally updates an exponential moving average (EMA) of the model's parameters if in training mode and EMA is not None.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/policy.py\":112-136",
            "content": "            # sample a diffusion iteration for each data point\n            timesteps = torch.randint(\n                0, self.noise_scheduler.config.num_train_timesteps, \n                (B,), device=obs_cond.device\n            ).long()\n            # add noise to the clean actions according to the noise magnitude at each diffusion iteration\n            # (this is the forward diffusion process)\n            noisy_actions = self.noise_scheduler.add_noise(\n                actions, noise, timesteps)\n            # predict the noise residual\n            noise_pred = nets['policy']['noise_pred_net'](noisy_actions, timesteps, global_cond=obs_cond)\n            # L2 loss\n            all_l2 = F.mse_loss(noise_pred, noise, reduction='none')\n            loss = (all_l2 * ~is_pad.unsqueeze(-1)).mean()\n            loss_dict = {}\n            loss_dict['l2_loss'] = loss\n            loss_dict['loss'] = loss\n            if self.training and self.ema is not None:\n                self.ema.step(nets)\n            return loss_dict"
        },
        {
            "comment": "This code is initializing action from Gaussian noise at inference time. It first determines the observation, action, and prediction horizons based on the policy settings. Then it retrieves the camera-specific networks and, if the exponential moving average (EMA) is not None, uses the averaged model instead of the current one. For each camera, it extracts features by passing images through the corresponding backbones, pools, and linears. Finally, it concatenates all extracted features with qpos, initializes noisy action from Gaussian noise, and sets naction to this noisy action.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/policy.py\":137-161",
            "content": "        else: # inference time\n            To = self.observation_horizon\n            Ta = self.action_horizon\n            Tp = self.prediction_horizon\n            action_dim = self.ac_dim\n            nets = self.nets\n            if self.ema is not None:\n                nets = self.ema.averaged_model\n            all_features = []\n            for cam_id in range(len(self.camera_names)):\n                cam_image = image[:, cam_id]\n                cam_features = nets['policy']['backbones'][cam_id](cam_image)\n                pool_features = nets['policy']['pools'][cam_id](cam_features)\n                pool_features = torch.flatten(pool_features, start_dim=1)\n                out_features = nets['policy']['linears'][cam_id](pool_features)\n                all_features.append(out_features)\n            obs_cond = torch.cat(all_features + [qpos], dim=1)\n            # initialize action from Guassian noise\n            noisy_action = torch.randn(\n                (B, Tp, action_dim), device=obs_cond.device)\n            naction = noisy_action"
        },
        {
            "comment": "The code initializes the noise scheduler and iterates through timesteps, predicting noise and performing inverse diffusion steps to remove noise from samples. It also includes functions for serializing and deserializing the model's parameters.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/policy.py\":163-192",
            "content": "            # init scheduler\n            self.noise_scheduler.set_timesteps(self.num_inference_timesteps)\n            for k in self.noise_scheduler.timesteps:\n                # predict noise\n                noise_pred = nets['policy']['noise_pred_net'](\n                    sample=naction, \n                    timestep=k,\n                    global_cond=obs_cond\n                )\n                # inverse diffusion step (remove noise)\n                naction = self.noise_scheduler.step(\n                    model_output=noise_pred,\n                    timestep=k,\n                    sample=naction\n                ).prev_sample\n            return naction\n    def serialize(self):\n        return {\n            \"nets\": self.nets.state_dict(),\n            \"ema\": self.ema.averaged_model.state_dict() if self.ema is not None else None,\n        }\n    def deserialize(self, model_dict):\n        status = self.nets.load_state_dict(model_dict[\"nets\"])\n        print('Loaded model')\n        if model_dict.get(\"ema\", None) is not None:"
        },
        {
            "comment": "The code defines an `ACTPolicy` class that uses the ACT model and optimizer for reinforcement learning tasks. It normalizes images, handles both training and testing scenarios, and calculates loss during training time. The kl_weight and vq arguments are taken from args_override.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/policy.py\":193-217",
            "content": "            print('Loaded EMA')\n            status_ema = self.ema.averaged_model.load_state_dict(model_dict[\"ema\"])\n            status = [status, status_ema]\n        return status\nclass ACTPolicy(nn.Module):\n    def __init__(self, args_override):\n        super().__init__()\n        model, optimizer = build_ACT_model_and_optimizer(args_override)\n        self.model = model # CVAE decoder\n        self.optimizer = optimizer\n        self.kl_weight = args_override['kl_weight']\n        self.vq = args_override['vq']\n        print(f'KL Weight {self.kl_weight}')\n    def __call__(self, qpos, image, actions=None, is_pad=None, vq_sample=None):\n        env_state = None\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n        image = normalize(image)\n        if actions is not None: # training time\n            actions = actions[:, :self.model.num_queries]\n            is_pad = is_pad[:, :self.model.num_queries]\n            loss_dict = dict()"
        },
        {
            "comment": "The code is defining a policy function for an agent in a reinforcement learning environment. It calculates loss based on differences between predicted and actual actions, as well as KL divergence to penalize the model's confidence in its predictions. The code also defines an optimizer for training and a function to encode actions into binary representations for VQ-VAE (Variable Quantization Variational Autoencoder) models.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/policy.py\":218-239",
            "content": "            a_hat, is_pad_hat, (mu, logvar), probs, binaries = self.model(qpos, image, env_state, actions, is_pad, vq_sample)\n            if self.vq or self.model.encoder is None:\n                total_kld = [torch.tensor(0.0)]\n            else:\n                total_kld, dim_wise_kld, mean_kld = kl_divergence(mu, logvar)\n            if self.vq:\n                loss_dict['vq_discrepancy'] = F.l1_loss(probs, binaries, reduction='mean')\n            all_l1 = F.l1_loss(actions, a_hat, reduction='none')\n            l1 = (all_l1 * ~is_pad.unsqueeze(-1)).mean()\n            loss_dict['l1'] = l1\n            loss_dict['kl'] = total_kld[0]\n            loss_dict['loss'] = loss_dict['l1'] + loss_dict['kl'] * self.kl_weight\n            return loss_dict\n        else: # inference time\n            a_hat, _, (_, _), _, _ = self.model(qpos, image, env_state, vq_sample=vq_sample) # no action, sample from prior\n            return a_hat\n    def configure_optimizers(self):\n        return self.optimizer\n    @torch.no_grad()\n    def vq_encode(self, qpos, actions, is_pad):"
        },
        {
            "comment": "This code defines a class for the CNNMLP policy model in an environment. The __init__ function initializes the model and optimizer based on arguments override, while the __call__ function takes in state (qpos), image, actions (if training time), and is_pad for processing. It normalizes the image, and if actions are provided, it calculates the MSE loss between predicted (a_hat) and actual (actions) actions.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/policy.py\":240-269",
            "content": "        actions = actions[:, :self.model.num_queries]\n        is_pad = is_pad[:, :self.model.num_queries]\n        _, _, binaries, _, _ = self.model.encode(qpos, actions, is_pad)\n        return binaries\n    def serialize(self):\n        return self.state_dict()\n    def deserialize(self, model_dict):\n        return self.load_state_dict(model_dict)\nclass CNNMLPPolicy(nn.Module):\n    def __init__(self, args_override):\n        super().__init__()\n        model, optimizer = build_CNNMLP_model_and_optimizer(args_override)\n        self.model = model # decoder\n        self.optimizer = optimizer\n    def __call__(self, qpos, image, actions=None, is_pad=None):\n        env_state = None # TODO\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n        image = normalize(image)\n        if actions is not None: # training time\n            actions = actions[:, 0]\n            a_hat = self.model(qpos, image, env_state, actions)\n            mse = F.mse_loss(actions, a_hat)"
        },
        {
            "comment": "This code is a part of a neural network policy model. It calculates the KL divergence between two variables, and depending on whether it's training or inference time, it either returns the action estimate (a_hat) or the losses for different loss types like mse. The optimizer configuration function returns the optimizer used by the model.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/policy.py\":270-294",
            "content": "            loss_dict = dict()\n            loss_dict['mse'] = mse\n            loss_dict['loss'] = loss_dict['mse']\n            return loss_dict\n        else: # inference time\n            a_hat = self.model(qpos, image, env_state) # no action, sample from prior\n            return a_hat\n    def configure_optimizers(self):\n        return self.optimizer\ndef kl_divergence(mu, logvar):\n    batch_size = mu.size(0)\n    assert batch_size != 0\n    if mu.data.ndimension() == 4:\n        mu = mu.view(mu.size(0), mu.size(1))\n    if logvar.data.ndimension() == 4:\n        logvar = logvar.view(logvar.size(0), logvar.size(1))\n    klds = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n    total_kld = klds.sum(1).mean(0, True)\n    dimension_wise_kld = klds.mean(0)\n    mean_kld = klds.mean(1).mean(0, True)\n    return total_kld, dimension_wise_kld, mean_kld"
        }
    ]
}