{
    "summary": "The code uses the ACT-Plus-Plus framework for robot manipulation, incorporating deep reinforcement learning and latent models with visual inputs. It saves and plots training curves while supporting customization through command-line arguments, adding new \"--vq_class\" and \"--vq_dim\" options for the latent model's class and dimensionality.",
    "details": [
        {
            "comment": "The code imports necessary libraries, defines functions for robot manipulation and data processing. It initializes parameters from command line inputs and sets a seed for reproducibility. This script aims to train a latent model in the ACT-Plus-Plus framework.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":0-35",
            "content": "import torch\nimport numpy as np\nimport os\nimport pickle\nimport argparse\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\nfrom tqdm import tqdm\nfrom einops import rearrange\nimport torch.nn.functional as F\nfrom constants import DT\nfrom constants import PUPPET_GRIPPER_JOINT_OPEN\nfrom utils import load_data # data functions\nfrom utils import sample_box_pose, sample_insertion_pose # robot functions\nfrom utils import compute_dict_mean, set_seed, detach_dict # helper functions\nfrom policy import ACTPolicy, CNNMLPPolicy\nfrom visualize_episodes import save_videos\nfrom detr.models.latent_model import Latent_Model_Transformer\nfrom sim_env import BOX_POSE\nimport IPython\ne = IPython.embed\ndef main(args):\n    set_seed(1)\n    # command line parameters\n    is_eval = args['eval']\n    ckpt_dir = args['ckpt_dir']\n    policy_class = args['policy_class']\n    onscreen_render = args['onscreen_render']\n    task_name = args['task_name']\n    batch_size_train = args['batch_size']\n    batch_size_val = args['batch_size']\n    num_epochs = args['num_epochs']"
        },
        {
            "comment": "This code retrieves task parameters from the task name and configuration files, sets fixed parameters for the model, and assigns values to variables like dataset_dir, num_episodes, episode_len, camera_names. The code also applies a lambda function as a name filter, if specified in the configuration file.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":37-64",
            "content": "    # get task parameters\n    is_sim = task_name[:4] == 'sim_'\n    if is_sim:\n        from constants import SIM_TASK_CONFIGS\n        task_config = SIM_TASK_CONFIGS[task_name]\n    else:\n        from aloha_scripts.constants import TASK_CONFIGS\n        task_config = TASK_CONFIGS[task_name]\n    dataset_dir = task_config['dataset_dir']\n    num_episodes = task_config['num_episodes']\n    episode_len = task_config['episode_len']\n    camera_names = task_config['camera_names']\n    name_filter = task_config.get('name_filter', lambda n: True)\n    # fixed parameters\n    state_dim = 14\n    lr_backbone = 1e-5\n    backbone = 'resnet18'\n    if policy_class == 'ACT':\n        enc_layers = 4\n        dec_layers = 7\n        nheads = 8\n        policy_config = {'lr': args['lr'],\n                         'num_queries': args['chunk_size'],\n                         'kl_weight': args['kl_weight'],\n                         'hidden_dim': args['hidden_dim'],\n                         'dim_feedforward': args['dim_feedforward'],\n                         'lr_backbone': lr_backbone,"
        },
        {
            "comment": "This code is defining the configuration for training a latent model. It has different policy classes, such as 'Transformer', 'CNNMLP', and others not yet implemented. The configuration includes parameters like learning rate, backbone architecture, camera names, episode length, etc. If an unsupported policy class is given, it raises a NotImplementedError.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":65-91",
            "content": "                         'backbone': backbone,\n                         'enc_layers': enc_layers,\n                         'dec_layers': dec_layers,\n                         'nheads': nheads,\n                         'camera_names': camera_names,\n                         'vq': True,\n                         'vq_class': args['vq_class'],\n                         'vq_dim': args['vq_dim'],\n                         }\n    elif policy_class == 'CNNMLP':\n        policy_config = {'lr': args['lr'], 'lr_backbone': lr_backbone, 'backbone' : backbone, 'num_queries': 1,\n                         'camera_names': camera_names,}\n    else:\n        raise NotImplementedError\n    config = {\n        'num_epochs': num_epochs,\n        'ckpt_dir': ckpt_dir,\n        'episode_len': episode_len,\n        'state_dim': state_dim,\n        'lr': args['lr'],\n        'policy_class': policy_class,\n        'onscreen_render': onscreen_render,\n        'policy_config': policy_config,\n        'task_name': task_name,\n        'seed': args['seed'],\n        'temporal_agg': args['temporal_agg'],"
        },
        {
            "comment": "This code snippet is loading data and training a behavioral cloning (BC) model. If `is_eval` is true, it evaluates the best checkpoint. It loads the data, saves the dataset stats if necessary, trains the BC model, and stores information about the best checkpoint.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":92-119",
            "content": "        'camera_names': camera_names,\n        'real_robot': not is_sim\n    }\n    # if is_eval:\n    #     ckpt_names = [f'policy_best.ckpt']\n    #     results = []\n    #     for ckpt_name in ckpt_names:\n    #         success_rate, avg_return = eval_bc(config, ckpt_name, save_episode=True)\n    #         results.append([ckpt_name, success_rate, avg_return])\n    #     for ckpt_name, success_rate, avg_return in results:\n    #         print(f'{ckpt_name}: {success_rate=} {avg_return=}')\n    #     print()\n    #     exit()\n    train_dataloader, val_dataloader, stats, _ = load_data(dataset_dir, name_filter, camera_names, batch_size_train, batch_size_val)\n    # save dataset stats\n    # if not os.path.isdir(ckpt_dir):\n    #     os.makedirs(ckpt_dir)\n    # stats_path = os.path.join(ckpt_dir, f'dataset_stats.pkl')\n    # with open(stats_path, 'wb') as f:\n    #     pickle.dump(stats, f)\n    ckpt_name = f'policy_last.ckpt'\n    best_ckpt_info = train_bc(train_dataloader, val_dataloader, config, ckpt_name)\n    best_epoch, min_val_loss, best_state_dict = best_ckpt_info"
        },
        {
            "comment": "Code snippet saves the best checkpoint for a latent model, defines a policy function based on the given class, and gets an image from the observations.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":121-153",
            "content": "    # save best checkpoint\n    ckpt_path = os.path.join(ckpt_dir, f'latent_model_best.ckpt')\n    torch.save(best_state_dict, ckpt_path)\n    print(f'Best ckpt, val loss {min_val_loss:.6f} @ epoch{best_epoch}')\ndef make_policy(policy_class, policy_config):\n    if policy_class == 'ACT':\n        policy = ACTPolicy(policy_config)\n    elif policy_class == 'CNNMLP':\n        policy = CNNMLPPolicy(policy_config)\n    else:\n        raise NotImplementedError\n    return policy\n# def make_optimizer(policy_class, policy):\n#     if policy_class == 'ACT':\n#         optimizer = policy.configure_optimizers()\n#     elif policy_class == 'CNNMLP':\n#         optimizer = policy.configure_optimizers()\n#     else:\n#         raise NotImplementedError\n#     return optimizer\ndef get_image(ts, camera_names):\n    curr_images = []\n    for cam_name in camera_names:\n        curr_image = rearrange(ts.observation['images'][cam_name], 'h w c -> c h w')\n        curr_images.append(curr_image)\n    curr_image = np.stack(curr_images, axis=0)\n    curr_image = torch.from_numpy(curr_image / 255.0).float().cuda().unsqueeze(0)"
        },
        {
            "comment": "This code defines a function to evaluate the performance of a trained policy. It loads the policy from a checkpoint file, prepares the necessary configurations, and then evaluates the policy by running episodes. The function takes in the configuration, checkpoint name, and an optional parameter for saving episode results. It uses torch and pickle libraries for loading and processing data.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":154-183",
            "content": "    return curr_image\n# def eval_bc(config, ckpt_name, save_episode=True):\n#     set_seed(1000)\n#     ckpt_dir = config['ckpt_dir']\n#     state_dim = config['state_dim']\n#     real_robot = config['real_robot']\n#     policy_class = config['policy_class']\n#     onscreen_render = config['onscreen_render']\n#     policy_config = config['policy_config']\n#     camera_names = config['camera_names']\n#     max_timesteps = config['episode_len']\n#     task_name = config['task_name']\n#     temporal_agg = config['temporal_agg']\n#     onscreen_cam = 'angle'\n#     # load policy and stats\n#     ckpt_path = os.path.join(ckpt_dir, ckpt_name)\n#     policy = make_policy(policy_class, policy_config)\n#     loading_status = policy.load_state_dict(torch.load(ckpt_path))\n#     print(loading_status)\n#     policy.cuda()\n#     policy.eval()\n#     print(f'Loaded: {ckpt_path}')\n#     stats_path = os.path.join(ckpt_dir, f'dataset_stats.pkl')\n#     with open(stats_path, 'rb') as f:\n#         stats = pickle.load(f)\n#     pre_process = lambda s_qpos: (s_qpos - stats['qpos_mean']) / stats['qpos_std']"
        },
        {
            "comment": "This code is initializing an environment, either real or simulated, based on the \"real_robot\" flag. It then sets up variables for rollout number of episodes, maximum timesteps, query frequency (which may change depending on temporal aggregation), and stores episode returns and highest rewards in lists. The last few lines seem to set up task-specific poses for certain tasks.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":184-212",
            "content": "#     post_process = lambda a: a * stats['action_std'] + stats['action_mean']\n#     # load environment\n#     if real_robot:\n#         from aloha_scripts.robot_utils import move_grippers # requires aloha\n#         from aloha_scripts.real_env import make_real_env # requires aloha\n#         env = make_real_env(init_node=True)\n#         env_max_reward = 0\n#     else:\n#         from sim_env import make_sim_env\n#         env = make_sim_env(task_name)\n#         env_max_reward = env.task.max_reward\n#     query_frequency = policy_config['num_queries']\n#     if temporal_agg:\n#         query_frequency = 1\n#         num_queries = policy_config['num_queries']\n#     max_timesteps = int(max_timesteps * 1) # may increase for real-world tasks\n#     num_rollouts = 50\n#     episode_returns = []\n#     highest_rewards = []\n#     for rollout_id in range(num_rollouts):\n#         rollout_id += 0\n#         ### set task\n#         if 'sim_transfer_cube' in task_name:\n#             BOX_POSE[0] = sample_box_pose() # used in sim reset\n#         elif 'sim_insertion' in task_name:"
        },
        {
            "comment": "This code snippet is part of a training process for a latent model. It resets the environment, performs on-screen rendering if needed, and then enters an evaluation loop to collect data for training. The code uses PyTorch for inference mode and handles on-screen rendering, image capturing, and storing data for further analysis or model training.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":213-237",
            "content": "#             BOX_POSE[0] = np.concatenate(sample_insertion_pose()) # used in sim reset\n#         ts = env.reset()\n#         ### onscreen render\n#         if onscreen_render:\n#             ax = plt.subplot()\n#             plt_img = ax.imshow(env._physics.render(height=480, width=640, camera_id=onscreen_cam))\n#             plt.ion()\n#         ### evaluation loop\n#         if temporal_agg:\n#             all_time_actions = torch.zeros([max_timesteps, max_timesteps+num_queries, state_dim]).cuda()\n#         qpos_history = torch.zeros((1, max_timesteps, state_dim)).cuda()\n#         image_list = [] # for visualization\n#         qpos_list = []\n#         target_qpos_list = []\n#         rewards = []\n#         with torch.inference_mode():\n#             for t in range(max_timesteps):\n#                 ### update onscreen render and wait for DT\n#                 if onscreen_render:\n#                     image = env._physics.render(height=480, width=640, camera_id=onscreen_cam)\n#                     plt_img.set_data(image)"
        },
        {
            "comment": "This code segment is part of a deep reinforcement learning algorithm that interacts with an environment. It processes observations, pre-processes state variables (qpos), and queries the policy to generate actions. The 'policy_class' determines whether to use an ACT policy or not. If so, it queries the policy for actions at specific intervals (query_frequency) and possibly aggregates them over time if temporal_agg is set to True. This algorithm likely trains a latent model in an environment with potential visual input from cameras.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":238-259",
            "content": "#                     plt.pause(DT)\n#                 ### process previous timestep to get qpos and image_list\n#                 obs = ts.observation\n#                 if 'images' in obs:\n#                     image_list.append(obs['images'])\n#                 else:\n#                     image_list.append({'main': obs['image']})\n#                 qpos_numpy = np.array(obs['qpos'])\n#                 qpos = pre_process(qpos_numpy)\n#                 qpos = torch.from_numpy(qpos).float().cuda().unsqueeze(0)\n#                 qpos_history[:, t] = qpos\n#                 curr_image = get_image(ts, camera_names)\n#                 ### query policy\n#                 if config['policy_class'] == \"ACT\":\n#                     if t % query_frequency == 0:\n#                         all_actions = policy(qpos, curr_image)\n#                     if temporal_agg:\n#                         all_time_actions[[t], t:t+num_queries] = all_actions\n#                         actions_for_curr_step = all_time_actions[:, t]\n#                         actions_populated = torch.all(actions_for_curr_step != 0, axis=1)"
        },
        {
            "comment": "This code determines the raw action for a given step in an environment. It first checks the policy class and then applies the appropriate method to get the raw action. If the policy class is \"Exponential\", it calculates weights based on actions, sums them, and uses them to compute the raw action. If the policy class is \"CNNMLP\", it calls a predefined function \"policy\" with the current state and image as inputs. If none of these conditions are met, it raises an error. The resulting raw_action is then post-processed and used to determine target_qpos for the next step in the environment.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":260-278",
            "content": "#                         actions_for_curr_step = actions_for_curr_step[actions_populated]\n#                         k = 0.01\n#                         exp_weights = np.exp(-k * np.arange(len(actions_for_curr_step)))\n#                         exp_weights = exp_weights / exp_weights.sum()\n#                         exp_weights = torch.from_numpy(exp_weights).cuda().unsqueeze(dim=1)\n#                         raw_action = (actions_for_curr_step * exp_weights).sum(dim=0, keepdim=True)\n#                     else:\n#                         raw_action = all_actions[:, t % query_frequency]\n#                 elif config['policy_class'] == \"CNNMLP\":\n#                     raw_action = policy(qpos, curr_image)\n#                 else:\n#                     raise NotImplementedError\n#                 ### post-process actions\n#                 raw_action = raw_action.squeeze(0).cpu().numpy()\n#                 action = post_process(raw_action)\n#                 target_qpos = action\n#                 ### step the environment"
        },
        {
            "comment": "This code segment is tracking the reward, episode return, and highest reward during a rollout in a robotics environment. It also handles visualization by appending qpos and target_qpos to lists, and has options to save videos of the episodes. It prints the rollout results and calculates the success rate based on the highest rewards achieved.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":279-301",
            "content": "#                 ts = env.step(target_qpos)\n#                 ### for visualization\n#                 qpos_list.append(qpos_numpy)\n#                 target_qpos_list.append(target_qpos)\n#                 rewards.append(ts.reward)\n#             plt.close()\n#         if real_robot:\n#             move_grippers([env.puppet_bot_left, env.puppet_bot_right], [PUPPET_GRIPPER_JOINT_OPEN] * 2, move_time=0.5)  # open\n#             pass\n#         rewards = np.array(rewards)\n#         episode_return = np.sum(rewards[rewards!=None])\n#         episode_returns.append(episode_return)\n#         episode_highest_reward = np.max(rewards)\n#         highest_rewards.append(episode_highest_reward)\n#         print(f'Rollout {rollout_id}\\n{episode_return=}, {episode_highest_reward=}, {env_max_reward=}, Success: {episode_highest_reward==env_max_reward}')\n#         if save_episode:\n#             save_videos(image_list, DT, video_path=os.path.join(ckpt_dir, f'video{rollout_id}.mp4'))\n#     success_rate = np.mean(np.array(highest_rewards) == env_max_reward)"
        },
        {
            "comment": "The code calculates the success rate and average return for a set of rollouts in an environment. It then creates a summary string with reward thresholds, success rate, and average return, and writes it to a text file along with episode returns and highest rewards. The function is part of a larger codebase for training a latent model using policy and latent_model parameters.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":302-325",
            "content": "#     avg_return = np.mean(episode_returns)\n#     summary_str = f'\\nSuccess rate: {success_rate}\\nAverage return: {avg_return}\\n\\n'\n#     for r in range(env_max_reward+1):\n#         more_or_equal_r = (np.array(highest_rewards) >= r).sum()\n#         more_or_equal_r_rate = more_or_equal_r / num_rollouts\n#         summary_str += f'Reward >= {r}: {more_or_equal_r}/{num_rollouts} = {more_or_equal_r_rate*100}%\\n'\n#     print(summary_str)\n#     # save success rate to txt\n#     result_file_name = 'result_' + ckpt_name.split('.')[0] + '.txt'\n#     with open(os.path.join(ckpt_dir, result_file_name), 'w') as f:\n#         f.write(summary_str)\n#         f.write(repr(episode_returns))\n#         f.write('\\n\\n')\n#         f.write(repr(highest_rewards))\n#     return success_rate, avg_return\ndef forward_pass(data, policy, latent_model):\n    image_data, qpos_data, action_data, is_pad = data\n    image_data, qpos_data, action_data, is_pad = image_data.cuda(), qpos_data.cuda(), action_data.cuda(), is_pad.cuda()\n    forward_dict = {}"
        },
        {
            "comment": "This code uses VQ-VAE to encode data, then feeds it into a latent model and calculates cross entropy loss. It also measures L1 error between output labels and ground truth labels for evaluation. The train_bc function trains the policy using a specified number of epochs with a given configuration and checkpoint directory.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":326-352",
            "content": "    gt_labels = policy.vq_encode(qpos_data, action_data, is_pad)\n    inputs = torch.cat([torch.zeros_like(gt_labels)[:, [0]], gt_labels[:, :-1]], dim=1)\n    output_logits = latent_model(inputs)\n    ce_loss = F.cross_entropy(output_logits, gt_labels)\n    with torch.no_grad():\n        output_labels = F.one_hot(torch.argmax(output_logits, dim=-1), num_classes=gt_labels.shape[-1]).float()\n        # output_latents = F.softmax(output_logits, dim=-1)\n        l1_error = F.l1_loss(output_labels, gt_labels, reduction='mean')\n        # l1_errors = []\n        # for i in range(l1_errors.shape[1]):\n        #     l1_errors.append(torch.mean(l1_errors[:, i]).item())\n    forward_dict['loss'] = ce_loss\n    forward_dict['l1_error'] = l1_error\n    return forward_dict\ndef train_bc(train_dataloader, val_dataloader, config, ckpt_name):\n    num_epochs = config['num_epochs']\n    ckpt_dir = config['ckpt_dir']\n    seed = config['seed']\n    policy_class = config['policy_class']\n    policy_config = config['policy_config']\n    set_seed(seed)"
        },
        {
            "comment": "This code initializes a latent model and policy, loads checkpoints for the policy, optimizes the latent model using AdamW, trains for specified number of epochs, and validates the performance at each epoch.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":354-381",
            "content": "    vq_dim = config['policy_config']['vq_dim']\n    vq_class = config['policy_config']['vq_class']\n    latent_model = Latent_Model_Transformer(vq_dim, vq_dim, vq_class)\n    latent_model.cuda()\n    ckpt_path = os.path.join(ckpt_dir, ckpt_name)\n    policy = make_policy(policy_class, policy_config)\n    loading_status = policy.load_state_dict(torch.load(ckpt_path))\n    policy.eval()\n    policy.cuda()\n    optimizer = torch.optim.AdamW(latent_model.parameters(), lr=config['lr'])\n    train_history = []\n    validation_history = []\n    min_val_loss = np.inf\n    best_ckpt_info = None\n    for epoch in tqdm(range(num_epochs)):\n        print(f'\\nEpoch {epoch}')\n        # validation\n        with torch.inference_mode():\n            latent_model.eval()\n            epoch_dicts = []\n            for batch_idx, data in enumerate(val_dataloader):\n                forward_dict = forward_pass(data, policy, latent_model)\n                epoch_dicts.append(forward_dict)\n            epoch_summary = compute_dict_mean(epoch_dicts)\n            validation_history.append(epoch_summary)"
        },
        {
            "comment": "This code is saving the best checkpoint, printing validation and training losses, iterating through dataloader for backpropagation, computing mean of dictionary values to get epoch summary, and storing it in a list.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":383-405",
            "content": "            epoch_val_loss = epoch_summary['loss']\n            if epoch_val_loss < min_val_loss:\n                min_val_loss = epoch_val_loss\n                best_ckpt_info = (epoch, min_val_loss, deepcopy(latent_model.state_dict()))\n        print(f'Val loss:   {epoch_val_loss:.5f}')\n        summary_string = ''\n        for k, v in epoch_summary.items():\n            summary_string += f'{k}: {v.item():.3f} '\n        print(summary_string)\n        # training\n        optimizer.zero_grad()\n        for batch_idx, data in enumerate(train_dataloader):\n            forward_dict = forward_pass(data, policy, latent_model)\n            # backward\n            loss = forward_dict['loss']\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            train_history.append(detach_dict(forward_dict))\n        epoch_summary = compute_dict_mean(train_history[(batch_idx+1)*epoch:(batch_idx+1)*(epoch+1)])\n        epoch_train_loss = epoch_summary['loss']\n        print(f'Train loss: {epoch_train_loss:.5f}')"
        },
        {
            "comment": "The code snippet saves the latent model's state at each epoch, keeps track of the best checkpoint, and plots the training curves. It prints the final validation loss and epoch where it occurred.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":406-430",
            "content": "        summary_string = ''\n        for k, v in epoch_summary.items():\n            summary_string += f'{k}: {v.item():.3f} '\n        print(summary_string)\n        if epoch % 100 == 0:\n            ckpt_path = os.path.join(ckpt_dir, f'latent_model_epoch_{epoch}_seed_{seed}.ckpt')\n            torch.save(latent_model.state_dict(), ckpt_path)\n            plot_history(train_history, validation_history, epoch, ckpt_dir, seed)\n    ckpt_path = os.path.join(ckpt_dir, f'latent_model_last.ckpt')\n    torch.save(latent_model.state_dict(), ckpt_path)\n    best_epoch, min_val_loss, best_state_dict = best_ckpt_info\n    ckpt_path = os.path.join(ckpt_dir, f'latent_model_epoch_{best_epoch}_seed_{seed}.ckpt')\n    torch.save(best_state_dict, ckpt_path)\n    print(f'Training finished:\\nSeed {seed}, val loss {min_val_loss:.6f} at epoch {best_epoch}')\n    # save training curves\n    plot_history(train_history, validation_history, num_epochs, ckpt_dir, seed)\n    return best_ckpt_info\ndef plot_history(train_history, validation_history, num_epochs, ckpt_dir, seed):"
        },
        {
            "comment": "This code saves training curves for a latent model and plots them. It iterates over keys in train_history, generates plots for each key (train and validation), and saves the plot to ckpt_dir with seed appended. The code also takes command-line arguments such as --eval, --onscreen_render, --ckpt_dir, and --policy_class.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":431-452",
            "content": "    # save training curves\n    for key in train_history[0]:\n        plot_path = os.path.join(ckpt_dir, f'latent_model_val_{key}_seed_{seed}.png')\n        plt.figure()\n        train_values = [summary[key].item() for summary in train_history]\n        val_values = [summary[key].item() for summary in validation_history]\n        plt.plot(np.linspace(0, num_epochs-1, len(train_history)), train_values, label='train')\n        plt.plot(np.linspace(0, num_epochs-1, len(validation_history)), val_values, label='validation')\n        # plt.ylim([-0.1, 1])\n        plt.tight_layout()\n        plt.legend()\n        plt.title(key)\n        plt.savefig(plot_path)\n    print(f'Saved plots to {ckpt_dir}')\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--eval', action='store_true')\n    parser.add_argument('--onscreen_render', action='store_true')\n    parser.add_argument('--ckpt_dir', action='store', type=str, help='ckpt_dir', required=True)\n    parser.add_argument('--policy_class', action='store', type=str, help='policy_class, capitalize', required=True)"
        },
        {
            "comment": "This code defines command-line arguments for the program, specifying required and optional parameters such as task_name, batch_size, seed, num_epochs, lr, kl_weight, chunk_size, hidden_dim, dim_feedforward, and temporal_agg. These options allow the user to customize the training process of the latent model.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":453-465",
            "content": "    parser.add_argument('--task_name', action='store', type=str, help='task_name', required=True)\n    parser.add_argument('--batch_size', action='store', type=int, help='batch_size', required=True)\n    parser.add_argument('--seed', action='store', type=int, help='seed', required=True)\n    parser.add_argument('--num_epochs', action='store', type=int, help='num_epochs', required=True)\n    parser.add_argument('--lr', action='store', type=float, help='lr', required=True)\n    # for ACT\n    parser.add_argument('--kl_weight', action='store', type=int, help='KL Weight', required=False)\n    parser.add_argument('--chunk_size', action='store', type=int, help='chunk_size', required=False)\n    parser.add_argument('--hidden_dim', action='store', type=int, help='hidden_dim', required=False)\n    parser.add_argument('--dim_feedforward', action='store', type=int, help='dim_feedforward', required=False)\n    parser.add_argument('--temporal_agg', action='store_true')\n    parser.add_argument('--use_vq', action='store_true')"
        },
        {
            "comment": "This code is adding two arguments, \"--vq_class\" and \"--vq_dim\", to the parser using store action and specifying their types as integer (int). These arguments provide parameters for a latent model's class and dimensionality. The main function is then called with these parameters obtained from parsing command line arguments.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/train_latent_model.py\":466-469",
            "content": "    parser.add_argument('--vq_class', action='store', type=int, help='vq_class')\n    parser.add_argument('--vq_dim', action='store', type=int, help='vq_dim')\n    main(vars(parser.parse_args()))"
        }
    ]
}