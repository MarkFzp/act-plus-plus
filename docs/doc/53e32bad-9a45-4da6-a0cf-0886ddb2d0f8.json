{
    "summary": "This code defines a DETRVAE model for image object detection, using deep learning architecture and presents a CVAE-DETR model that generates latent inputs. The transformer-based model predicts actions and latent variables using PyTorch.",
    "details": [
        {
            "comment": "This code defines the DETRVAE model and its associated functions. It uses modules like `torch`, `nn`, and `TransformerEncoder` to build a deep learning architecture for detecting objects in images. The `reparametrize` function is used for reparameterization trick, while `get_sinusoid_encoding_table` generates sinusoid encodings for positional encoding. The class `DETRVAE` is the main model implementation.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":0-34",
            "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\"\"\"\nDETR model and criterion classes.\n\"\"\"\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom .backbone import build_backbone\nfrom .transformer import build_transformer, TransformerEncoder, TransformerEncoderLayer\nimport numpy as np\nimport IPython\ne = IPython.embed\ndef reparametrize(mu, logvar):\n    std = logvar.div(2).exp()\n    eps = Variable(std.data.new(std.size()).normal_())\n    return mu + std * eps\ndef get_sinusoid_encoding_table(n_position, d_hid):\n    def get_position_angle_vec(position):\n        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\nclass DETRVAE(nn.Module):"
        },
        {
            "comment": "The code defines a class called `DETR` for object detection. It takes in backbone, transformer, encoder, state_dim, num_queries, camera_names, vq, vq_class, and vq_dim as parameters to initialize the model. The `num_queries` represents the maximal number of objects that DETR can detect in a single image, and auxiliary decoding losses are optional.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":35-51",
            "content": "    \"\"\" This is the DETR module that performs object detection \"\"\"\n    def __init__(self, backbones, transformer, encoder, state_dim, num_queries, camera_names, vq, vq_class, vq_dim, action_dim):\n        \"\"\" Initializes the model.\n        Parameters:\n            backbones: torch module of the backbone to be used. See backbone.py\n            transformer: torch module of the transformer architecture. See transformer.py\n            state_dim: robot state dimension of the environment\n            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n                         DETR can detect in a single image. For COCO, we recommend 100 queries.\n            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n        \"\"\"\n        super().__init__()\n        self.num_queries = num_queries\n        self.camera_names = camera_names\n        self.transformer = transformer\n        self.encoder = encoder\n        self.vq, self.vq_class, self.vq_dim = vq, vq_class, vq_dim"
        },
        {
            "comment": "The code initializes the DETR-VAE model by setting state and action dimensions, defining linear layers for action and pad heads, an embedding layer for queries, and additional layers based on whether backbones are provided or not. If no backbones are provided, it adds separate layers for robot state and environment state projections, a position embedding, and sets the backbones to None. It also sets the latent dimension of the latent z variable to 32 (to be tuned) and adds an extra cls token embedding.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":52-70",
            "content": "        self.state_dim, self.action_dim = state_dim, action_dim\n        hidden_dim = transformer.d_model\n        self.action_head = nn.Linear(hidden_dim, action_dim)\n        self.is_pad_head = nn.Linear(hidden_dim, 1)\n        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n        if backbones is not None:\n            self.input_proj = nn.Conv2d(backbones[0].num_channels, hidden_dim, kernel_size=1)\n            self.backbones = nn.ModuleList(backbones)\n            self.input_proj_robot_state = nn.Linear(state_dim, hidden_dim)\n        else:\n            # input_dim = 14 + 7 # robot_state + env_state\n            self.input_proj_robot_state = nn.Linear(state_dim, hidden_dim)\n            self.input_proj_env_state = nn.Linear(7, hidden_dim)\n            self.pos = torch.nn.Embedding(2, hidden_dim)\n            self.backbones = None\n        # encoder extra parameters\n        self.latent_dim = 32 # final size of latent z # TODO tune\n        self.cls_embed = nn.Embedding(1, hidden_dim) # extra cls token embedding"
        },
        {
            "comment": "The code initializes the layers for a variational autoencoder (VAE) in DETR model. It includes linear layers to project actions and qpos to embedding, VQ-VAE specific latent projection, and decoder parameters such as latent out projection and learned position embeddings for proprio and latent. The encode function takes qpos, actions, is_pad, and vq_sample as inputs.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":71-89",
            "content": "        self.encoder_action_proj = nn.Linear(action_dim, hidden_dim) # project action to embedding\n        self.encoder_joint_proj = nn.Linear(state_dim, hidden_dim)  # project qpos to embedding\n        print(f'Use VQ: {self.vq}, {self.vq_class}, {self.vq_dim}')\n        if self.vq:\n            self.latent_proj = nn.Linear(hidden_dim, self.vq_class * self.vq_dim)\n        else:\n            self.latent_proj = nn.Linear(hidden_dim, self.latent_dim*2) # project hidden state to latent std, var\n        self.register_buffer('pos_table', get_sinusoid_encoding_table(1+1+num_queries, hidden_dim)) # [CLS], qpos, a_seq\n        # decoder extra parameters\n        if self.vq:\n            self.latent_out_proj = nn.Linear(self.vq_class * self.vq_dim, hidden_dim)\n        else:\n            self.latent_out_proj = nn.Linear(self.latent_dim, hidden_dim) # project latent sample to embedding\n        self.additional_pos_embed = nn.Embedding(2, hidden_dim) # learned position embedding for proprio and latent\n    def encode(self, qpos, actions=None, is_pad=None, vq_sample=None):"
        },
        {
            "comment": "This code is part of a CVAE (Conditional Variational Autoencoder) model. It obtains the latent variable z from an action sequence and a query position during training. The encoder projects the action sequence to an embedding dimension and concatenates it with a query position embedding and a fixed CLs token embedding. These inputs are then passed to the encoder to get the latent representation.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":90-106",
            "content": "        bs, _ = qpos.shape\n        if self.encoder is None:\n            latent_sample = torch.zeros([bs, self.latent_dim], dtype=torch.float32).to(qpos.device)\n            latent_input = self.latent_out_proj(latent_sample)\n            probs = binaries = mu = logvar = None\n        else:\n            # cvae encoder\n            is_training = actions is not None # train or val\n            ### Obtain latent z from action sequence\n            if is_training:\n                # project action sequence to embedding dim, and concat with a CLS token\n                action_embed = self.encoder_action_proj(actions) # (bs, seq, hidden_dim)\n                qpos_embed = self.encoder_joint_proj(qpos)  # (bs, hidden_dim)\n                qpos_embed = torch.unsqueeze(qpos_embed, axis=1)  # (bs, 1, hidden_dim)\n                cls_embed = self.cls_embed.weight # (1, hidden_dim)\n                cls_embed = torch.unsqueeze(cls_embed, axis=0).repeat(bs, 1, 1) # (bs, 1, hidden_dim)\n                encoder_input = torch.cat([cls_embed, qpos_embed, action_embed], axis=1) # (bs, seq+1, hidden_dim)"
        },
        {
            "comment": "This code snippet is part of a DETR model, specifically the VAE (Variational Autoencoder) implementation. Here, it prepares the input for the encoder and then passes it through the encoder to obtain an encoded representation (latent_info). This encoding is used for the VQ-VAE loss (if enabled), where a one-hot binary encoding of the latents is used to learn a codebook.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":107-122",
            "content": "                encoder_input = encoder_input.permute(1, 0, 2) # (seq+1, bs, hidden_dim)\n                # do not mask cls token\n                cls_joint_is_pad = torch.full((bs, 2), False).to(qpos.device) # False: not a padding\n                is_pad = torch.cat([cls_joint_is_pad, is_pad], axis=1)  # (bs, seq+1)\n                # obtain position embedding\n                pos_embed = self.pos_table.clone().detach()\n                pos_embed = pos_embed.permute(1, 0, 2)  # (seq+1, 1, hidden_dim)\n                # query model\n                encoder_output = self.encoder(encoder_input, pos=pos_embed, src_key_padding_mask=is_pad)\n                encoder_output = encoder_output[0] # take cls output only\n                latent_info = self.latent_proj(encoder_output)\n                if self.vq:\n                    logits = latent_info.reshape([*latent_info.shape[:-1], self.vq_class, self.vq_dim])\n                    probs = torch.softmax(logits, dim=-1)\n                    binaries = F.one_hot(torch.mult"
        },
        {
            "comment": "This code is for a Variational Autoencoder (VAE) model, specifically the DETR-VAE. It calculates the latent input based on whether or not the model is in VQ-VAE mode. If it is, it computes binaries and probs, subtracts them, passes through the latent projection layer, and assigns them to mu and logvar as None. If not, it uses either the provided vq_sample (if available) or calculates the latent input using the latent projection layer if VQ mode is disabled.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":122-140",
            "content": "inomial(probs.view(-1, self.vq_dim), 1).squeeze(-1), self.vq_dim).view(-1, self.vq_class, self.vq_dim).float()\n                    binaries_flat = binaries.view(-1, self.vq_class * self.vq_dim)\n                    probs_flat = probs.view(-1, self.vq_class * self.vq_dim)\n                    straigt_through = binaries_flat - probs_flat.detach() + probs_flat\n                    latent_input = self.latent_out_proj(straigt_through)\n                    mu = logvar = None\n                else:\n                    probs = binaries = None\n                    mu = latent_info[:, :self.latent_dim]\n                    logvar = latent_info[:, self.latent_dim:]\n                    latent_sample = reparametrize(mu, logvar)\n                    latent_input = self.latent_out_proj(latent_sample)\n            else:\n                mu = logvar = binaries = probs = None\n                if self.vq:\n                    latent_input = self.latent_out_proj(vq_sample.view(-1, self.vq_class * self.vq_dim))\n                else:\n "
        },
        {
            "comment": "This code snippet defines a method for creating latent samples, initializing variables, and performing encoding using a VAE (Variational AutoEncoder). The forward function takes in inputs like qpos, image, env_state, actions, is_pad, and vq_sample. It encodes the input using the encode method and then applies the CVAE decoder if backbones are provided.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":140-162",
            "content": "                   latent_sample = torch.zeros([bs, self.latent_dim], dtype=torch.float32).to(qpos.device)\n                    latent_input = self.latent_out_proj(latent_sample)\n        return latent_input, probs, binaries, mu, logvar\n    def forward(self, qpos, image, env_state, actions=None, is_pad=None, vq_sample=None):\n        \"\"\"\n        qpos: batch, qpos_dim\n        image: batch, num_cam, channel, height, width\n        env_state: None\n        actions: batch, seq, action_dim\n        \"\"\"\n        latent_input, probs, binaries, mu, logvar = self.encode(qpos, actions, is_pad, vq_sample)\n        # cvae decoder\n        if self.backbones is not None:\n            # Image observation features and position embeddings\n            all_cam_features = []\n            all_cam_pos = []\n            for cam_id, cam_name in enumerate(self.camera_names):\n                features, pos = self.backbones[cam_id](image[:, cam_id])\n                features = features[0] # take the last layer feature\n                pos = pos[0]"
        },
        {
            "comment": "This code defines a model for predicting actions and latent variables. It includes a transformer network, action head, and is pad head. The input includes camera features, proprioception features, robot state, and environment state. The model handles both scenarios with or without cameras. The CNNMLP class initializes the model using backbones, state_dim, and camera names.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":163-183",
            "content": "                all_cam_features.append(self.input_proj(features))\n                all_cam_pos.append(pos)\n            # proprioception features\n            proprio_input = self.input_proj_robot_state(qpos)\n            # fold camera dimension into width dimension\n            src = torch.cat(all_cam_features, axis=3)\n            pos = torch.cat(all_cam_pos, axis=3)\n            hs = self.transformer(src, None, self.query_embed.weight, pos, latent_input, proprio_input, self.additional_pos_embed.weight)[0]\n        else:\n            qpos = self.input_proj_robot_state(qpos)\n            env_state = self.input_proj_env_state(env_state)\n            transformer_input = torch.cat([qpos, env_state], axis=1) # seq length = 2\n            hs = self.transformer(transformer_input, None, self.query_embed.weight, self.pos.weight)[0]\n        a_hat = self.action_head(hs)\n        is_pad_hat = self.is_pad_head(hs)\n        return a_hat, is_pad_hat, [mu, logvar], probs, binaries\nclass CNNMLP(nn.Module):\n    def __init__(self, backbones, state_dim, camera_names):"
        },
        {
            "comment": "This code initializes the model and takes parameters for backbones, transformer, state_dim, num_queries, and aux_loss. It creates an action head using a linear layer with 1000 input size and state_dim output size. If backbones are provided, it creates a ModuleList of backbones and initializes down_proj for each backbone using conv2d with specified parameters.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":184-201",
            "content": "        \"\"\" Initializes the model.\n        Parameters:\n            backbones: torch module of the backbone to be used. See backbone.py\n            transformer: torch module of the transformer architecture. See transformer.py\n            state_dim: robot state dimension of the environment\n            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n                         DETR can detect in a single image. For COCO, we recommend 100 queries.\n            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n        \"\"\"\n        super().__init__()\n        self.camera_names = camera_names\n        self.action_head = nn.Linear(1000, state_dim) # TODO add more\n        if backbones is not None:\n            self.backbones = nn.ModuleList(backbones)\n            backbone_down_projs = []\n            for backbone in backbones:\n                down_proj = nn.Sequential(\n                    nn.Conv2d(backbone.num_channels, 128, kernel_size=5),"
        },
        {
            "comment": "This code is for a DETR model in PyTorch. It defines the architecture and forward pass. The backbone network consists of two convolutions to downsample the input, followed by a mlp layer if needed. The forward method takes in qpos, image, env_state (None in this case), and optionally actions for training or validation. It extracts image features from each camera view using backbones, concatenates them, and performs positional encoding.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":202-226",
            "content": "                    nn.Conv2d(128, 64, kernel_size=5),\n                    nn.Conv2d(64, 32, kernel_size=5)\n                )\n                backbone_down_projs.append(down_proj)\n            self.backbone_down_projs = nn.ModuleList(backbone_down_projs)\n            mlp_in_dim = 768 * len(backbones) + state_dim\n            self.mlp = mlp(input_dim=mlp_in_dim, hidden_dim=1024, output_dim=self.action_dim, hidden_depth=2)\n        else:\n            raise NotImplementedError\n    def forward(self, qpos, image, env_state, actions=None):\n        \"\"\"\n        qpos: batch, qpos_dim\n        image: batch, num_cam, channel, height, width\n        env_state: None\n        actions: batch, seq, action_dim\n        \"\"\"\n        is_training = actions is not None # train or val\n        bs, _ = qpos.shape\n        # Image observation features and position embeddings\n        all_cam_features = []\n        for cam_id, cam_name in enumerate(self.camera_names):\n            features, pos = self.backbones[cam_id](image[:, cam_id])\n            features = features[0] # take the last layer feature"
        },
        {
            "comment": "This code defines a DETR VAE model, including functions for building the encoder and creating an MLP. The encoder takes input features and positions (qpos) to create a flattened feature matrix, which is then passed through an MLP to produce the final output (a_hat).",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":227-253",
            "content": "            pos = pos[0] # not used\n            all_cam_features.append(self.backbone_down_projs[cam_id](features))\n        # flatten everything\n        flattened_features = []\n        for cam_feature in all_cam_features:\n            flattened_features.append(cam_feature.reshape([bs, -1]))\n        flattened_features = torch.cat(flattened_features, axis=1) # 768 each\n        features = torch.cat([flattened_features, qpos], axis=1) # qpos: 14\n        a_hat = self.mlp(features)\n        return a_hat\ndef mlp(input_dim, hidden_dim, output_dim, hidden_depth):\n    if hidden_depth == 0:\n        mods = [nn.Linear(input_dim, output_dim)]\n    else:\n        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n        for i in range(hidden_depth - 1):\n            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n        mods.append(nn.Linear(hidden_dim, output_dim))\n    trunk = nn.Sequential(*mods)\n    return trunk\ndef build_encoder(args):\n    d_model = args.hidden_dim # 256\n    dropout = args.dropout # 0.1"
        },
        {
            "comment": "This code builds a DETRVAE model by defining its components and parameters. It initializes the transformer encoder, decoder, and VAE components based on provided arguments. The backbone for image processing is built using a function call to build_backbone(args). If no encoder is required, it sets the encoder as None.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":254-288",
            "content": "    nhead = args.nheads # 8\n    dim_feedforward = args.dim_feedforward # 2048\n    num_encoder_layers = args.enc_layers # 4 # TODO shared with VAE decoder\n    normalize_before = args.pre_norm # False\n    activation = \"relu\"\n    encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n                                            dropout, activation, normalize_before)\n    encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n    encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n    return encoder\ndef build(args):\n    state_dim = 14 # TODO hardcode\n    # From state\n    # backbone = None # from state for now, no need for conv nets\n    # From image\n    backbones = []\n    for _ in args.camera_names:\n        backbone = build_backbone(args)\n        backbones.append(backbone)\n    transformer = build_transformer(args)\n    if args.no_encoder:\n        encoder = None\n    else:\n        encoder = build_transformer(args)\n    model = DETRVAE(\n        backbones,\n        transformer,"
        },
        {
            "comment": "This code defines two functions, `detr_vae` and `build_cnnmlp`, which build different models. Both functions return a model object after printing the number of parameters it has. The `detr_vae` function requires additional arguments like `state_dim`, `num_queries`, `camera_names`, `vq`, `vq_class`, and `action_dim`. The `build_cnnmlp` function requires an `args` argument, which it uses to create a CNNMLP model by building backbones for each camera name provided in the arguments.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/detr/models/detr_vae.py\":289-324",
            "content": "        encoder,\n        state_dim=state_dim,\n        num_queries=args.num_queries,\n        camera_names=args.camera_names,\n        vq=args.vq,\n        vq_class=args.vq_class,\n        vq_dim=args.vq_dim,\n        action_dim=args.action_dim,\n    )\n    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(\"number of parameters: %.2fM\" % (n_parameters/1e6,))\n    return model\ndef build_cnnmlp(args):\n    state_dim = 14 # TODO hardcode\n    # From state\n    # backbone = None # from state for now, no need for conv nets\n    # From image\n    backbones = []\n    for _ in args.camera_names:\n        backbone = build_backbone(args)\n        backbones.append(backbone)\n    model = CNNMLP(\n        backbones,\n        state_dim=state_dim,\n        camera_names=args.camera_names,\n    )\n    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(\"number of parameters: %.2fM\" % (n_parameters/1e6,))\n    return model"
        }
    ]
}