{
    "summary": "EpisodicDataset class processes data, applies augmentations, handles legacy data, and provides torch tensor compatibility for model usage. It loads images, creates masks, retrieves stats, and includes functions for locating HDF5 files, generating batches, pre/post-processing, sampling poses, calculating means, and setting random seeds.",
    "details": [
        {
            "comment": "Class EpisodicDataset loads episode data from a list of paths. It can optionally augment images depending on the chosen policy class. The dataset is initialized with the given parameters, including the number of episodes, their IDs, and lengths. It calculates the cumulative length of episodes and checks if the policy class is \"Diffusion\" to determine whether or not to apply image augmentations.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":0-32",
            "content": "import numpy as np\nimport torch\nimport os\nimport h5py\nimport pickle\nimport fnmatch\nimport cv2\nfrom time import time\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision.transforms as transforms\nimport IPython\ne = IPython.embed\ndef flatten_list(l):\n    return [item for sublist in l for item in sublist]\nclass EpisodicDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset_path_list, camera_names, norm_stats, episode_ids, episode_len, chunk_size, policy_class):\n        super(EpisodicDataset).__init__()\n        self.episode_ids = episode_ids\n        self.dataset_path_list = dataset_path_list\n        self.camera_names = camera_names\n        self.norm_stats = norm_stats\n        self.episode_len = episode_len\n        self.chunk_size = chunk_size\n        self.cumulative_len = np.cumsum(self.episode_len)\n        self.max_episode_len = max(episode_len)\n        self.policy_class = policy_class\n        if self.policy_class == 'Diffusion':\n            self.augment_images = True\n        else:\n            self.augment_images = False"
        },
        {
            "comment": "This code initializes transformations and is_sim, defines a function to locate transition based on index, and gets item at specified index by locating the transition using episode ID and start timestamp. It also handles legacy data without certain attributes.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":33-57",
            "content": "        self.transformations = None\n        self.__getitem__(0) # initialize self.is_sim and self.transformations\n        self.is_sim = False\n    # def __len__(self):\n    #     return sum(self.episode_len)\n    def _locate_transition(self, index):\n        assert index < self.cumulative_len[-1]\n        episode_index = np.argmax(self.cumulative_len > index) # argmax returns first True index\n        start_ts = index - (self.cumulative_len[episode_index] - self.episode_len[episode_index])\n        episode_id = self.episode_ids[episode_index]\n        return episode_id, start_ts\n    def __getitem__(self, index):\n        episode_id, start_ts = self._locate_transition(index)\n        dataset_path = self.dataset_path_list[episode_id]\n        try:\n            # print(dataset_path)\n            with h5py.File(dataset_path, 'r') as root:\n                try: # some legacy data does not have this attribute\n                    is_sim = root.attrs['sim']\n                except:\n                    is_sim = False\n                compressed = root.attrs.get('compress', False)"
        },
        {
            "comment": "This code block is for processing the input data based on whether a base action is specified or not. If it exists, the base action is preprocessed and concatenated with the given action, otherwise a dummy base action is added before concatenation. It also stores the initial observation and image data at the start timestamp.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":58-76",
            "content": "                if '/base_action' in root:\n                    base_action = root['/base_action'][()]\n                    base_action = preprocess_base_action(base_action)\n                    action = np.concatenate([root['/action'][()], base_action], axis=-1)\n                else:  \n                    action = root['/action'][()]\n                    dummy_base_action = np.zeros([action.shape[0], 2])\n                    action = np.concatenate([action, dummy_base_action], axis=-1)\n                original_action_shape = action.shape\n                episode_len = original_action_shape[0]\n                # get observation at start_ts only\n                qpos = root['/observations/qpos'][start_ts]\n                qvel = root['/observations/qvel'][start_ts]\n                image_dict = dict()\n                for cam_name in self.camera_names:\n                    image_dict[cam_name] = root[f'/observations/images/{cam_name}'][start_ts]\n                if compressed:\n                    for cam_name in image_dict.keys():"
        },
        {
            "comment": "This code segment is preprocessing video data for an agent in a simulation. It loads and decompresses images from the dictionary, adjusts actions based on timestamps, pads actions to match the maximum episode length, creates a padding mask, and stores camera images into a list.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":77-98",
            "content": "                        decompressed_image = cv2.imdecode(image_dict[cam_name], 1)\n                        image_dict[cam_name] = np.array(decompressed_image)\n                # get all actions after and including start_ts\n                if is_sim:\n                    action = action[start_ts:]\n                    action_len = episode_len - start_ts\n                else:\n                    action = action[max(0, start_ts - 1):] # hack, to make timesteps more aligned\n                    action_len = episode_len - max(0, start_ts - 1) # hack, to make timesteps more aligned\n            # self.is_sim = is_sim\n            padded_action = np.zeros((self.max_episode_len, original_action_shape[1]), dtype=np.float32)\n            padded_action[:action_len] = action\n            is_pad = np.zeros(self.max_episode_len)\n            is_pad[action_len:] = 1\n            padded_action = padded_action[:self.chunk_size]\n            is_pad = is_pad[:self.chunk_size]\n            # new axis for different cameras\n            all_cam_images = []"
        },
        {
            "comment": "The code reads images from multiple camera sources, stacks them into a single numpy array, and converts the arrays to torch tensors. It then rearranges the image tensor's dimensions for compatibility with the model, applies optional augmentations such as cropping and rotation, and assigns boolean values to indicate padding positions.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":99-120",
            "content": "            for cam_name in self.camera_names:\n                all_cam_images.append(image_dict[cam_name])\n            all_cam_images = np.stack(all_cam_images, axis=0)\n            # construct observations\n            image_data = torch.from_numpy(all_cam_images)\n            qpos_data = torch.from_numpy(qpos).float()\n            action_data = torch.from_numpy(padded_action).float()\n            is_pad = torch.from_numpy(is_pad).bool()\n            # channel last\n            image_data = torch.einsum('k h w c -> k c h w', image_data)\n            # augmentation\n            if self.transformations is None:\n                print('Initializing transformations')\n                original_size = image_data.shape[2:]\n                ratio = 0.95\n                self.transformations = [\n                    transforms.RandomCrop(size=[int(original_size[0] * ratio), int(original_size[1] * ratio)]),\n                    transforms.Resize(original_size, antialias=True),\n                    transforms.RandomRotation(degrees=[-5.0, 5.0], expand=False),"
        },
        {
            "comment": "The code applies transformations to image data, normalizes the image and action data based on policy class, and adjusts qpos data based on mean and std. It also handles any potential errors while loading the dataset.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":121-144",
            "content": "                    transforms.ColorJitter(brightness=0.3, contrast=0.4, saturation=0.5) #, hue=0.08)\n                ]\n            if self.augment_images:\n                for transform in self.transformations:\n                    image_data = transform(image_data)\n            # normalize image and change dtype to float\n            image_data = image_data / 255.0\n            if self.policy_class == 'Diffusion':\n                # normalize to [-1, 1]\n                action_data = ((action_data - self.norm_stats[\"action_min\"]) / (self.norm_stats[\"action_max\"] - self.norm_stats[\"action_min\"])) * 2 - 1\n            else:\n                # normalize to mean 0 std 1\n                action_data = (action_data - self.norm_stats[\"action_mean\"]) / self.norm_stats[\"action_std\"]\n            qpos_data = (qpos_data - self.norm_stats[\"qpos_mean\"]) / self.norm_stats[\"qpos_std\"]\n        except:\n            print(f'Error loading {dataset_path} in __getitem__')\n            quit()\n        # print(image_data.dtype, qpos_data.dtype, action_data.dtype, is_pad.dtype)"
        },
        {
            "comment": "This function, \"get_norm_stats\", takes a list of dataset paths and returns image data, qpos data, action data, and an indicator whether the pad is needed or not. It first initializes empty lists for all_qpos_data, all_action_data, and all_episode_len. Then, it iterates over each dataset path in the list. For each path, it opens the HDF5 file using 'r' mode and extracts qpos and qvel data from specific paths within the file. If a '/base_action' path exists, it retrieves base_action data and preprocesses it before concatenating with action data. Otherwise, it assumes dummy base_action and performs concatenation. The extracted data is appended to their respective lists, but if an error occurs during loading, the function prints an error message and quits.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":145-170",
            "content": "        return image_data, qpos_data, action_data, is_pad\ndef get_norm_stats(dataset_path_list):\n    all_qpos_data = []\n    all_action_data = []\n    all_episode_len = []\n    for dataset_path in dataset_path_list:\n        try:\n            with h5py.File(dataset_path, 'r') as root:\n                qpos = root['/observations/qpos'][()]\n                qvel = root['/observations/qvel'][()]\n                if '/base_action' in root:\n                    base_action = root['/base_action'][()]\n                    base_action = preprocess_base_action(base_action)\n                    action = np.concatenate([root['/action'][()], base_action], axis=-1)\n                else:\n                    action = root['/action'][()]\n                    dummy_base_action = np.zeros([action.shape[0], 2])\n                    action = np.concatenate([action, dummy_base_action], axis=-1)\n        except Exception as e:\n            print(f'Error loading {dataset_path} in get_norm_stats')\n            print(e)\n            quit()\n        all_qpos_data.append(torch.from_numpy(qpos))"
        },
        {
            "comment": "This code is processing and normalizing data for training in a machine learning context. It appends action and qpos data, normalizes the action and qpos data by calculating their means, standard deviations, and clipping them to avoid large values, and stores these statistics along with minimum and maximum action values and an example qpos. Finally, it returns these statistics and all episode lengths.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":171-195",
            "content": "        all_action_data.append(torch.from_numpy(action))\n        all_episode_len.append(len(qpos))\n    all_qpos_data = torch.cat(all_qpos_data, dim=0)\n    all_action_data = torch.cat(all_action_data, dim=0)\n    # normalize action data\n    action_mean = all_action_data.mean(dim=[0]).float()\n    action_std = all_action_data.std(dim=[0]).float()\n    action_std = torch.clip(action_std, 1e-2, np.inf) # clipping\n    # normalize qpos data\n    qpos_mean = all_qpos_data.mean(dim=[0]).float()\n    qpos_std = all_qpos_data.std(dim=[0]).float()\n    qpos_std = torch.clip(qpos_std, 1e-2, np.inf) # clipping\n    action_min = all_action_data.min(dim=0).values.float()\n    action_max = all_action_data.max(dim=0).values.float()\n    eps = 0.0001\n    stats = {\"action_mean\": action_mean.numpy(), \"action_std\": action_std.numpy(),\n             \"action_min\": action_min.numpy() - eps,\"action_max\": action_max.numpy() + eps,\n             \"qpos_mean\": qpos_mean.numpy(), \"qpos_std\": qpos_std.numpy(),\n             \"example_qpos\": qpos}\n    return stats, all_episode_len"
        },
        {
            "comment": "The code provides two functions: \"find_all_hdf5\" and \"BatchSampler\". The first function searches for all HDF5 files in a specified directory, excluding any with 'features' in their name or 'mirror' if skipping mirrored data is set. It then returns the list of found files. The second function, BatchSampler, generates batches of samples from a list of episode lengths and sample weights (if provided). It randomly selects an episode, a step within that episode, and appends it to the batch until the desired batch size is reached.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":197-217",
            "content": "def find_all_hdf5(dataset_dir, skip_mirrored_data):\n    hdf5_files = []\n    for root, dirs, files in os.walk(dataset_dir):\n        for filename in fnmatch.filter(files, '*.hdf5'):\n            if 'features' in filename: continue\n            if skip_mirrored_data and 'mirror' in filename:\n                continue\n            hdf5_files.append(os.path.join(root, filename))\n    print(f'Found {len(hdf5_files)} hdf5 files')\n    return hdf5_files\ndef BatchSampler(batch_size, episode_len_l, sample_weights):\n    sample_probs = np.array(sample_weights) / np.sum(sample_weights) if sample_weights is not None else None\n    sum_dataset_len_l = np.cumsum([0] + [np.sum(episode_len) for episode_len in episode_len_l])\n    while True:\n        batch = []\n        for _ in range(batch_size):\n            episode_idx = np.random.choice(len(episode_len_l), p=sample_probs)\n            step_idx = np.random.randint(sum_dataset_len_l[episode_idx], sum_dataset_len_l[episode_idx + 1])\n            batch.append(step_idx)\n        yield batch"
        },
        {
            "comment": "This function loads data from one or multiple directories, applying a name filter and splitting the data into training and validation sets. It also supports skipping mirrored data and loading pre-trained data. The train/val split is done based on a provided ratio, and the data is shuffled randomly before splitting.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":219-232",
            "content": "def load_data(dataset_dir_l, name_filter, camera_names, batch_size_train, batch_size_val, chunk_size, skip_mirrored_data=False, load_pretrain=False, policy_class=None, stats_dir_l=None, sample_weights=None, train_ratio=0.99):\n    if type(dataset_dir_l) == str:\n        dataset_dir_l = [dataset_dir_l]\n    dataset_path_list_list = [find_all_hdf5(dataset_dir, skip_mirrored_data) for dataset_dir in dataset_dir_l]\n    num_episodes_0 = len(dataset_path_list_list[0])\n    dataset_path_list = flatten_list(dataset_path_list_list)\n    dataset_path_list = [n for n in dataset_path_list if name_filter(n)]\n    num_episodes_l = [len(dataset_path_list) for dataset_path_list in dataset_path_list_list]\n    num_episodes_cumsum = np.cumsum(num_episodes_l)\n    # obtain train test split on dataset_dir_l[0]\n    shuffled_episode_ids_0 = np.random.permutation(num_episodes_0)\n    train_episode_ids_0 = shuffled_episode_ids_0[:int(train_ratio * num_episodes_0)]\n    val_episode_ids_0 = shuffled_episode_ids_0[int(train_ratio * num_episodes_0):]"
        },
        {
            "comment": "Code generates train and validation episode IDs for multiple datasets, concatenates them, and prints details about the data. It also loads normalization stats for qpos and action (if load_pretrain is True) from a specific file path. The code then calculates the length of each episode for training and validation sets based on all_episode_len list.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":233-246",
            "content": "    train_episode_ids_l = [train_episode_ids_0] + [np.arange(num_episodes) + num_episodes_cumsum[idx] for idx, num_episodes in enumerate(num_episodes_l[1:])]\n    val_episode_ids_l = [val_episode_ids_0]\n    train_episode_ids = np.concatenate(train_episode_ids_l)\n    val_episode_ids = np.concatenate(val_episode_ids_l)\n    print(f'\\n\\nData from: {dataset_dir_l}\\n- Train on {[len(x) for x in train_episode_ids_l]} episodes\\n- Test on {[len(x) for x in val_episode_ids_l]} episodes\\n\\n')\n    # obtain normalization stats for qpos and action\n    # if load_pretrain:\n    #     with open(os.path.join('/home/zfu/interbotix_ws/src/act/ckpts/pretrain_all', 'dataset_stats.pkl'), 'rb') as f:\n    #         norm_stats = pickle.load(f)\n    #     print('Loaded pretrain dataset stats')\n    _, all_episode_len = get_norm_stats(dataset_path_list)\n    train_episode_len_l = [[all_episode_len[i] for i in train_episode_ids] for train_episode_ids in train_episode_ids_l]\n    val_episode_len_l = [[all_episode_len[i] for i in val_episode_ids] for val_episode_ids in val_episode_ids_l]"
        },
        {
            "comment": "This code block initializes training and validation episode lengths, checks the stats directory type, fetches normalization statistics from HDF5 files, creates batch samplers for training and validation sets, constructs EpisodicDataset instances for training and validation data.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":247-263",
            "content": "    train_episode_len = flatten_list(train_episode_len_l)\n    val_episode_len = flatten_list(val_episode_len_l)\n    if stats_dir_l is None:\n        stats_dir_l = dataset_dir_l\n    elif type(stats_dir_l) == str:\n        stats_dir_l = [stats_dir_l]\n    norm_stats, _ = get_norm_stats(flatten_list([find_all_hdf5(stats_dir, skip_mirrored_data) for stats_dir in stats_dir_l]))\n    print(f'Norm stats from: {stats_dir_l}')\n    batch_sampler_train = BatchSampler(batch_size_train, train_episode_len_l, sample_weights)\n    batch_sampler_val = BatchSampler(batch_size_val, val_episode_len_l, None)\n    # print(f'train_episode_len: {train_episode_len}, val_episode_len: {val_episode_len}, train_episode_ids: {train_episode_ids}, val_episode_ids: {val_episode_ids}')\n    # construct dataset and dataloader\n    train_dataset = EpisodicDataset(dataset_path_list, camera_names, norm_stats, train_episode_ids, train_episode_len, chunk_size, policy_class)\n    val_dataset = EpisodicDataset(dataset_path_list, camera_names, norm_stats, val_episode_ids, val_episode_len, chunk_size, policy_class)"
        },
        {
            "comment": "This code sets the number of workers for training and validation data loaders based on whether images are being augmented or not. It also defines a function to calibrate linear velocity, smooths the base action using convolution with a moving average filter, and returns the train and validation dataloaders along with other variables.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":264-283",
            "content": "    train_num_workers = (8 if os.getlogin() == 'zfu' else 16) if train_dataset.augment_images else 2\n    val_num_workers = 8 if train_dataset.augment_images else 2\n    print(f'Augment images: {train_dataset.augment_images}, train_num_workers: {train_num_workers}, val_num_workers: {val_num_workers}')\n    train_dataloader = DataLoader(train_dataset, batch_sampler=batch_sampler_train, pin_memory=True, num_workers=train_num_workers, prefetch_factor=2)\n    val_dataloader = DataLoader(val_dataset, batch_sampler=batch_sampler_val, pin_memory=True, num_workers=val_num_workers, prefetch_factor=2)\n    return train_dataloader, val_dataloader, norm_stats, train_dataset.is_sim\ndef calibrate_linear_vel(base_action, c=None):\n    if c is None:\n        c = 0.0 # 0.19\n    v = base_action[..., 0]\n    w = base_action[..., 1]\n    base_action = base_action.copy()\n    base_action[..., 0] = v - c * w\n    return base_action\ndef smooth_base_action(base_action):\n    return np.stack([\n        np.convolve(base_action[:, i], np.ones(5)/5, mode='same') for i in range(base_action.shape[1])"
        },
        {
            "comment": "This code defines several functions for preprocessing and postprocessing base actions, as well as sampling random poses for objects. It uses numpy array manipulations and random sampling to accomplish these tasks. The calibration and smoothing of the base action are used to refine input data before it is passed on or returned from a function. The two pose-sampling functions generate random positions and orientations for an object (cube or peg) within specified ranges.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":284-323",
            "content": "    ], axis=-1).astype(np.float32)\ndef preprocess_base_action(base_action):\n    # base_action = calibrate_linear_vel(base_action)\n    base_action = smooth_base_action(base_action)\n    return base_action\ndef postprocess_base_action(base_action):\n    linear_vel, angular_vel = base_action\n    linear_vel *= 1.0\n    angular_vel *= 1.0\n    # angular_vel = 0\n    # if np.abs(linear_vel) < 0.05:\n    #     linear_vel = 0\n    return np.array([linear_vel, angular_vel])\n### env utils\ndef sample_box_pose():\n    x_range = [0.0, 0.2]\n    y_range = [0.4, 0.6]\n    z_range = [0.05, 0.05]\n    ranges = np.vstack([x_range, y_range, z_range])\n    cube_position = np.random.uniform(ranges[:, 0], ranges[:, 1])\n    cube_quat = np.array([1, 0, 0, 0])\n    return np.concatenate([cube_position, cube_quat])\ndef sample_insertion_pose():\n    # Peg\n    x_range = [0.1, 0.2]\n    y_range = [0.4, 0.6]\n    z_range = [0.05, 0.05]\n    ranges = np.vstack([x_range, y_range, z_range])\n    peg_position = np.random.uniform(ranges[:, 0], ranges[:, 1])\n    peg_quat = np.array([1, 0, 0, 0])"
        },
        {
            "comment": "Function: compute_dict_mean\nPurpose: Calculate the mean of values for each key in a list of dictionaries.\n\nFunction: detach_dict\nPurpose: Create a new dictionary where all values are detached from their current computation graph.\n\nFunction: set_seed\nPurpose: Set random seed for both PyTorch and NumPy to ensure reproducible results.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/utils.py\":324-359",
            "content": "    peg_pose = np.concatenate([peg_position, peg_quat])\n    # Socket\n    x_range = [-0.2, -0.1]\n    y_range = [0.4, 0.6]\n    z_range = [0.05, 0.05]\n    ranges = np.vstack([x_range, y_range, z_range])\n    socket_position = np.random.uniform(ranges[:, 0], ranges[:, 1])\n    socket_quat = np.array([1, 0, 0, 0])\n    socket_pose = np.concatenate([socket_position, socket_quat])\n    return peg_pose, socket_pose\n### helper functions\ndef compute_dict_mean(epoch_dicts):\n    result = {k: None for k in epoch_dicts[0]}\n    num_items = len(epoch_dicts)\n    for k in result:\n        value_sum = 0\n        for epoch_dict in epoch_dicts:\n            value_sum += epoch_dict[k]\n        result[k] = value_sum / num_items\n    return result\ndef detach_dict(d):\n    new_d = dict()\n    for k, v in d.items():\n        new_d[k] = v.detach()\n    return new_d\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    np.random.seed(seed)"
        }
    ]
}