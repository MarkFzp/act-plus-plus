{
    "summary": "This code includes ACT, Diffusion Policy, and VINN implementations with two simulated environments, installation instructions for dependencies and environment, demo scripts, data generation and visualization guides, training tips, and expected success rate evaluation.",
    "details": [
        {
            "comment": "This code contains the implementation of ACT, Diffusion Policy, and VINN along with two simulated environments (Transfer Cube and Bimanual Insertion) that can be trained and evaluated in sim or real settings. It also requires installing Mobile ALOHA from a separate repository, which has been forked from the ACT repo. The code is organized into several Python files, each responsible for specific aspects of the algorithms or environments. Demo scripts for simulated environments are available online.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/README.md\":0-19",
            "content": "# Imitation Learning algorithms and Co-training for Mobile ALOHA\n#### Project Website: https://mobile-aloha.github.io/\nThis repo contains the implementation of ACT, Diffusion Policy and VINN, together with 2 simulated environments:\nTransfer Cube and Bimanual Insertion. You can train and evaluate them in sim or real.\nFor real, you would also need to install [Mobile ALOHA](https://github.com/MarkFzp/mobile-aloha). This repo is forked from the [ACT repo](https://github.com/tonyzhaozh/act).\n### Updates:\nYou can find all scripted/human demo for simulated environments [here](https://drive.google.com/drive/folders/1gPR03v05S1xiInoVJn7G7VJ9pDCnxq9O?usp=share_link).\n### Repo Structure\n- ``imitate_episodes.py`` Train and Evaluate ACT\n- ``policy.py`` An adaptor for ACT policy\n- ``detr`` Model definitions of ACT, modified from DETR\n- ``sim_env.py`` Mujoco + DM_Control environments with joint space control\n- ``ee_sim_env.py`` Mujoco + DM_Control environments with EE space control\n- ``scripted_policy.py`` Scripted policies for sim environments"
        },
        {
            "comment": "This code provides installation instructions for the environment and dependencies needed to run the ALOHA codebase. It also mentions the necessary steps to set up a new terminal and highlights some of the available simulation experiments.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/README.md\":20-56",
            "content": "- ``constants.py`` Constants shared across files\n- ``utils.py`` Utils such as data loading and helper functions\n- ``visualize_episodes.py`` Save videos from a .hdf5 dataset\n### Installation\n    conda create -n aloha python=3.8.10\n    conda activate aloha\n    pip install torchvision\n    pip install torch\n    pip install pyquaternion\n    pip install pyyaml\n    pip install rospkg\n    pip install pexpect\n    pip install mujoco==2.3.7\n    pip install dm_control==1.0.14\n    pip install opencv-python\n    pip install matplotlib\n    pip install einops\n    pip install packaging\n    pip install h5py\n    pip install ipython\n    cd act/detr && pip install -e .\n- also need to install https://github.com/ARISE-Initiative/robomimic/tree/r2d2 (note the r2d2 branch) for Diffusion Policy by `pip install -e .`\n### Example Usages\nTo set up a new terminal, run:\n    conda activate aloha\n    cd <path to act repo>\n### Simulated experiments (LEGACY table-top ALOHA environments)\nWe use ``sim_transfer_cube_scripted`` task in the examples below. Another option is ``sim_insertion_scripted``."
        },
        {
            "comment": "This code provides instructions for generating and visualizing data, training the ACT model, and evaluating its performance. It also mentions the expected success rates for different tasks and includes an option for temporal ensembling.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/README.md\":57-76",
            "content": "To generated 50 episodes of scripted data, run:\n    python3 record_sim_episodes.py --task_name sim_transfer_cube_scripted --dataset_dir <data save dir> --num_episodes 50\nTo can add the flag ``--onscreen_render`` to see real-time rendering.\nTo visualize the simulated episodes after it is collected, run\n    python3 visualize_episodes.py --dataset_dir <data save dir> --episode_idx 0\nNote: to visualize data from the mobile-aloha hardware, use the visualize_episodes.py from https://github.com/MarkFzp/mobile-aloha\nTo train ACT:\n    # Transfer Cube task\n    python3 imitate_episodes.py --task_name sim_transfer_cube_scripted --ckpt_dir <ckpt dir> --policy_class ACT --kl_weight 10 --chunk_size 100 --hidden_dim 512 --batch_size 8 --dim_feedforward 3200 --num_epochs 2000  --lr 1e-5 --seed 0\nTo evaluate the policy, run the same command but add ``--eval``. This loads the best validation checkpoint.\nThe success rate should be around 90% for transfer cube, and around 50% for insertion.\nTo enable temporal ensembling, add flag ``--temporal_agg``."
        },
        {
            "comment": "This code snippet provides instructions for saving videos to a specified directory during rollouts, and suggests using the \"--onscreen_render\" option for real-time rendering. It recommends training for at least 5000 epochs or three to four times the length of data after loss plateaus for better results in real-world scenarios. The code also provides a link to additional tuning tips for further information and emphasizes that longer training can improve success rate and smoothness even when the loss has plateaued.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/README.md\":77-84",
            "content": "Videos will be saved to ``<ckpt_dir>`` for each rollout.\nYou can also add ``--onscreen_render`` to see real-time rendering during evaluation.\nFor real-world data where things can be harder to model, train for at least 5000 epochs or 3-4 times the length after the loss has plateaued.\nPlease refer to [tuning tips](https://docs.google.com/document/d/1FVIZfoALXg_ZkYKaYVh-qOlaXveq5CtvJHXkY25eYhs/edit?usp=sharing) for more info.\n### [ACT tuning tips](https://docs.google.com/document/d/1FVIZfoALXg_ZkYKaYVh-qOlaXveq5CtvJHXkY25eYhs/edit?usp=sharing)\nTL;DR: if your ACT policy is jerky or pauses in the middle of an episode, just train for longer! Success rate and smoothness can improve way after loss plateaus."
        }
    ]
}