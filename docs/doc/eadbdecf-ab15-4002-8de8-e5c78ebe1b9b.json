{
    "summary": "This program trains a policy network for robot control using reinforcement learning, VQ-VAE implementation, and behavioral cloning, while logging data, saving checkpoints, and validating performance.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines functions for a reinforcement learning task. It sets up the environment, loads data, and initializes policy models. The `get_auto_index` function is used to find the next available index in the dataset directory.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":0-34",
            "content": "import torch\nimport numpy as np\nimport os\nimport pickle\nimport argparse\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\nfrom itertools import repeat\nfrom tqdm import tqdm\nfrom einops import rearrange\nimport wandb\nimport time\nfrom torchvision import transforms\nfrom constants import FPS\nfrom constants import PUPPET_GRIPPER_JOINT_OPEN\nfrom utils import load_data # data functions\nfrom utils import sample_box_pose, sample_insertion_pose # robot functions\nfrom utils import compute_dict_mean, set_seed, detach_dict, calibrate_linear_vel, postprocess_base_action # helper functions\nfrom policy import ACTPolicy, CNNMLPPolicy, DiffusionPolicy\nfrom visualize_episodes import save_videos\nfrom detr.models.latent_model import Latent_Model_Transformer\nfrom sim_env import BOX_POSE\nimport IPython\ne = IPython.embed\ndef get_auto_index(dataset_dir):\n    max_idx = 1000\n    for i in range(max_idx+1):\n        if not os.path.isfile(os.path.join(dataset_dir, f'qpos_{i}.npy')):\n            return i\n    raise Exception(f\"Error getting auto index, or more than {max_idx} episodes\")"
        },
        {
            "comment": "The code defines a main function that takes command line arguments and uses them to set up the environment for running the simulation. It first sets the seed, then parses various parameters such as is_eval, ckpt_dir, policy_class, onscreen_render, task_name, batch_size_train, batch_size_val, num_steps, eval_every, validate_every, save_every, and resume_ckpt_path. It also determines if the task is simulation-based or not, then retrieves the task parameters from either SIM_TASK_CONFIGS or TASK_CONFIGS based on the task name. These parameters include dataset_dir, episode_len, camera_names, and stats_dir.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":36-64",
            "content": "def main(args):\n    set_seed(1)\n    # command line parameters\n    is_eval = args['eval']\n    ckpt_dir = args['ckpt_dir']\n    policy_class = args['policy_class']\n    onscreen_render = args['onscreen_render']\n    task_name = args['task_name']\n    batch_size_train = args['batch_size']\n    batch_size_val = args['batch_size']\n    num_steps = args['num_steps']\n    eval_every = args['eval_every']\n    validate_every = args['validate_every']\n    save_every = args['save_every']\n    resume_ckpt_path = args['resume_ckpt_path']\n    # get task parameters\n    is_sim = task_name[:4] == 'sim_'\n    if is_sim or task_name == 'all':\n        from constants import SIM_TASK_CONFIGS\n        task_config = SIM_TASK_CONFIGS[task_name]\n    else:\n        from aloha_scripts.constants import TASK_CONFIGS\n        task_config = TASK_CONFIGS[task_name]\n    dataset_dir = task_config['dataset_dir']\n    # num_episodes = task_config['num_episodes']\n    episode_len = task_config['episode_len']\n    camera_names = task_config['camera_names']\n    stats_dir = task_config.get('stats_dir', None)"
        },
        {
            "comment": "This code sets various fixed parameters for the ACT policy. It gets the sample weights, train ratio, and name filter from the task configuration. The state dimension is set to 14. Backbone learning rate is set to 1e-5 with a predefined backbone model. If the policy class is ACT, it further defines encoder layers, decoder layers, number of attention heads, and other configurations for the policy based on provided arguments. Camera names are also defined if needed. It also handles whether or not to use VQ (if specified by args).",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":65-89",
            "content": "    sample_weights = task_config.get('sample_weights', None)\n    train_ratio = task_config.get('train_ratio', 0.99)\n    name_filter = task_config.get('name_filter', lambda n: True)\n    # fixed parameters\n    state_dim = 14\n    lr_backbone = 1e-5\n    backbone = 'resnet18'\n    if policy_class == 'ACT':\n        enc_layers = 4\n        dec_layers = 7\n        nheads = 8\n        policy_config = {'lr': args['lr'],\n                         'num_queries': args['chunk_size'],\n                         'kl_weight': args['kl_weight'],\n                         'hidden_dim': args['hidden_dim'],\n                         'dim_feedforward': args['dim_feedforward'],\n                         'lr_backbone': lr_backbone,\n                         'backbone': backbone,\n                         'enc_layers': enc_layers,\n                         'dec_layers': dec_layers,\n                         'nheads': nheads,\n                         'camera_names': camera_names,\n                         'vq': args['use_vq'],\n                         'vq_class': args['vq_class'],"
        },
        {
            "comment": "This code is setting up different configurations for the policy based on the given policy_class. The 'AuxCritic' configuration includes an auxiliary critic, 'Diffusion' uses diffusion-based policy, and 'CNNMLP' uses a CNN and MLP-based policy. All configurations include learning rate (lr), camera names, and actuator network directory settings.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":90-114",
            "content": "                         'vq_dim': args['vq_dim'],\n                         'action_dim': 16,\n                         'no_encoder': args['no_encoder'],\n                         }\n    elif policy_class == 'Diffusion':\n        policy_config = {'lr': args['lr'],\n                         'camera_names': camera_names,\n                         'action_dim': 16,\n                         'observation_horizon': 1,\n                         'action_horizon': 8,\n                         'prediction_horizon': args['chunk_size'],\n                         'num_queries': args['chunk_size'],\n                         'num_inference_timesteps': 10,\n                         'ema_power': 0.75,\n                         'vq': False,\n                         }\n    elif policy_class == 'CNNMLP':\n        policy_config = {'lr': args['lr'], 'lr_backbone': lr_backbone, 'backbone' : backbone, 'num_queries': 1,\n                         'camera_names': camera_names,}\n    else:\n        raise NotImplementedError\n    actuator_config = {\n        'actuator_network_dir': args['actuator_network_dir'],"
        },
        {
            "comment": "The code is defining and initializing two dictionaries: 'train_args' and 'config'. These dictionaries store various arguments for the training process. The code also checks if a directory exists and creates it if not, and stores configuration information in a file named 'config.pkl' within that directory. This information will likely be used to train an agent for a specific task or environment.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":115-145",
            "content": "        'history_len': args['history_len'],\n        'future_len': args['future_len'],\n        'prediction_len': args['prediction_len'],\n    }\n    config = {\n        'num_steps': num_steps,\n        'eval_every': eval_every,\n        'validate_every': validate_every,\n        'save_every': save_every,\n        'ckpt_dir': ckpt_dir,\n        'resume_ckpt_path': resume_ckpt_path,\n        'episode_len': episode_len,\n        'state_dim': state_dim,\n        'lr': args['lr'],\n        'policy_class': policy_class,\n        'onscreen_render': onscreen_render,\n        'policy_config': policy_config,\n        'task_name': task_name,\n        'seed': args['seed'],\n        'temporal_agg': args['temporal_agg'],\n        'camera_names': camera_names,\n        'real_robot': not is_sim,\n        'load_pretrain': args['load_pretrain'],\n        'actuator_config': actuator_config,\n    }\n    if not os.path.isdir(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    config_path = os.path.join(ckpt_dir, 'config.pkl')\n    expr_name = ckpt_dir.split('/')[-1]"
        },
        {
            "comment": "The code initializes the WandB for evaluation, updates the config file if not in evaluation mode, and then evaluates different checkpoints. It logs success rate and average return for each checkpoint, prints them on console, and exits the program. If in training mode, it loads data, creates dataloaders, and returns necessary objects.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":146-164",
            "content": "    if not is_eval:\n        wandb.init(project=\"mobile-aloha2\", reinit=True, entity=\"mobile-aloha2\", name=expr_name)\n        wandb.config.update(config)\n    with open(config_path, 'wb') as f:\n        pickle.dump(config, f)\n    if is_eval:\n        ckpt_names = [f'policy_last.ckpt']\n        results = []\n        for ckpt_name in ckpt_names:\n            success_rate, avg_return = eval_bc(config, ckpt_name, save_episode=True, num_rollouts=10)\n            # wandb.log({'success_rate': success_rate, 'avg_return': avg_return})\n            results.append([ckpt_name, success_rate, avg_return])\n        for ckpt_name, success_rate, avg_return in results:\n            print(f'{ckpt_name}: {success_rate=} {avg_return=}')\n        print()\n        exit()\n    train_dataloader, val_dataloader, stats, _ = load_data(dataset_dir, name_filter, camera_names, batch_size_train, batch_size_val, args['chunk_size'], args['skip_mirrored_data'], config['load_pretrain'], policy_class, stats_dir_l=stats_dir, sample_weights=sample_weights, train_ratio=train_ratio)"
        },
        {
            "comment": "This code saves dataset statistics, trains a behavioral cloning model, and saves the best checkpoint. It also creates a policy object based on the policy class and configures an optimizer for it.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":166-197",
            "content": "    # save dataset stats\n    stats_path = os.path.join(ckpt_dir, f'dataset_stats.pkl')\n    with open(stats_path, 'wb') as f:\n        pickle.dump(stats, f)\n    best_ckpt_info = train_bc(train_dataloader, val_dataloader, config)\n    best_step, min_val_loss, best_state_dict = best_ckpt_info\n    # save best checkpoint\n    ckpt_path = os.path.join(ckpt_dir, f'policy_best.ckpt')\n    torch.save(best_state_dict, ckpt_path)\n    print(f'Best ckpt, val loss {min_val_loss:.6f} @ step{best_step}')\n    wandb.finish()\ndef make_policy(policy_class, policy_config):\n    if policy_class == 'ACT':\n        policy = ACTPolicy(policy_config)\n    elif policy_class == 'CNNMLP':\n        policy = CNNMLPPolicy(policy_config)\n    elif policy_class == 'Diffusion':\n        policy = DiffusionPolicy(policy_config)\n    else:\n        raise NotImplementedError\n    return policy\ndef make_optimizer(policy_class, policy):\n    if policy_class == 'ACT':\n        optimizer = policy.configure_optimizers()\n    elif policy_class == 'CNNMLP':\n        optimizer = policy.configure_optimizers()"
        },
        {
            "comment": "This code snippet checks the policy class and configures the optimizer accordingly. If the policy class is 'Diffusion', it sets the optimizer using the policy's method. For any other policy class, a NotImplementedError is raised. The get_image function takes timestep (ts), camera names, and rand_crop_resize flag as input. It retrieves images from ts observation and reshapes them into a tensor for further processing. If rand_crop_resize is True, it randomly crops and resizes the image while maintaining aspect ratio.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":198-221",
            "content": "    elif policy_class == 'Diffusion':\n        optimizer = policy.configure_optimizers()\n    else:\n        raise NotImplementedError\n    return optimizer\ndef get_image(ts, camera_names, rand_crop_resize=False):\n    curr_images = []\n    for cam_name in camera_names:\n        curr_image = rearrange(ts.observation['images'][cam_name], 'h w c -> c h w')\n        curr_images.append(curr_image)\n    curr_image = np.stack(curr_images, axis=0)\n    curr_image = torch.from_numpy(curr_image / 255.0).float().cuda().unsqueeze(0)\n    if rand_crop_resize:\n        print('rand crop resize is used!')\n        original_size = curr_image.shape[-2:]\n        ratio = 0.95\n        curr_image = curr_image[..., int(original_size[0] * (1 - ratio) / 2): int(original_size[0] * (1 + ratio) / 2),\n                     int(original_size[1] * (1 - ratio) / 2): int(original_size[1] * (1 + ratio) / 2)]\n        curr_image = curr_image.squeeze(0)\n        resize_transform = transforms.Resize(original_size, antialias=True)\n        curr_image = resize_transform(curr_image)"
        },
        {
            "comment": "The code snippet loads a policy model from a checkpoint file and sets the model to evaluation mode. It also initializes variables related to the task, such as state dimensions and camera names. The policy is created using a specified class and configuration, and if the policy uses a VQ-VAE, it initializes the corresponding dimensions.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":222-252",
            "content": "        curr_image = curr_image.unsqueeze(0)\n    return curr_image\ndef eval_bc(config, ckpt_name, save_episode=True, num_rollouts=50):\n    set_seed(1000)\n    ckpt_dir = config['ckpt_dir']\n    state_dim = config['state_dim']\n    real_robot = config['real_robot']\n    policy_class = config['policy_class']\n    onscreen_render = config['onscreen_render']\n    policy_config = config['policy_config']\n    camera_names = config['camera_names']\n    max_timesteps = config['episode_len']\n    task_name = config['task_name']\n    temporal_agg = config['temporal_agg']\n    onscreen_cam = 'angle'\n    vq = config['policy_config']['vq']\n    actuator_config = config['actuator_config']\n    use_actuator_net = actuator_config['actuator_network_dir'] is not None\n    # load policy and stats\n    ckpt_path = os.path.join(ckpt_dir, ckpt_name)\n    policy = make_policy(policy_class, policy_config)\n    loading_status = policy.deserialize(torch.load(ckpt_path))\n    print(loading_status)\n    policy.cuda()\n    policy.eval()\n    if vq:\n        vq_dim = config['policy_config']['vq_dim']"
        },
        {
            "comment": "This code is loading a policy from the specified checkpoint path and a latent model from the specified latent_model_ckpt_path. It also loads dataset statistics from stats_path. Additionally, if use_actuator_net is True, it initializes an ActuatorNetwork object with specific parameters, and loads the actuator network from its designated checkpoint path.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":253-273",
            "content": "        vq_class = config['policy_config']['vq_class']\n        latent_model = Latent_Model_Transformer(vq_dim, vq_dim, vq_class)\n        latent_model_ckpt_path = os.path.join(ckpt_dir, 'latent_model_last.ckpt')\n        latent_model.deserialize(torch.load(latent_model_ckpt_path))\n        latent_model.eval()\n        latent_model.cuda()\n        print(f'Loaded policy from: {ckpt_path}, latent model from: {latent_model_ckpt_path}')\n    else:\n        print(f'Loaded: {ckpt_path}')\n    stats_path = os.path.join(ckpt_dir, f'dataset_stats.pkl')\n    with open(stats_path, 'rb') as f:\n        stats = pickle.load(f)\n    # if use_actuator_net:\n    #     prediction_len = actuator_config['prediction_len']\n    #     future_len = actuator_config['future_len']\n    #     history_len = actuator_config['history_len']\n    #     actuator_network_dir = actuator_config['actuator_network_dir']\n    #     from act.train_actuator_network import ActuatorNetwork\n    #     actuator_network = ActuatorNetwork(prediction_len)\n    #     actuator_network_path = os.path.join(actuator_network_dir, 'actuator_net_last.ckpt')"
        },
        {
            "comment": "Loading the actuator network from the specified path, evaluating the network, moving it to GPU if available, and printing a message confirming the loading status. The actuator_net_stats.pkl file is opened and actuator stats are loaded. Two lambda functions, actuator_unnorm and actuator_norm, are defined for data normalization. A function named collect_base_action is defined to collect base actions after post-processing them. A pre_process lambda function is also defined for normalizing the state qpos.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":274-289",
            "content": "    #     loading_status = actuator_network.load_state_dict(torch.load(actuator_network_path))\n    #     actuator_network.eval()\n    #     actuator_network.cuda()\n    #     print(f'Loaded actuator network from: {actuator_network_path}, {loading_status}')\n    #     actuator_stats_path  = os.path.join(actuator_network_dir, 'actuator_net_stats.pkl')\n    #     with open(actuator_stats_path, 'rb') as f:\n    #         actuator_stats = pickle.load(f)\n    #     actuator_unnorm = lambda x: x * actuator_stats['commanded_speed_std'] + actuator_stats['commanded_speed_std']\n    #     actuator_norm = lambda x: (x - actuator_stats['observed_speed_mean']) / actuator_stats['observed_speed_mean']\n    #     def collect_base_action(all_actions, norm_episode_all_base_actions):\n    #         post_processed_actions = post_process(all_actions.squeeze(0).cpu().numpy())\n    #         norm_episode_all_base_actions += actuator_norm(post_processed_actions[:, -2:]).tolist()\n    pre_process = lambda s_qpos: (s_qpos - stats['qpos_mean']) / stats['qpos_std']"
        },
        {
            "comment": "This code block initializes the environment and sets up parameters based on whether it is running in a real-world or simulation environment. It also accounts for temporal aggregation and potential delay in the real world. Finally, it initializes empty lists to store episode returns and highest rewards during the learning process.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":290-317",
            "content": "    if policy_class == 'Diffusion':\n        post_process = lambda a: ((a + 1) / 2) * (stats['action_max'] - stats['action_min']) + stats['action_min']\n    else:\n        post_process = lambda a: a * stats['action_std'] + stats['action_mean']\n    # load environment\n    if real_robot:\n        from aloha_scripts.robot_utils import move_grippers # requires aloha\n        from aloha_scripts.real_env import make_real_env # requires aloha\n        env = make_real_env(init_node=True, setup_robots=True, setup_base=True)\n        env_max_reward = 0\n    else:\n        from sim_env import make_sim_env\n        env = make_sim_env(task_name)\n        env_max_reward = env.task.max_reward\n    query_frequency = policy_config['num_queries']\n    if temporal_agg:\n        query_frequency = 1\n        num_queries = policy_config['num_queries']\n    if real_robot:\n        BASE_DELAY = 13\n        query_frequency -= BASE_DELAY\n    max_timesteps = int(max_timesteps * 1) # may increase for real-world tasks\n    episode_returns = []\n    highest_rewards = []"
        },
        {
            "comment": "This code initializes a rollout_id for a loop, sets the task based on the task name, resets the environment, renders the screen if desired, and prepares variables for an evaluation loop. If \"use_actuator_net\" is enabled, this will be used.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":318-346",
            "content": "    for rollout_id in range(num_rollouts):\n        if real_robot:\n            e()\n        rollout_id += 0\n        ### set task\n        if 'sim_transfer_cube' in task_name:\n            BOX_POSE[0] = sample_box_pose() # used in sim reset\n        elif 'sim_insertion' in task_name:\n            BOX_POSE[0] = np.concatenate(sample_insertion_pose()) # used in sim reset\n        ts = env.reset()\n        ### onscreen render\n        if onscreen_render:\n            ax = plt.subplot()\n            plt_img = ax.imshow(env._physics.render(height=480, width=640, camera_id=onscreen_cam))\n            plt.ion()\n        ### evaluation loop\n        if temporal_agg:\n            all_time_actions = torch.zeros([max_timesteps, max_timesteps+num_queries, 16]).cuda()\n        # qpos_history = torch.zeros((1, max_timesteps, state_dim)).cuda()\n        qpos_history_raw = np.zeros((max_timesteps, state_dim))\n        image_list = [] # for visualization\n        qpos_list = []\n        target_qpos_list = []\n        rewards = []\n        # if use_actuator_net:"
        },
        {
            "comment": "The code updates the onscreen render and waits for a delay (DT), processes previous timestep to get qpos and image_list, and pre-processes qpos. It does this within a loop for maximum timesteps, with timing measurements at specific points.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":347-369",
            "content": "        #     norm_episode_all_base_actions = [actuator_norm(np.zeros(history_len, 2)).tolist()]\n        with torch.inference_mode():\n            time0 = time.time()\n            DT = 1 / FPS\n            culmulated_delay = 0 \n            for t in range(max_timesteps):\n                time1 = time.time()\n                ### update onscreen render and wait for DT\n                if onscreen_render:\n                    image = env._physics.render(height=480, width=640, camera_id=onscreen_cam)\n                    plt_img.set_data(image)\n                    plt.pause(DT)\n                ### process previous timestep to get qpos and image_list\n                time2 = time.time()\n                obs = ts.observation\n                if 'images' in obs:\n                    image_list.append(obs['images'])\n                else:\n                    image_list.append({'main': obs['image']})\n                qpos_numpy = np.array(obs['qpos'])\n                qpos_history_raw[t] = qpos_numpy\n                qpos = pre_process(qpos_numpy)"
        },
        {
            "comment": "This code performs query-based policy execution in a reinforcement learning environment. It prepares input data and queries the policy network for action choices based on the current state. If the frequency requirement is met, it captures the image from a specified camera and applies any required preprocessing. The code also includes a warm-up step to prepare the neural network before executing the policy, and handles generating samples from a latent model if necessary.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":370-391",
            "content": "                qpos = torch.from_numpy(qpos).float().cuda().unsqueeze(0)\n                # qpos_history[:, t] = qpos\n                if t % query_frequency == 0:\n                    curr_image = get_image(ts, camera_names, rand_crop_resize=(config['policy_class'] == 'Diffusion'))\n                # print('get image: ', time.time() - time2)\n                if t == 0:\n                    # warm up\n                    for _ in range(10):\n                        policy(qpos, curr_image)\n                    print('network warm up done')\n                    time1 = time.time()\n                ### query policy\n                time3 = time.time()\n                if config['policy_class'] == \"ACT\":\n                    if t % query_frequency == 0:\n                        if vq:\n                            if rollout_id == 0:\n                                for _ in range(10):\n                                    vq_sample = latent_model.generate(1, temperature=1, x=None)\n                                    print(torch.nonzero(vq_sample[0])[:, 1].cpu().numpy())"
        },
        {
            "comment": "This code generates an action based on the given state and either additional latent variables or just the state. If using a real robot, it modifies the generated actions to account for a base delay in the actuator response time. If temporal aggregation is enabled, the code collects all-time actions, filters out any zeros, and assigns weights based on an exponential function of the action index.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":392-407",
            "content": "                            vq_sample = latent_model.generate(1, temperature=1, x=None)\n                            all_actions = policy(qpos, curr_image, vq_sample=vq_sample)\n                        else:\n                            # e()\n                            all_actions = policy(qpos, curr_image)\n                        # if use_actuator_net:\n                        #     collect_base_action(all_actions, norm_episode_all_base_actions)\n                        if real_robot:\n                            all_actions = torch.cat([all_actions[:, :-BASE_DELAY, :-2], all_actions[:, BASE_DELAY:, -2:]], dim=2)\n                    if temporal_agg:\n                        all_time_actions[[t], t:t+num_queries] = all_actions\n                        actions_for_curr_step = all_time_actions[:, t]\n                        actions_populated = torch.all(actions_for_curr_step != 0, axis=1)\n                        actions_for_curr_step = actions_for_curr_step[actions_populated]\n                        k = 0.01\n                        exp_weights = np.exp(-k * np.arange(len(actions_for_curr_step)))"
        },
        {
            "comment": "This code appears to be part of a larger program that utilizes different policies and actions for robotic control. It seems to handle policy selection based on the current time step, t, and query frequency. If the policy is set as \"Diffusion\", it retrieves new actions from the policy at specific intervals, potentially accounting for delays or base actions. The code also handles real robot interactions, adjusting action sequences accordingly.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":408-422",
            "content": "                        exp_weights = exp_weights / exp_weights.sum()\n                        exp_weights = torch.from_numpy(exp_weights).cuda().unsqueeze(dim=1)\n                        raw_action = (actions_for_curr_step * exp_weights).sum(dim=0, keepdim=True)\n                    else:\n                        raw_action = all_actions[:, t % query_frequency]\n                        # if t % query_frequency == query_frequency - 1:\n                        #     # zero out base actions to avoid overshooting\n                        #     raw_action[0, -2:] = 0\n                elif config['policy_class'] == \"Diffusion\":\n                    if t % query_frequency == 0:\n                        all_actions = policy(qpos, curr_image)\n                        # if use_actuator_net:\n                        #     collect_base_action(all_actions, norm_episode_all_base_actions)\n                        if real_robot:\n                            all_actions = torch.cat([all_actions[:, :-BASE_DELAY, :-2], all_actions[:, BASE_DELAY:, -2:]], dim=2)"
        },
        {
            "comment": "This code selects the policy based on the config value and performs necessary actions. It uses CNNMLP for querying the policy, post-processes the raw action output, and assigns target_qpos from the processed action values. It also handles actuator net usage with temporal aggregation if configured.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":423-443",
            "content": "                    raw_action = all_actions[:, t % query_frequency]\n                elif config['policy_class'] == \"CNNMLP\":\n                    raw_action = policy(qpos, curr_image)\n                    all_actions = raw_action.unsqueeze(0)\n                    # if use_actuator_net:\n                    #     collect_base_action(all_actions, norm_episode_all_base_actions)\n                else:\n                    raise NotImplementedError\n                # print('query policy: ', time.time() - time3)\n                ### post-process actions\n                time4 = time.time()\n                raw_action = raw_action.squeeze(0).cpu().numpy()\n                action = post_process(raw_action)\n                target_qpos = action[:-2]\n                # if use_actuator_net:\n                #     assert(not temporal_agg)\n                #     if t % prediction_len == 0:\n                #         offset_start_ts = t + history_len\n                #         actuator_net_in = np.array(norm_episode_all_base_actions[offset_start_ts - history_len: offset_start_ts + future_len])"
        },
        {
            "comment": "Code segment is responsible for updating the base action based on whether an actuator network prediction is available or not. If a prediction exists, it normalizes and detaches the prediction before selecting the relevant chunk. Else, it uses the last two elements of the given action as the base action after applying linear velocity calibration (commented out) and post-processing (also commented out). The code then steps the environment using the calculated base action and appends current qpos to qpos_list and target_qpos to target_qpos_list for visualization purposes.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":444-464",
            "content": "                #         actuator_net_in = torch.from_numpy(actuator_net_in).float().unsqueeze(dim=0).cuda()\n                #         pred = actuator_network(actuator_net_in)\n                #         base_action_chunk = actuator_unnorm(pred.detach().cpu().numpy()[0])\n                #     base_action = base_action_chunk[t % prediction_len]\n                # else:\n                base_action = action[-2:]\n                # base_action = calibrate_linear_vel(base_action, c=0.19)\n                # base_action = postprocess_base_action(base_action)\n                # print('post process: ', time.time() - time4)\n                ### step the environment\n                time5 = time.time()\n                if real_robot:\n                    ts = env.step(target_qpos, base_action)\n                else:\n                    ts = env.step(target_qpos)\n                # print('step env: ', time.time() - time5)\n                ### for visualization\n                qpos_list.append(qpos_numpy)\n                target_qpos_list.append(target_qpos)"
        },
        {
            "comment": "The code appends rewards to a list, calculates and controls sleep time for synchronization, handles step duration longer than DT by accumulating delay, prints warning and updates cumulative delay if necessary, calculates average FPS, closes the plot window. If real_robot is True, it opens grippers and saves qpos_history_raw in a specified directory with an auto-incrementing index.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":465-483",
            "content": "                rewards.append(ts.reward)\n                duration = time.time() - time1\n                sleep_time = max(0, DT - duration)\n                # print(sleep_time)\n                time.sleep(sleep_time)\n                # time.sleep(max(0, DT - duration - culmulated_delay))\n                if duration >= DT:\n                    culmulated_delay += (duration - DT)\n                    print(f'Warning: step duration: {duration:.3f} s at step {t} longer than DT: {DT} s, culmulated delay: {culmulated_delay:.3f} s')\n                # else:\n                #     culmulated_delay = max(0, culmulated_delay - (DT - duration))\n            print(f'Avg fps {max_timesteps / (time.time() - time0)}')\n            plt.close()\n        if real_robot:\n            move_grippers([env.puppet_bot_left, env.puppet_bot_right], [PUPPET_GRIPPER_JOINT_OPEN] * 2, move_time=0.5)  # open\n            # save qpos_history_raw\n            log_id = get_auto_index(ckpt_dir)\n            np.save(os.path.join(ckpt_dir, f'qpos_{log_id}.npy'), qpos_history_raw)"
        },
        {
            "comment": "The code plots the history of qpos for each dimension and saves it as an image, calculates episode return and highest reward, prints the results, and checks if the highest reward equals the environment's maximum reward. It then calculates the success rate based on the highest rewards.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":484-507",
            "content": "            plt.figure(figsize=(10, 20))\n            # plot qpos_history_raw for each qpos dim using subplots\n            for i in range(state_dim):\n                plt.subplot(state_dim, 1, i+1)\n                plt.plot(qpos_history_raw[:, i])\n                # remove x axis\n                if i != state_dim - 1:\n                    plt.xticks([])\n            plt.tight_layout()\n            plt.savefig(os.path.join(ckpt_dir, f'qpos_{log_id}.png'))\n            plt.close()\n        rewards = np.array(rewards)\n        episode_return = np.sum(rewards[rewards!=None])\n        episode_returns.append(episode_return)\n        episode_highest_reward = np.max(rewards)\n        highest_rewards.append(episode_highest_reward)\n        print(f'Rollout {rollout_id}\\n{episode_return=}, {episode_highest_reward=}, {env_max_reward=}, Success: {episode_highest_reward==env_max_reward}')\n        # if save_episode:\n        #     save_videos(image_list, DT, video_path=os.path.join(ckpt_dir, f'video{rollout_id}.mp4'))\n    success_rate = np.mean(np.array(highest_rewards) == env_max_reward)"
        },
        {
            "comment": "Code block calculates success rate and average return from episode results, displays summary in console, writes the summary to a text file along with episode returns and highest rewards.\n\nThe forward_pass function takes input data (image_data, qpos_data, action_data, is_pad) and passes it through the policy network.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":508-531",
            "content": "    avg_return = np.mean(episode_returns)\n    summary_str = f'\\nSuccess rate: {success_rate}\\nAverage return: {avg_return}\\n\\n'\n    for r in range(env_max_reward+1):\n        more_or_equal_r = (np.array(highest_rewards) >= r).sum()\n        more_or_equal_r_rate = more_or_equal_r / num_rollouts\n        summary_str += f'Reward >= {r}: {more_or_equal_r}/{num_rollouts} = {more_or_equal_r_rate*100}%\\n'\n    print(summary_str)\n    # save success rate to txt\n    result_file_name = 'result_' + ckpt_name.split('.')[0] + '.txt'\n    with open(os.path.join(ckpt_dir, result_file_name), 'w') as f:\n        f.write(summary_str)\n        f.write(repr(episode_returns))\n        f.write('\\n\\n')\n        f.write(repr(highest_rewards))\n    return success_rate, avg_return\ndef forward_pass(data, policy):\n    image_data, qpos_data, action_data, is_pad = data\n    image_data, qpos_data, action_data, is_pad = image_data.cuda(), qpos_data.cuda(), action_data.cuda(), is_pad.cuda()\n    return policy(qpos_data, image_data, action_data, is_pad) # TODO remove None"
        },
        {
            "comment": "The code defines a \"train_bc\" function which trains a policy using a specified data loader. It sets up various configurations, checks if it should load pre-trained weights or resume training from a previous checkpoint, and initializes the optimizer. The function uses a repeater to repeat the training data loader for consistency.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":534-559",
            "content": "def train_bc(train_dataloader, val_dataloader, config):\n    num_steps = config['num_steps']\n    ckpt_dir = config['ckpt_dir']\n    seed = config['seed']\n    policy_class = config['policy_class']\n    policy_config = config['policy_config']\n    eval_every = config['eval_every']\n    validate_every = config['validate_every']\n    save_every = config['save_every']\n    set_seed(seed)\n    policy = make_policy(policy_class, policy_config)\n    if config['load_pretrain']:\n        loading_status = policy.deserialize(torch.load(os.path.join('/home/zfu/interbotix_ws/src/act/ckpts/pretrain_all', 'policy_step_50000_seed_0.ckpt')))\n        print(f'loaded! {loading_status}')\n    if config['resume_ckpt_path'] is not None:\n        loading_status = policy.deserialize(torch.load(config['resume_ckpt_path']))\n        print(f'Resume policy from: {config[\"resume_ckpt_path\"]}, Status: {loading_status}')\n    policy.cuda()\n    optimizer = make_optimizer(policy_class, policy)\n    min_val_loss = np.inf\n    best_ckpt_info = None\n    train_dataloader = repeater(train_dataloader)"
        },
        {
            "comment": "This code is performing a validation step at certain intervals during training. It logs the validation summary to WandB and keeps track of the best validation loss seen so far. The best model checkpoint information is updated if the current validation loss is lower than the minimum previously observed.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":560-583",
            "content": "    for step in tqdm(range(num_steps+1)):\n        # validation\n        if step % validate_every == 0:\n            print('validating')\n            with torch.inference_mode():\n                policy.eval()\n                validation_dicts = []\n                for batch_idx, data in enumerate(val_dataloader):\n                    forward_dict = forward_pass(data, policy)\n                    validation_dicts.append(forward_dict)\n                    if batch_idx > 50:\n                        break\n                validation_summary = compute_dict_mean(validation_dicts)\n                epoch_val_loss = validation_summary['loss']\n                if epoch_val_loss < min_val_loss:\n                    min_val_loss = epoch_val_loss\n                    best_ckpt_info = (step, min_val_loss, deepcopy(policy.serialize()))\n            for k in list(validation_summary.keys()):\n                validation_summary[f'val_{k}'] = validation_summary.pop(k)            \n            wandb.log(validation_summary, step=step)\n            print(f'Val loss:   {epoch_val_loss:.5f}')"
        },
        {
            "comment": "The code performs validation, evaluation, and training steps. It logs the success rate of evaluations, saves checkpoints at certain intervals, trains a policy network using forward and backward passes, and logs data for later analysis.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":584-610",
            "content": "            summary_string = ''\n            for k, v in validation_summary.items():\n                summary_string += f'{k}: {v.item():.3f} '\n            print(summary_string)\n        # evaluation\n        if (step > 0) and (step % eval_every == 0):\n            # first save then eval\n            ckpt_name = f'policy_step_{step}_seed_{seed}.ckpt'\n            ckpt_path = os.path.join(ckpt_dir, ckpt_name)\n            torch.save(policy.serialize(), ckpt_path)\n            success, _ = eval_bc(config, ckpt_name, save_episode=True, num_rollouts=10)\n            wandb.log({'success': success}, step=step)\n        # training\n        policy.train()\n        optimizer.zero_grad()\n        data = next(train_dataloader)\n        forward_dict = forward_pass(data, policy)\n        # backward\n        loss = forward_dict['loss']\n        loss.backward()\n        optimizer.step()\n        wandb.log(forward_dict, step=step) # not great, make training 1-2% slower\n        if step % save_every == 0:\n            ckpt_path = os.path.join(ckpt_dir, f'policy_step_{step}_seed_{seed}.ckpt')"
        },
        {
            "comment": "The code defines a function to train and save a policy, repeats the data loader for multiple epochs, and takes command-line arguments for evaluation, on-screen rendering, checkpoint directory, and policy class. The training finishes when it finds the best model based on validation loss, saves it, and prints information about the best step, seed, and validation loss.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":611-637",
            "content": "            torch.save(policy.serialize(), ckpt_path)\n    ckpt_path = os.path.join(ckpt_dir, f'policy_last.ckpt')\n    torch.save(policy.serialize(), ckpt_path)\n    best_step, min_val_loss, best_state_dict = best_ckpt_info\n    ckpt_path = os.path.join(ckpt_dir, f'policy_step_{best_step}_seed_{seed}.ckpt')\n    torch.save(best_state_dict, ckpt_path)\n    print(f'Training finished:\\nSeed {seed}, val loss {min_val_loss:.6f} at step {best_step}')\n    return best_ckpt_info\ndef repeater(data_loader):\n    epoch = 0\n    for loader in repeat(data_loader):\n        for data in loader:\n            yield data\n        print(f'Epoch {epoch} done')\n        epoch += 1\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--eval', action='store_true')\n    parser.add_argument('--onscreen_render', action='store_true')\n    parser.add_argument('--ckpt_dir', action='store', type=str, help='ckpt_dir', required=True)\n    parser.add_argument('--policy_class', action='store', type=str, help='policy_class, capitalize', required=True)"
        },
        {
            "comment": "The code above is using the ArgumentParser from Python's argparse module to add various command-line arguments for a task. These arguments include 'task_name', 'batch_size', 'seed', 'num_steps', 'lr', 'load_pretrain', 'eval_every', and 'validate_every'. The 'save_every' argument is optional, as well as the 'resume_ckpt_path'. These arguments are required or defaulted depending on the specifications.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":638-647",
            "content": "    parser.add_argument('--task_name', action='store', type=str, help='task_name', required=True)\n    parser.add_argument('--batch_size', action='store', type=int, help='batch_size', required=True)\n    parser.add_argument('--seed', action='store', type=int, help='seed', required=True)\n    parser.add_argument('--num_steps', action='store', type=int, help='num_steps', required=True)\n    parser.add_argument('--lr', action='store', type=float, help='lr', required=True)\n    parser.add_argument('--load_pretrain', action='store_true', default=False)\n    parser.add_argument('--eval_every', action='store', type=int, default=500, help='eval_every', required=False)\n    parser.add_argument('--validate_every', action='store', type=int, default=500, help='validate_every', required=False)\n    parser.add_argument('--save_every', action='store', type=int, default=500, help='save_every', required=False)\n    parser.add_argument('--resume_ckpt_path', action='store', type=str, help='resume_ckpt_path', required=False)"
        },
        {
            "comment": "This code is using the Argparse module to define command-line arguments for a Python script. The arguments include options such as skipping mirrored data, specifying directories and lengths for history, future, and prediction. For ACT (Adaptive Computation Time) model, additional arguments like KL weight, chunk size, hidden dimension, feedforward dimension, and use of Variational Quantization are defined. These arguments allow the user to customize the behavior of the script based on their specific needs.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":648-661",
            "content": "    parser.add_argument('--skip_mirrored_data', action='store_true')\n    parser.add_argument('--actuator_network_dir', action='store', type=str, help='actuator_network_dir', required=False)\n    parser.add_argument('--history_len', action='store', type=int)\n    parser.add_argument('--future_len', action='store', type=int)\n    parser.add_argument('--prediction_len', action='store', type=int)\n    # for ACT\n    parser.add_argument('--kl_weight', action='store', type=int, help='KL Weight', required=False)\n    parser.add_argument('--chunk_size', action='store', type=int, help='chunk_size', required=False)\n    parser.add_argument('--hidden_dim', action='store', type=int, help='hidden_dim', required=False)\n    parser.add_argument('--dim_feedforward', action='store', type=int, help='dim_feedforward', required=False)\n    parser.add_argument('--temporal_agg', action='store_true')\n    parser.add_argument('--use_vq', action='store_true')\n    parser.add_argument('--vq_class', action='store', type=int, help='vq_class')"
        },
        {
            "comment": "These lines are adding command line arguments to the parser object, allowing users to specify values for 'vq_dim' and 'no_encoder'. The first argument, '--vq_dim', uses integer type and provides a help message. The second argument, '--no_encoder', is set as a boolean flag when true. Lastly, the main function is called with the parsed arguments passed in as keyword arguments.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/imitate_episodes.py\":662-665",
            "content": "    parser.add_argument('--vq_dim', action='store', type=int, help='vq_dim')\n    parser.add_argument('--no_encoder', action='store_true')\n    main(vars(parser.parse_args()))"
        }
    ]
}