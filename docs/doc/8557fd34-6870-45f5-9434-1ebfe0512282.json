{
    "summary": "The code compresses images, handles HDF5 datasets, and processes videos. It removes depth images, concatenates camera videos, decompresses/compresses images, and saves the first episode video.",
    "details": [
        {
            "comment": "The code compresses a dataset by creating a new compressed HDF5 file. It checks if the output path already exists, loads the uncompressed dataset, creates the compressed dataset with the same non-image data and attributes, and then copies over only the 'observations' key from the input file to the output file.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/compress_data.py\":0-34",
            "content": "\"\"\"\nExample usage:\n$ python3 script/compress_data.py --dataset_dir /scr/lucyshi/dataset/aloha_test\n\"\"\"\nimport os\nimport h5py\nimport cv2\nimport numpy as np\nimport argparse\nfrom tqdm import tqdm\n# Constants\nDT = 0.02\nJOINT_NAMES = [\"waist\", \"shoulder\", \"elbow\", \"forearm_roll\", \"wrist_angle\", \"wrist_rotate\"]\nSTATE_NAMES = JOINT_NAMES + [\"gripper\"]\ndef compress_dataset(input_dataset_path, output_dataset_path):\n    # Check if output path exists\n    if os.path.exists(output_dataset_path):\n        print(f\"The file {output_dataset_path} already exists. Exiting...\")\n        return\n    # Load the uncompressed dataset\n    with h5py.File(input_dataset_path, 'r') as infile:\n        # Create the compressed dataset\n        with h5py.File(output_dataset_path, 'w') as outfile:\n            outfile.attrs['sim'] = infile.attrs['sim']\n            outfile.attrs['compress'] = True\n            # Copy non-image data directly\n            for key in infile.keys():\n                if key != 'observations':\n                    outfile.copy(infile[key], key)"
        },
        {
            "comment": "Creates observation group in output file, copies non-image data, creates image group in observations, applies JPEG compression parameters, skips depth images, stores compressed lengths for each camera.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/compress_data.py\":36-60",
            "content": "            obs_group = infile['observations']\n            # Create observation group in the output\n            out_obs_group = outfile.create_group('observations')\n            # Copy non-image data in observations directly\n            for key in obs_group.keys():\n                if key != 'images':\n                    out_obs_group.copy(obs_group[key], key)\n            image_group = obs_group['images']\n            out_image_group = out_obs_group.create_group('images')\n            # JPEG compression parameters\n            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 50]\n            compressed_lens = []  # List to store compressed lengths for each camera\n            for cam_name in image_group.keys():\n                if \"_depth\" in cam_name:  # Depth images are not compressed\n                    out_image_group.copy(image_group[cam_name], cam_name)\n                else:\n                    images = image_group[cam_name]\n                    compressed_images = []\n                    cam_compressed_lens = []  # List to store compressed lengths for this camera"
        },
        {
            "comment": "This code compresses images and stores their lengths in a list. It then finds the maximum length of the compressed images and creates a dataset to store them in an HDF5 file, with the same length as the number of images. Finally, it saves the compressed lengths to the HDF5 file.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/compress_data.py\":62-81",
            "content": "                    # Compress each image\n                    for image in images:\n                        result, encoded_image = cv2.imencode('.jpg', image, encode_param)\n                        compressed_images.append(encoded_image)\n                        cam_compressed_lens.append(len(encoded_image))  # Store the length\n                    compressed_lens.append(cam_compressed_lens)\n                    # Find the maximum length of the compressed images\n                    max_len = max(len(img) for img in compressed_images)\n                    # Create dataset to store compressed images\n                    compressed_dataset = out_image_group.create_dataset(cam_name, (len(compressed_images), max_len), dtype='uint8')\n                    # Store compressed images\n                    for i, img in enumerate(compressed_images):\n                        compressed_dataset[i, :len(img)] = img\n            # Save the compressed lengths to the HDF5 file\n            compressed_lens = np.array(compressed_lens)"
        },
        {
            "comment": "Code saves a compressed dataset to the specified output path. It first checks if the video is in a list or dictionary format, and then creates a VideoWriter object with the desired parameters. For each frame of the video, it concatenates images from all cameras into one image, swaps B and R channels, and writes the resulting image to the output file. Finally, it releases the VideoWriter object and prints the saved video path.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/compress_data.py\":82-107",
            "content": "            _ = outfile.create_dataset('compress_len', compressed_lens.shape)\n            outfile['/compress_len'][...] = compressed_lens\n    print(f\"Compressed dataset saved to {output_dataset_path}\")\ndef save_videos(video, dt, video_path=None):\n    if isinstance(video, list):\n        cam_names = list(video[0].keys())\n        h, w, _ = video[0][cam_names[0]].shape\n        w = w * len(cam_names)\n        fps = int(1/dt)\n        out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n        # bitrate = 1000000\n        # out.set(cv2.VIDEOWRITER_PROP_BITRATE, bitrate)\n        for ts, image_dict in enumerate(video):\n            images = []\n            for cam_name in cam_names:\n                image = image_dict[cam_name]\n                image = image[:, :, [2, 1, 0]] # swap B and R channel\n                images.append(image)\n            images = np.concatenate(images, axis=1)\n            out.write(images)\n        out.release()\n        print(f'Saved video to: {video_path}')\n    elif isinstance(video, dict):"
        },
        {
            "comment": "This code loads an HDF5 dataset, removes depth images, concatenates remaining camera videos along the width dimension, saves the resulting video, and provides functions for loading and saving the first episode video.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/compress_data.py\":108-134",
            "content": "        cam_names = list(video.keys())\n        # Remove depth images\n        cam_names = [cam_name for cam_name in cam_names if '_depth' not in cam_name]\n        all_cam_videos = []\n        for cam_name in cam_names:\n            all_cam_videos.append(video[cam_name])\n        all_cam_videos = np.concatenate(all_cam_videos, axis=2) # width dimension\n        n_frames, h, w, _ = all_cam_videos.shape\n        fps = int(1 / dt)\n        out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n        for t in range(n_frames):\n            image = all_cam_videos[t]\n            image = image[:, :, [2, 1, 0]]  # swap B and R channel\n            out.write(image)\n        out.release()\n        print(f'Saved video to: {video_path}')\ndef load_and_save_first_episode_video(dataset_dir, video_path):\n    dataset_name = 'episode_0'\n    _, _, _, _, image_dict = load_hdf5(dataset_dir, dataset_name)\n    save_videos(image_dict, DT, video_path=video_path)\ndef load_hdf5(dataset_dir, dataset_name):\n    dataset_path = os.path.join(dataset_dir, dataset_name + '.hdf5')"
        },
        {
            "comment": "This code checks if the dataset file exists, loads compressed images from the file, and returns an image dictionary. If the dataset file is missing, it prints a message and exits. Compressed images are loaded for each camera, and the compressed images are decompressed into a list of images per camera. The final result is the image dictionary containing these lists of images.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/compress_data.py\":135-158",
            "content": "    if not os.path.isfile(dataset_path):\n        print(f'Dataset does not exist at \\n{dataset_path}\\n')\n        exit()\n    with h5py.File(dataset_path, 'r') as root:\n        compressed = root.attrs.get('compress', False)\n        image_dict = dict()\n        for cam_name in root[f'/observations/images/'].keys():\n            image_dict[cam_name] = root[f'/observations/images/{cam_name}'][()]\n        if compressed:\n            compress_len = root['/compress_len'][()]\n    if compressed:\n        for cam_id, cam_name in enumerate(image_dict.keys()):\n            padded_compressed_image_list = image_dict[cam_name]\n            image_list = []\n            for frame_id, padded_compressed_image in enumerate(padded_compressed_image_list):\n                image_len = int(compress_len[cam_id, frame_id])\n                compressed_image = padded_compressed_image\n                image = cv2.imdecode(compressed_image, 1)\n                image_list.append(image)\n            image_dict[cam_name] = image_list\n    return None, None, None, None, image_dict  # Return only the image dict for this application"
        },
        {
            "comment": "This code compresses all HDF5 datasets in a specified directory. It requires the directory path, creates a compressed dataset directory, iterates over each file ending with '.hdf5', compresses the dataset using 'compress_dataset' function, and after processing all datasets, loads and saves the video for the first episode.",
            "location": "\"/media/root/Prima/works/act-plus-plus/docs/src/compress_data.py\":161-180",
            "content": "if __name__ == '__main__':\n    parser = argparse.ArgumentParser(description=\"Compress all HDF5 datasets in a directory.\")\n    parser.add_argument('--dataset_dir', action='store', type=str, required=True, help='Directory containing the uncompressed datasets.')\n    args = parser.parse_args()\n    output_dataset_dir = args.dataset_dir + '_compressed'\n    os.makedirs(output_dataset_dir, exist_ok=True)\n    # Iterate over each file in the directory\n    for filename in tqdm(os.listdir(args.dataset_dir), desc=\"Compressing data\"):\n        if filename.endswith('.hdf5'):\n            input_path = os.path.join(args.dataset_dir, filename)\n            output_path = os.path.join(output_dataset_dir, filename)\n            compress_dataset(input_path, output_path)\n    # After processing all datasets, load and save the video for the first episode\n    print(f'Saving video for episode 0 in {output_dataset_dir}')\n    video_path = os.path.join(output_dataset_dir, 'episode_0_video.mp4')\n    load_and_save_first_episode_video(output_dataset_dir, video_path)"
        }
    ]
}